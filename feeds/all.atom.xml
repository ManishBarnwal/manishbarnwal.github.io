<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Manish Barnwal</title><link href="http://manishbarnwal.github.io/" rel="alternate"></link><link href="http://manishbarnwal.github.io/feeds/all.atom.xml" rel="self"></link><id>http://manishbarnwal.github.io/</id><updated>2017-07-12T00:00:00-03:00</updated><entry><title>Yaadon ki chuskiya</title><link href="http://manishbarnwal.github.io/blog/2017/07/12/yaadon_ki_chuskiya/" rel="alternate"></link><updated>2017-07-12T00:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-07-12:blog/2017/07/12/yaadon_ki_chuskiya/</id><summary type="html">&lt;p&gt;Recently, my friend, Tushar had come to Bangalore to visit me and Rahul before he left for the US for his Masters. He was here for a weekend. We intended to visit places around Bangalore. And we ended up visiting Lal Bagh. I know you are judging me right now. Who goes to Lal Bagh with friends! I had the same thought until I visited this place. Lal Bagh is beautiful and serene. This place is so under-rated. Go there in morning and you will love the place.
&lt;br&gt;
&lt;br&gt;
Without digressing further, let us talk about how we managed to write a song, play guitar to it, record it on phone and upload it on Youtube. On a Saturday evening, four of us were getting bored. We had just come enjoying our tea from the tea-stall outside my apartment. Tea is important here. See, we were already bored. We didn't have any plans for the night as well. All of us were seated in the room when we thought of writing a song. Ahh, it may sound a little weird but I have done this before. I like writing lyrics of a song at random times. 
&lt;br&gt;
&lt;br&gt;
So, we first decided what should be the theme of the song. Without much discussion, we all agreed on friendship. Since Tushar was leaving for the US, all our thoughts started moving towards that direction. How would he feel after a year when he is seated in a room far from his country. Rahul was the one mostly coming up with the important lines. Dipu and I were mostly into the guitar deciding the music and the lead. Tushar was mostly confused amongst all this. Fidgeting with his mobile. He ended up playing the egg-shaker. 
&lt;br&gt;
&lt;br&gt;
The song is in key of C but the song starts with G chord which gives a different sound to the song. You will see most of the songs in Bollywood, that are in C scale, starts with C-major chord or its relative minor - A-minor. And this sounds repetitive. So I thought of starting with G-major.
&lt;br&gt;
&lt;br&gt;
This is what we came up with in four hours. Here is the recording of the song - &lt;a href="https://www.youtube.com/watch?v=6YfoCfvhpko"&gt;Yaadon Ki Chuskiya&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Verse 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chuskiya, chai ki chuskiya... &lt;br&gt;
Chutkiya, yaaron ki chutkiya... &lt;br&gt;
Khushiya, chhoti chhoti khushiya &lt;br&gt;
Humm once... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yaad aati hai...bas yaad aati hai... &lt;br&gt;
Yaad aati hai...bas yaad aati hai... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Humming&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Hmmm...la la la la la... &lt;br&gt;
Hmmm...la la la... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Verse 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Khoya...kyu hoon main khoya &lt;br&gt;
Roya ...kyu hoon main roya &lt;br&gt;
Akela...kyu main akela &lt;br&gt;
Humm once... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Savera... hai andhera &lt;br&gt;
Pura... phir q adhura &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Savera... hai andhera &lt;br&gt;
Pura... phir kyu adhura &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Humming&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Hmmm...la la la la la... &lt;br&gt;
Hmmm...la la la... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Lead&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Play the lead&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yaad...aati hai...bas yaad aati hai... &lt;br&gt;
Yaad...aati hai...bas yaad aati hai... &lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Humming&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Hmmm...la la la... &lt;br&gt;
Hmmm...la la la... &lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Manish&lt;/p&gt;</summary><category term="nostalgia"></category><category term="friendship"></category><category term="college"></category><category term="get-together"></category><category term="bored"></category></entry><entry><title>Top lessons I learned from 3 years at Walmart</title><link href="http://manishbarnwal.github.io/blog/2017/07/05/lessons_learnt_from_3_years_of_job_at_walmart/" rel="alternate"></link><updated>2017-07-05T00:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-07-05:blog/2017/07/05/lessons_learnt_from_3_years_of_job_at_walmart/</id><summary type="html">&lt;p&gt;&lt;img src="http://manishbarnwal.com/images/life/top_lessons_walmart/life_lessons.jpg" alt="Photo of life lessons"
width="1150" height="40"&gt;
&lt;br&gt;
&lt;br&gt;
Having graduated from college, I joined WalmartLabs as Statistical analyst in the year 2014. We were a batch of four freshers, all from different colleges — one from IISc. Bangalore, two from ISI Kolkata, and I from IIT Kharagpur. We were the first batch of freshers to join the data science team. There were three seniors to guide us through the on-boarding formalities and getting to know the Walmart systems.
&lt;br&gt;
&lt;br&gt;
I clearly remember the first few days of the office. I used to be super excited everyday. I used to feel like there is so much to learn. There was a spark of learning that kept me excited. I worked at WalmartLabs for close to three years. People say, the first company is always special. And I won’t deny this. I have enjoyed my days at WalmartLabs. The stay here was packed with learnings, great friends, and wonderful experiences.
&lt;br&gt;
&lt;br&gt;
Today as I reckon how the last three years has passed, I think to myself what I could have done better? What were a few things that I took way too seriously. What could I have done better? What were my learnings?
&lt;br&gt;
&lt;br&gt;
Below are my learnings from the 1000 days I have spent working in a corporate job.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don’t hesitate to ask questions.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I had joined as a fresher, there were so many terms I had never heard of. WFH, WebEx, EDLP, EDLC were alien to me. Technology wise, I had never worked on R. But that never discouraged me asking questions whenever I got stuck. And the earlier you clarify these questions the better for you. You don’t want to ask the fresher-tagged questions after one year of your employment. Moreover, asking questions clarify concepts. So, ask as many questions as possible.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;New phase. Write a new story.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a new phase of life after college. We all are excited about it. This is a chance for you get a better version of yourself. You are surrounded by new people. You have a fresh page to start with. You can write a new story or continue with the story you have been living in your college. If there is something you want to change in your life. This is the time. When everyone around you is new. You don’t have your old people to drag you down. It’s a great opportunity to overcome any challenges or weaknesses from your past.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Your 9-to-5 job is not enough for your learnings.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do not be under the impression that your project will teach you all. There will always be gaps in your learnings. Bridge these gaps with your personal projects. I participate in hacks, read books, write codes for personal projects, and then blog about them. This keeps me motivated. Choose what you truly want to learn. This shall excite you and keep you on top of the learning curve.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;First impressions are exaggerated.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First impressions are exaggerated — be it a good one or a bad one. Let time pass by and then decide. The bad thing about first impressions is that people forget to update it.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Don’t get busy in 9 to 5 cycle.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is very easy to get comfortable in the 9–5 daily cycle and the weekly Monday-to-Friday cycle. Time flies week by week and you won’t even realize that you completed a year of your job. Learn new skills — be it boxing, guitar or anything you always wanted to learn. Weekends should not be looked upon as your lazy days.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Save money. Don’t waste all your money on beers, drinks, and getting yourself pampered.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After a year of job, I realized I had not saved enough. Saving is a practice. Inculcate it. It is up to you how much you want to save. It could be as simple as opening a recurrent deposit account.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Your manager should be aware of all your work.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Make sure you and your manager frequently connect to discuss the work you are doing. Always ask for feedback. Any improvement in any area that is required of you. If your manager doesn’t have answer to your questions, ask a senior you trust and respect.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;You are the average of 5 people you spend your most time with.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If there is one mantra in life I follow whole-heartedly, it is this one. You are the average of five people you spend your most time with. So choose your group wisely.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Be the go to person.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;No matter how many blogs or books you read, if you don’t solve real problems you won’t learn. Try to solve as many problems as possible. Become an expert of your skill. You should be seen as the go-to-person for this topic by people in your office. This will expose you to solve new problems and enhance your learnings.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Don’t get excited by hike. Let your work excite you.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is easier said than done. I will be lying if I say that I abide by this. I try to find peace and excitement in the work I am doing. Always ask the question — Why am I doing this? How will this help in the bigger picture? Try to relate yourself to the learnings from the work you are doing. If you are interested in your work, hike will come automatically as a byproduct. Never work just for hikes.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Give chance to your juniors. Always.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you are a senior, you have a different responsibility of nurturing your juniors. Pass on your learnings to them. Don’t look upon your juniors as the resource to do your shittiest work. Once they become senior, they will realize this and hate you for this. When presented with an opportunity, give chance to your juniors to present the solution.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Find a mentor. The right one.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A mentor is one who knows your problems, has faced similar problems in the past, and has answer to your problems. There will be times when nothing will work in your favor. You want to come out of the muddle. But you don’t know how. Your mentor will be your hero. I was fortunate to have some of my seniors as best mentors — Jeeban, Pralabh, and Issac. You find yours. The right ones. Not the ones who become your mentor just for the sake of company’s policy.
&lt;br&gt;
&lt;br&gt;
This is obviously not an exhaustive list but for now this is what I have. I will keep visiting this page to add more of my learnings. Please share your thoughts in comments. Share it with people who you think would enjoy reading this.&lt;/p&gt;</summary><category term="corporate lessons"></category><category term="walmart"></category><category term="job"></category><category term="learnings"></category></entry><entry><title>The essence of music in my life</title><link href="http://manishbarnwal.github.io/blog/2017/05/30/music_in_my_life/" rel="alternate"></link><updated>2017-05-30T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-30:blog/2017/05/30/music_in_my_life/</id><summary type="html">&lt;p&gt;&lt;img src="http://manishbarnwal.com/images/music/essence_music_life/Youtube-channel-logo.jpg" alt="Photo of music channel"
width="1150" height="40"&gt;
&lt;br&gt;
&lt;br&gt;
Guitar is cool. I reckon from my childhood when seeing someone play guitar in movies inspired a sense of great appreciation for the character playing it. Remember &lt;em&gt;Hrithik&lt;/em&gt; from the movie &lt;em&gt;Kaho na pyaar hai&lt;/em&gt;. I always wondered how does one play this instrument.
&lt;br&gt;
&lt;br&gt;
I loved listening to songs. &lt;em&gt;Sonu Nigam&lt;/em&gt;, &lt;em&gt;KK&lt;/em&gt;, &lt;em&gt;Kunal Ganjawala&lt;/em&gt;, and &lt;em&gt;Neeraj Sridhar&lt;/em&gt; (&lt;em&gt;Wo chali wo chali&lt;/em&gt;, remember this song?) were a few singers who kept me hooked to my CD player. I used to even remember the tough lyrics of a few Punjabi songs - &lt;em&gt;Kunal Ganjawala's Channa ve ghar aa jaa ve&lt;/em&gt;. And then, the next day sing the song with classmates during the recess.
&lt;br&gt;
&lt;br&gt;
My mother had this talent of identifying a singer by just listening to the singer's voice. And at that point of time, this was a huge talent for me. I was in awe of this talent of my mother's. I would play a song in the TV room and run to mother in the kitchen and ask her who was the singer. I don't remember even a single time when she flunked my tests.
&lt;br&gt;
&lt;br&gt;
So, today when I look back I realize music was something I really enjoyed. Sadly, I was not aware of this then. The idea of music was alien to me. I had never looked at it the way I look at it today.
&lt;br&gt;
&lt;br&gt;
The closest I could get to music was singing while taking the shower. And that too when father was not around. Singing in front of elders was considered rude. I don't know the reason. Even today, I don't sing in front of my father. Mum is cool. I used to listen and enjoy songs a lot. Many a times while solving the Mathematics problems on simultaneous equations or finding the area of hexagon. Music soothed me. Solving Mathematical problems while listening to music was heaven for the child in me. The best time of the day.
&lt;br&gt;
&lt;br&gt;
Days passed and the adolescent me got involved in other important responsibilities like attending classes, solving Irodov, DPPs (Daily Practice Problems), preparing for JEE. I was not at home now. I had shifted to Kota far far away from home. Now I was a fan of Atif songs. &lt;em&gt;Dooorie&lt;/em&gt;, &lt;em&gt;Pehli nazar mein&lt;/em&gt; were few of the songs to whose tune I used to solve DPPs - finding the maximum range of a projectile or identifying the equation of a plane satisfying a few constraints, or finding the entropy of a system.
&lt;br&gt;
&lt;br&gt;
Life used to start with breakfast at &lt;em&gt;TTS&lt;/em&gt; (Tina Tea Stall) in &lt;em&gt;Vigyan Nagar&lt;/em&gt;, attend the classes at &lt;em&gt;Gaurav Tower&lt;/em&gt;, have a &lt;em&gt;masala pattiz&lt;/em&gt; (a type of quick snacks) while returning from the class and end with solving the DPPs for that day. This was the routine. Kota has been the learning years of my life. I have learnt so much about life, responsibilities, and people from my stay at Kota. Life had only one aim - Crack the IIT JEE exam.
&lt;br&gt;
&lt;br&gt;
In the year of 2009 I got admitted to IIT Kharagpur, Maths and Computing. Mathematics was always with me. Place changed, people changed. The world here was different. People were different. The first year passed in getting acquainted to the place. We were masters of our world. Proxy, &lt;em&gt;bhaat&lt;/em&gt; (talk to your friends on the least important topics for hours), mid-sem exams, and most importantly &lt;em&gt;peace&lt;/em&gt; had become part of my life.
&lt;br&gt;
&lt;br&gt;
In the second year, there was an audition happening for the music club in the hostel. I was naive to think that I sing pretty decent. So, I too went for audition. I failed miserably. There were candidates far better than me. I came back from the audition, talked to my friends, headed to the canteen for tea and the audition chapter was closed.
&lt;br&gt;
&lt;br&gt;
It was in the winter of year 2013 when my brother, Mukesh had a winter break and he had asked me to recommend what he could do for a month? Without much thought, I suggested him to get a guitar. And he did. My parents tried to intervene saying his studies would get affected. Parents, I tell you, they care a lot about your future. I convinced my mother to get him the guitar or I will buy him one from my stipend. And they agreed.
&lt;br&gt;
&lt;br&gt;
Mukesh tried his hands on learning guitar for a month. Joined a tutor. But he didn't enjoy it much. Guitar is not everyone's cup of tea. He was more into photography. So later, I got him a DSLR. He has got his repository of clicked photos on Flickr. You can check out his photos at &lt;a href="https://www.flickr.com/photos/133478356@N02/"&gt;Mukesh Barnwal Flickr&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
My other brother, Mitesh had a knack for music. He used to hum songs a lot. The best one I reckon him singing is the song - &lt;em&gt;Kaho na pyaar hai&lt;/em&gt;. Now, the guitar belonged to Mitesh. He took it to his college in Jalpaiguri. He joined a teacher there. And slowly he picked up the tabs, the chords. He even auditioned for the music club in his college and got selected.
&lt;br&gt;
&lt;br&gt;
Within no time he started singing song while playing guitar. I still remember how amazed I was when I had first listened to his recorded audio on Whatsapp. I was proud of my brother. I was elated that he had learnt something that a lot of people would love to but failed to invest the time it needs to learn the guitar. You can listen to some of his songs at &lt;a href="https://www.youtube.com/channel/UCd0LdTnijWUa31TAfpxjnHg"&gt;Mitesh Barnwal Youtube&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
In June 2014, after completing my 20 days trip in Uttarakhand and Chandigarh, I was at home for another 20 days waiting to join Walmart Labs, my first job. I had no idea what to do for these 20 days.
&lt;br&gt;
&lt;br&gt;
It was one fine morning, Mitesh was practicing on his guitar that I sat next to him and saw him playing the guitar. I asked him, if I could try my hands on the instrument. He agreed. I asked him to teach me something. He smiled. Nevertheless, he explained me a few things- what the strings are named, what each part of the guitar is called and a few other details.
&lt;br&gt;
&lt;br&gt;
I further enquired about the effective way to learn the guitar. He had a few downloaded tutorials on learning to play guitar. During those few days, I did a few of those tutorials. My guitar sounded pathetic. The sound was not something one would call music. My brother explained, it takes time before you create something soothing. I used to listen to whatever he would say. I have a huge respect for a person who has learnt something from scratch. It shows that the person really enjoys it and has taken the pain to go through the learning curve.
&lt;br&gt;
&lt;br&gt;
I continued practicing the guitar for those 20 days. Sometimes my mother would enter the room and would ask with a smirk on her face - &lt;em&gt;You too want to learn the guitar, son? Ohh then we would have two guitars!&lt;/em&gt;. My mother hates anything that takes up space in the room. She always had a dislike for guitar as well, because of the space issue. Not so much though, once she got impressed by my brother's performance on guitar.
&lt;br&gt;
&lt;br&gt;
Post those 20 days, I landed in Bangalore to join the corporate world. Here too, I had a friend who used to play the guitar. I started using his guitar. After a month, I got the first salary credited in my account. The next day me and my friend, Dipu headed to Furtados, a music shop in Koramangala to get the guitar. I got an Epiphone brand guitar.
&lt;br&gt;
&lt;br&gt;
I started taking classes at &lt;a href="https://www.justinguitar.com/"&gt;justinguitar.com&lt;/a&gt;. This site is the best online resource available for a beginner. After a few months, I realized I needed a tutor for better learning curve. And it has been since then that I have been playing guitar.
&lt;br&gt;
&lt;br&gt;
The one thing I used to crave for after work in office was to sit down and practice the tabs and chords on the guitar. I have been playing this instrument for the last 2.5 years now.
&lt;br&gt;
&lt;br&gt;
I am in love with guitar. When I am happy I like to play guitar, when I am tensed I like to play the scales, when I am stressed I like to play something rock. If not never, it is very rare when I don't feel like playing guitar. I hope I continue playing with the same rigor in the coming years and the love for this beauty never abates.
&lt;br&gt;
&lt;br&gt;
I hope you enjoyed reading this. I will write more on guitar and music in the coming few days.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;</summary><category term="Music"></category></entry><entry><title>How to choose the probability cut-off in classification problem</title><link href="http://manishbarnwal.github.io/blog/2017/05/18/choosing_probability_cut-off_in_classification/" rel="alternate"></link><updated>2017-05-18T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-18:blog/2017/05/18/choosing_probability_cut-off_in_classification/</id><summary type="html">&lt;p&gt;Yesterday, I was taking a session on Data Science for few of my colleagues. The aim was to give a brief overview of machine learning. There were two of us taking the session. We had a rough idea what all we wanted to cover in the two hours session.
&lt;br&gt;
&lt;br&gt;
I started the session starting with what machine learning is. The types of learning - supervised and unsupervised and the examples that fall into each of these.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is machine learning?&lt;/h3&gt;
&lt;p&gt;Machine learning helps the machine learn from the data. Understand the pattern in the data and use it to predict the future values. There is a true function that maps the inputs to the output values. Machine learning is the process that helps to estimate that function. We try to find the proxy function that is as close as possible to the true function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The essence of machine learning is function estimation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you are interested to read more on this, click on this post. &lt;a href="http://manishbarnwal.com/blog/2017/05/12/machine_learning_function_estimation/"&gt;The essence of machine learning is function estimation&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
We then moved to explaining the simplest technique in machine learning - linear regression. We then moved to cover classification problems - where you have to classify an observation into one of the pre-defined classes. Talked about logistic regression, explained how in regression we try to regress the response variable with respect to the independent variables and how in logistic regression we regress the probability that the response variable will belong to class one.
&lt;br&gt;
&lt;br&gt;
In binary classification problem, logistic regression gives you the probability to belong to class 1. And if you want the probability for class 0, you just subtract this probability from 1. So what you have right now is the probability to fall in class 1.
&lt;br&gt;
&lt;br&gt;
What you want is the class to which the observations would fall? How do you convert this probabilities to get the classes? I posed this question to my colleagues. One of them said, this should come from business knowledge. How strict or lenient you want your model to become? Some said they would take any value greater than 0.5 as belonging to class 1 or else class 0. And few others were still contemplating.
&lt;br&gt;
&lt;br&gt;
I am assuming you understand what TPR and FPR means. If not, you may want to visit this post. &lt;a href="http://manishbarnwal.com/blog/2016/09/26/three_letter_acronym_roc_and_auc/"&gt;TPR, FPR, ROC, and AUC&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
By this time, I had already explained them ROC curve and confusion matrix. We went back to ROC curve and explained how the ROC curve gives you the true positive rate, false positive rate corresponding to a probability cut-off. The graph looks something like below.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you generate the above graph?&lt;/h3&gt;
&lt;p&gt;There are functions in R that can give you this plot in a single line. However, for the sake of doing it, I have written the below code that generated the above plot.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Choosing the probability cut-off&lt;/h3&gt;
&lt;p&gt;Once you have an understanding of ROC curve, we will proceed further to understand how we can use this plot to get the probability cut-off.
You choose some probability cut-offs say from 0.5 till 0.9 with some increment say 0.05 and calculate the TPR and FPR corresponding to each probability value.
&lt;br&gt;
&lt;br&gt;
You have to decide how much TPR and FPR you want. There is a trade-off between the tpr and fpr. If you want to increase TPR, your FPR will also increase. So depending on whether you want to detect all the positives (higher TPR) and willing to incur some error in terms of FPR, you decide the probability cut-off.
&lt;br&gt;
&lt;br&gt;
Many a times you may want to choose probability that gives you the maximum accuracy. However, care should be taken when you have a case where the response column is skewed. For instance, a bank wants to predict the loan defaulters.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>Markdown commands I use frequently</title><link href="http://manishbarnwal.github.io/blog/2017/05/15/markdown_commands_I_use_frequently/" rel="alternate"></link><updated>2017-05-15T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-15:blog/2017/05/15/markdown_commands_I_use_frequently/</id><summary type="html">&lt;p&gt;This post is aimed to capture the list of commands I use frequently while writing a post for this site in Markdown. You can write a post both in Markdown format and in HTML format. Markdown is preferred when you want to have a free-flow writing style. HTML is used when you want to have more to do with the publishing of the post.
&lt;br&gt;
&lt;br&gt;
I normally write in Markdown format because I have to write a lot of content. There is not much to do with image placement or ordering of the content. However, many a times I have to resort to HTML form of writing as well. For instance, the image that I have embedded in the 'About' tab is written in HTML format because I want to have an image floating to the right of the text. And it is easy to do this using HTML's image tag. Let us get started.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Inserting an image of appropriate size&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;url&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;alt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;some_text&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;width:width;height:height;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Use the HTML &lt;img&gt; element to define an image&lt;/li&gt;
&lt;li&gt;Use the HTML src attribute to define the URL of the image&lt;/li&gt;
&lt;li&gt;Use the HTML alt attribute to define an alternate text for an image, if it cannot be displayed properly&lt;/li&gt;
&lt;li&gt;Use the HTML width and height attributes to define the size of the image&lt;/li&gt;
&lt;li&gt;Use the CSS width and height properties to define the size of the image (alternatively)&lt;/li&gt;
&lt;li&gt;Use the CSS float property to let the image float&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;p&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;img&lt;/span&gt; &lt;span class="na"&gt;src=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://manishbarnwal.com/images/author/high.jpeg&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;alt=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Photo of author&amp;quot;&lt;/span&gt; &lt;span class="na"&gt;style=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;float:right;width:128px;height:128px;&amp;quot;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
I am a Senior Statistical Analyst at &lt;span class="nt"&gt;&amp;lt;b&amp;gt;&lt;/span&gt;@WalmartLabs&lt;span class="nt"&gt;&amp;lt;/b&amp;gt;&lt;/span&gt;. In am a graduate from IIT Kharagpur with a Masters in Mathematics and Computing. I joined @WalmartLabs in July 2014 where I have been working mostly on supply chain projects and have been deploying machine learning models across Hadoop cluster.
&lt;span class="nt"&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For instance, the above code would produce the below result.&lt;/p&gt;
&lt;p&gt;
&lt;img src="http://manishbarnwal.com/images/author/high.jpeg" alt="Photo of author" style="float:right;width:128px;height:128px;"&gt;
I am a Senior Statistical Analyst at &lt;b&gt;@WalmartLabs&lt;/b&gt;. In am a graduate from IIT Kharagpur with a Masters in Mathematics and Computing. I joined @WalmartLabs in July 2014 where I have been working mostly on supply chain projects and have been deploying machine learning models across Hadoop cluster.
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Inserting an image without much change in layout&lt;/h3&gt;</summary><category term="Miscellaneous"></category></entry><entry><title>Tutorial on dplyr- a package for data manipulation in R</title><link href="http://manishbarnwal.github.io/blog/2017/05/15/tutorial_on_dplyr/" rel="alternate"></link><updated>2017-05-15T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-15:blog/2017/05/15/tutorial_on_dplyr/</id><summary type="html">&lt;p&gt;R is the most used tool in data science. It has no dearth of packages for specific use cases. There are three packages that I feel can get your most of the work done - &lt;em&gt;ggplot2, dplyr, data.table&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ggplot2&lt;/strong&gt;- Used for visualization. Also known as grammar of graphics. This package is used to plot graphs. The syntax is intuitive and easy to learn.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;dplyr&lt;/strong&gt;- Used for data manipulation. Also known as grammar of data manipulation. Most of the data munging processes and methods gets done easily using this package.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;data.table&lt;/strong&gt;- Used for large files. You can read huge files within seconds. The data manipulation library for larger datasets.
&lt;br&gt;
&lt;br&gt;
In this post we will focus on learning dplyr package. Dplyr is a fast tool for data manipulation for data frame like object both in memory and out of memory. Let us get started. We will use hflights dataset to demonstrate the functions and syntax of dplyr.
&lt;br&gt;
&lt;br&gt;
We need to first install dplyr and hflights if you don't have it already in your R environment. You can install these using:
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;install&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dplyr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dependencies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;install&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hflights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dependencies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dplyr is a package which is referred to as a grammar of data manipulation. hflights is a package which has the dataframe hflights which consists of details about all the flight that fly in and out of Houston area in the year 2011
&lt;br&gt;
&lt;br&gt;
Load the installed packages and data using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hflights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;hflights&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Get an idea of the data we will be working on by looking at its structure and printing the first few rows of the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hflights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hflights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let me introduce to &lt;em&gt;tbl_df&lt;/em&gt;. Look at the code below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tbl_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hflights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;prints&lt;/span&gt; &lt;span class="n"&gt;only&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;only&lt;/span&gt; &lt;span class="n"&gt;those&lt;/span&gt; &lt;span class="n"&gt;many&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;fits&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;screen&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;easy&lt;/span&gt; &lt;span class="n"&gt;view&lt;/span&gt;

&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;can&lt;/span&gt; &lt;span class="n"&gt;also&lt;/span&gt; &lt;span class="n"&gt;specify&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt; &lt;span class="n"&gt;many&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;want&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;display&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;tbl_df&lt;/em&gt; creates a local data frame. It creates a wrapper around the original data frame that prints nicely. &lt;em&gt;tbl_df&lt;/em&gt; introduces a new data frame like structure called &lt;em&gt;tbl&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;tbl&lt;/em&gt; is of class data frame just that data manipulation is easy with this. This means that any function that we use with a data frame can be used by tbl type objects.
&lt;br&gt;
&lt;br&gt;
We will not use &lt;em&gt;tbl&lt;/em&gt; objects. I rarely use it. I am comfortable with data frame class and we will focus on data frames for the rest of the tutorial as well. Below are the list of some of the important functions of dplyr.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;convert&lt;/span&gt; &lt;span class="n"&gt;back&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;frame&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;see&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;filter: Keep rows matching criteria&lt;/h3&gt;
&lt;p&gt;Example: View all flights on January 1&lt;/p&gt;
&lt;p&gt;Base approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Month&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;DayofMonth&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dplyr approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Month&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;DayofMonth&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;Return&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;matching&lt;/span&gt; &lt;span class="n"&gt;conditions&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;AA&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;AS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;pipe&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;OR&lt;/span&gt; &lt;span class="n"&gt;condition&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;select: Pick columns by name&lt;/h3&gt;
&lt;p&gt;Example: Select data for only &lt;em&gt;UniqueCarrier, Distance, AirTime&lt;/em&gt; columns&lt;/p&gt;
&lt;p&gt;Base approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;[,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;UniqueCarrier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Distance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AirTime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dplyr approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AirTime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;keeps&lt;/span&gt; &lt;span class="n"&gt;only&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;variables&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;mention&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To get contiguous columns, use &lt;em&gt;starts_with(), ends_with, contains&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;DayOfWeek&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;starts_with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Taxi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ends_with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;em&gt;Chaining&lt;/em&gt; or &lt;em&gt;Pipelining&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;%&amp;gt;%&lt;/strong&gt; operator helps you write multiple operations in a chain. The output of first operation becomes an input to the next command.
&lt;br&gt;
&lt;br&gt;
Let us try to understand this operator using an example. Say we want to select &lt;em&gt;UniqueCarrier&lt;/em&gt; and &lt;em&gt;DepDelay&lt;/em&gt; columns and filter only rows having delays over 60 minutes.
&lt;br&gt;
&lt;br&gt;
Normal approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="cp"&gt;# nesting method to select UniqueCarrier and DepDelay columns and filter for delays over 60 minutes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Chain operator approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DepDelay&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;arrange: Reorder rows&lt;/h3&gt;
&lt;p&gt;Example: Select &lt;em&gt;UniqueCarrier&lt;/em&gt; and &lt;em&gt;DepDelay&lt;/em&gt; columns and sort by &lt;em&gt;DepDelay&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Base approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;order&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;UniqueCarrier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;DepDelay&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;increasing&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dplyr approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;arrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;increasing&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;ascending&lt;/span&gt;


&lt;span class="cp"&gt;# use `desc` for descending&lt;/span&gt;
&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;arrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;mutate: Add new variables&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mutate adds new variables and preserves existing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create new variables that are functions of existing variables
&lt;br&gt;
&lt;br&gt;
Example: Add new variable named &lt;em&gt;Speed&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Base approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Speed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;Distance&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;AirTime&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;
&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;[,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Distance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AirTime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Speed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;dplyr approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AirTime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Speed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Distance&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;AirTime&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above code doesn't add the &lt;em&gt;Speed&lt;/em&gt; variable to flights data frame. To explicitly add variable to the table you need to assign this to a flights table. The below code does this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;# Adding &amp;#39;Speed&amp;#39; variable to the table&lt;/span&gt;
&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="n"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Speed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Distance&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;AirTime&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;strong&gt;with&lt;/strong&gt; and &lt;strong&gt;within&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Perform R expressions using the items (variables) contained in a list or data frame&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The within function will even keep track of changes made, including adding or deleting elements, and return a new object with these revised contents.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;with&lt;/em&gt; is a generic function that evaluates expr in a local environment constructed from data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ArrDelay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AirTime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flightsWith&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="cp"&gt;# note that expr in with takes place only in the environment constructed&lt;/span&gt;

&lt;span class="cp"&gt;# Using &amp;#39;with&amp;#39; you cannot assign the output value to any variable&lt;/span&gt;

&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flightsWith&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;still&lt;/span&gt; &lt;span class="n"&gt;have&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;within&lt;/em&gt; is similar, except that it examines the environment after the evaluation of expr and makes the corresponding modifications to a copy of data (this may fail in the data frame case if objects are created which cannot be stored in a data frame) and returns it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flightsTemp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flights&lt;/span&gt;
&lt;span class="n"&gt;flightsTemp1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;within&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flightsTemp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Year&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flightsTemp1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;summarise: Reduce variables to values&lt;/h3&gt;
&lt;p&gt;Example: Create a table grouped by &lt;em&gt;Dest&lt;/em&gt;, and then &lt;em&gt;summarise&lt;/em&gt; each group by taking the mean of &lt;em&gt;ArrDelay&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;dplyr approach&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dest&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;summarise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;avg_delay&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ArrDelay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Another example: For each carrier, calculate the percentage of flights cancelled or diverted&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;summarise_each&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;funs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Cancelled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Diverted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One more example: For each carrier, calculate the minimum and maximum arrival and departure delays&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueCarrier&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;summarise_each&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;funs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(.,&lt;/span&gt; &lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(.,&lt;/span&gt; &lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;ArrDelay&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DepDelay&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Helper function n() counts the number of rows in a group&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Helper function n_distinct(vector) counts the number of unique items in that vector
&lt;br&gt;
&lt;br&gt;
Example: For each day of the year, count the total number of flights and sort in descending order&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;flights&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DayofMonth&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;summarise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flight_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
        &lt;span class="n"&gt;arrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flight_count&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;SQL Joins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;inner_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows from x where there are matching values in y, and all columns from x and y. If there are multiple matches between x and y, all combination of the matches are returned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;left_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;right_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows from y, and all columns from x and y. Rows in y with no match in x will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;semi_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows from x where there are matching values in y, keeping just columns from x. A semi join differs from an inner join because an inner join will return one row of x for each matching row of y, where a semi join will never duplicate rows of x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;anti_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows from x where there are not matching values in y, keeping just columns from x.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;full_join(x, y)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Return all rows and all columns from both x and y. Where there are not matching values, returns NA for the one missing.
&lt;br&gt;
&lt;br&gt;
There are many other functions in dplyr. For now, I have listed down a few of these. I will update this post soon.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>The essence of machine learning is function estimation</title><link href="http://manishbarnwal.github.io/blog/2017/05/12/machine_learning_function_estimation/" rel="alternate"></link><updated>2017-05-12T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-12:blog/2017/05/12/machine_learning_function_estimation/</id><summary type="html">&lt;p&gt;Machine learning is cool. There is no denying in that. In this post we will try to make it a little uncool, well it will still be cool but you may start looking at it differently. Machine learning is not a black box. It is intuitive and this post is just to convey that.
&lt;br&gt;
&lt;br&gt;
If I give you this function &lt;code&gt;f(x) = x^2 + log(x)&lt;/code&gt; and ask to you tell me what will be &lt;code&gt;f(2)&lt;/code&gt;, you will first laugh at me and then run away to do something important. This is trivial for you, right? If a function is there that maps inputs to outputs then it is very easy to get the output for any new input.
&lt;br&gt;
&lt;br&gt;
Machine learning helps you get a function that can map the input to the output. How does it do it? What is this function? We will try to answer such questions in the paragraphs below.
&lt;br&gt;
&lt;br&gt;
Let us try to answer the above questions using a problem that can be solved using machine learning. Assume, you are a technical recruiter. You have been running a recruitment firm for the last 3 years. Now you being tech savvy, you follow the latest trends in technology and you came to know about machine learning. You understand that machine learning can be used to predict the future given you have data from the past.
&lt;br&gt;
&lt;br&gt;
You thought, how can I use it to predict the expected salary of a candidate given other factors. The first thing that comes to your mind- do you have the data? And you hear out a pleasant yes!
&lt;br&gt;
&lt;br&gt;
You have the following data collected at individual level:
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Age of the candidate&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gender of the candidate&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of years of experience&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Highest level education degree&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;College - Top notch, Average, normal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Current salary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sector - IT, Finance, Electronics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Salary
&lt;br&gt;
&lt;br&gt;
And a few others. For now let us assume we have just these features and we want to predict the expected salary using these features. We have 3 years of data that has approximately 10,000 rows. So your dataset looks something like the below data:
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="photo of salary_data.png" src="http://manishbarnwal.com/images/ml_function_estimation/salary_data.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
So essentially we have seven independent features, &lt;strong&gt;X&lt;/strong&gt; - &lt;em&gt;age&lt;/em&gt;,  &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;years of experience&lt;/em&gt;, &lt;em&gt;highest level of education&lt;/em&gt;, &lt;em&gt;college&lt;/em&gt;, &lt;em&gt;current salary&lt;/em&gt;, &lt;em&gt;sector&lt;/em&gt; and corresponding &lt;em&gt;salary&lt;/em&gt;, &lt;strong&gt;Y&lt;/strong&gt;. What we want is next time when we have a candidate, we would obviously have his &lt;em&gt;age&lt;/em&gt;,  &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;years of experience&lt;/em&gt;, and other features. What we won't have is his salary. And, we want to estimate this value.
&lt;br&gt;
&lt;br&gt;
There would be some function, say &lt;strong&gt;f&lt;/strong&gt; that would map these &lt;strong&gt;X&lt;/strong&gt; to the &lt;strong&gt;Y&lt;/strong&gt; values. How do we find this function? We will use the 3 years of data we have - the training data.
&lt;br&gt;
&lt;br&gt;
We won't be able to find the actual function, the true function, &lt;strong&gt;f&lt;/strong&gt; because we don't have all of the data in the world. You can't collect the entire dataset available. It is impossible. What we use is a sample of data from the population. And, we use this sample as our training data.
&lt;br&gt;
&lt;br&gt;
Many a times, there are some factors that can't be captured. The set of independent features that we have captured is not an exhaustive list. There would obviously be other features that will have an impact on the salary.
&lt;br&gt;
&lt;br&gt;
Say in our example of salary prediction, some of the factors like exclusive and exceptional knowledge on some rare topics may land a candidate exorbitant offers from few of the companies. It is difficult to capture factors like these.
&lt;br&gt;
&lt;br&gt;
Now we understand why we can't have the true f. So we will try to get an estimate of &lt;strong&gt;f&lt;/strong&gt;, say &lt;strong&gt;f^&lt;/strong&gt;. We want this &lt;strong&gt;f^&lt;/strong&gt; to be as close to the true &lt;strong&gt;f&lt;/strong&gt; i.e. a proxy for the true function. There would obviously be error in estimating this true function and w want to minimize this error to as low as possible. How do we go about getting this &lt;strong&gt;f^&lt;/strong&gt;, an estimate of the true function, &lt;strong&gt;f&lt;/strong&gt;?
&lt;br&gt;
&lt;br&gt;
We have the data, remember the 3 years of historical data which contained the &lt;strong&gt;X&lt;/strong&gt; features and the corresponding &lt;strong&gt;Y&lt;/strong&gt; values. This is called the training data and there is a reason why it is called training data. Because we use this data to train the underlying algorithms to get the estimated function &lt;strong&gt;f^&lt;/strong&gt;.
&lt;br&gt;
&lt;br&gt;
You get that we use training data to get the estimated function, &lt;strong&gt;f^&lt;/strong&gt;. But how do we do it? We try to minimize the error between the true salary, &lt;strong&gt;Y&lt;/strong&gt; and the predicted salary, &lt;strong&gt;Y^&lt;/strong&gt; from the model. For now, understand that there is a way to minimize this error and get the estimated function.
&lt;br&gt;
&lt;br&gt;
Now these functions could be a simple one like having a linear relationship between the salary and the features or many a times a complex relationship which is not linear. There are techniques say linear regression or say decision trees that help you get the simple estimate or even a complex one respectively.
&lt;br&gt;
&lt;br&gt;
Once you have this estimate of function,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;f&lt;/strong&gt;(age,  gender, years of experience, highest level of education, college, current salary, sector) ---&amp;gt;  salary&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;you just pass in the &lt;strong&gt;X&lt;/strong&gt; and you should get your &lt;strong&gt;Y&lt;/strong&gt;. There, you have a machine learning model. And you know what you have done - you have just come up with a nice estimate of the true function.
&lt;br&gt;
&lt;br&gt;
Once you have this estimate, there are other questions that you might want to think over. How good an estimate is this function to the true function? What all assumptions you made to estimate this function? When would this estimate not be a good choice?. I will try to answer these questions in future posts. For now, I hope you get the gist that the essence of machine learning is function estimation.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>Time series and forecasting using R</title><link href="http://manishbarnwal.github.io/blog/2017/05/03/time_series_and_forecasting_using_R/" rel="alternate"></link><updated>2017-05-03T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-05-03:blog/2017/05/03/time_series_and_forecasting_using_R/</id><summary type="html">&lt;p&gt;Time series forecasting is a skill that few people claim to know. Machine learning is cool. And there are a lot of people interested in becoming a machine learning expert. But forecasting is something that is a little domain specific.
&lt;br&gt;
&lt;br&gt;
Retailers like Walmart, Target use forecasting systems and tools to replenish their products in the stores. An excellent forecast system helps in winning the other pipelines of the supply chain. If you are good at predicting the sale of items in the store, you can plan your inventory count well. You can plan your assortment well.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A good forecast leads to a series of wins in the other pipelines in the supply chain.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Disclaimer&lt;/strong&gt;: &lt;em&gt;The following post is  my notes on forecasting which I have taken while having read several posts from Prof. Hyndman&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;
Let us get started. First things first.
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is time series?&lt;/h3&gt;
&lt;p&gt;A time series is a sequence of observations collected at some time intervals. Time plays an important role here. The observations collected are dependent on the time at which it is collected.&lt;/p&gt;
&lt;p&gt;The sale of an item say Turkey wings in a retail store like Walmart will be a time series. The sale could be at daily level or weekly level. The number of people flying from Bangalore to Kolkata on daily basis is a time series. Time is important here. During Durga Puja holidays, this number would be humongous compared to the other days. This is know as seasonality.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is the difference between a time series and a normal series?&lt;/h3&gt;
&lt;p&gt;Time component is important here. The time series is dependent on the time. However a normal series say 1, 2, 3...100 has no time component to it. When the value that a series will take depends on the time it was recorded, it is a time series.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How to define a time series object in R&lt;/h3&gt;
&lt;p&gt;ts() function is used for equally spaced time series data, it can be at any level. Daily, weekly, monthly, quarterly, yearly or even at minutes level. If you wish to use unequally spaced observations then you will have to use other packages.
&lt;br&gt;
&lt;br&gt;
ts() is used for numerical observations and you can set frequency of the data.
ts() takes a single frequency argument. There are times when there will be multiple frequencies in a time series. We use msts() multiple seasonality time series in such cases. I will talk about msts() in later part of the post. For now, let us define what is frequency.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Frequency&lt;/h3&gt;
&lt;p&gt;When setting the frequency, many people are confused what should be the correct value. This is the simple definition of frequency.
Frequency is the number of observations per cycle. Now, how you define what a cycle is for a time series?
&lt;br&gt;
&lt;br&gt;
Say, you have electricity consumption of Bangalore at hourly level. The cycle could be a day, a week or even annual. I will cover what frequency would be for all different type of time series.
&lt;br&gt;
&lt;br&gt;
Before we proceed I will reiterate this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Frequency is the number of observations per cycle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
We will see what values frequency takes for different interval time series.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Daily data&lt;/strong&gt; There could be a weekly cycle or annual cycle. So the frequency could be 7 or 365.25.&lt;/p&gt;
&lt;p&gt;Some of the years have 366 days (leap years). So if your time series data has longer periods, it is better to use frequency = 365.25. This takes care of the leap year as well which may come in your data.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Weekly data&lt;/strong&gt;
There could be an annual cycle.
frequency = 52 and if you want to take care of leap years then use frequency = 365.25/7
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Monthly data&lt;/strong&gt;
Cycle is of one year. So frequency = 12
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Quarterly data&lt;/strong&gt; Again cycle is of one year. So frequency = 4
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Yearly data&lt;/strong&gt; Frequency = 1
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How about frequency for smaller interval time series&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Hourly&lt;/strong&gt; The cycles could be a day, a week, a year. Corresponding frequencies could be 24, 24 X 7, 24 X 7 X 365.25
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Half-hourly&lt;/strong&gt; The cycle could be a day, a week, a year. Corresponding frequencies could be 48, 48 X 7, 48 X 7 X 365.25
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Minutes&lt;/strong&gt;
The cycle could be hourly, daily, weekly, annual. Corresponding frequencies would be 60, 60 X 24, 60 X 24 X 7, 60 X 24 X 365.25
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Seconds&lt;/strong&gt; The cycle could be a minute, hourly, daily, weekly, annual. Corresponding frequencies would be 60, 60 X 60, 60 X 60 X 24,
60 X 60 X 24 X 7, 60 X 60 X 24 X 365.25
&lt;br&gt;
&lt;br&gt;
You might have observed, I have not included monthly cycles in any of the time series be it daily or weekly, minutes, etc. The short answer is, it is rare to have monthly seasonality in time series. To read more on this visit &lt;a href="http://robjhyndman.com/hyndsight/monthly-seasonality/"&gt;monthly-seasonality&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
Now that we understand what is time series and how frequency is associated with it let us look at some practical examples.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Some useful packages&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;forecast: For forecasting functions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tseries: For unit root tests and GARC models&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mcomp: Time series data from forecasting competitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fma: For data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;expsmooth: For data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fpp: For data
&lt;br&gt;
&lt;br&gt;
We will now look at few examples of forecasting. We will look at three examples. Before that we will need to install and load this R package - &lt;em&gt;fpp&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
install.packages('fpp')
library(fpp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Three examples&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Sale of beer in Australia&lt;/strong&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
dev.off() # to open up the plots with default settings.
ausbeer # is at quarterly level the sale of beer in each quarter.
plot(ausbeer)
beer &lt;- aggregate(ausbeer) # Converting to sale of beer at yearly level
plot(beer, main = 'Sale of beer at yearly level')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;img alt="photo of beer_plot" src="http://manishbarnwal.com/images/time_series_basics/beer_plot.png" /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sales of a group of pharmaceuticals&lt;/strong&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
*a10 is a group of pharmaceuticals*
a10 # Sale of pharmaceuticals at monthly level from 1991 to 2008
head(a10)
plot(a10, main = 'Sale of pharmaceuticals at monthly level')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Electricity demand for a period of 12 weeks on daily basis&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;taylor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;taylor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Fully automated forecast&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;forecast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beer&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="photo of beer_forecast_plot.png" src="http://manishbarnwal.com/images/time_series_basics/beer_forecast_plot.png" /&gt;&lt;/p&gt;
&lt;p&gt;The blue line is a point forecast. You can see it has picked the annual trend. The inner shade is a 90% prediction interval and the outer shade is a 95% prediction interval.
&lt;br&gt;
&lt;br&gt;
Similar forecast plots for a10 and electricity demand can be plotted using&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;forecast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;forecast&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;taylor&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Some simple forecasting methods&lt;/h3&gt;
&lt;p&gt;These are benchmark methods. You shouldn't use them. You will see why. These are naive and basic methods.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mean method&lt;/strong&gt;: Forecast of all future values is equal to mean of historical data
   Mean: meanf(x, h=10)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Naive method&lt;/strong&gt;: Forecasts equal to last observed value
   Optimal for efficient stock markets
   naive(x, h=10) or rwf(x, h=10); rwf stands for random walk function&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Seasonal Naive method&lt;/strong&gt;: Forecast equal to last historical value in the same season
   snaive(x, h=10)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Drift method&lt;/strong&gt;: Forecasts equal to last value plus average change
   Equivalent to extrapolating the line between the first and last observations
   rwf(x, drift = T, h=10)
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Forecast objects in R&lt;/h3&gt;
&lt;p&gt;Functions that output a forecast object are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;meanf()&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;croston() Method used in supply chain forecast. For example to forecast the number of spare parts required in weekend&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;holt(), hw()&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;stlf()&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ses() Simple exponential smoothing
&lt;br&gt;
&lt;br&gt;
Once you train a forecast model on a time series object, the model returns an output of forecast class that contains the following:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Original series&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Point forecasts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction intervals&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Forecasting method used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Residuals and in-sample one-step forecasts
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;A simple example on the beer time series&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;plot&lt;span class="p"&gt;(&lt;/span&gt;beer&lt;span class="p"&gt;)&lt;/span&gt;
fit &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ses&lt;span class="p"&gt;(&lt;/span&gt;beer&lt;span class="p"&gt;)&lt;/span&gt;
attributes&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Measures of forecast accuracy&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MAE&lt;/strong&gt;: Mean Absolute Error&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MSE or RMSE&lt;/strong&gt;: Mean Square Error or Root Mean Square Error&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MAPE&lt;/strong&gt;: Mean Absolute Percentage Error
&lt;br&gt;
&lt;br&gt;
MAE, MSE, RMSE are scale dependent.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MAPE is scale independent but is only sensible if the time series values &amp;gt;&amp;gt;0 for all i and y has a natural zero
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Test methods on a test set&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ausbeer &lt;span class="c1"&gt;# is at quarterly level the sale of beer in each quarter.&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;ausbeer&lt;span class="p"&gt;)&lt;/span&gt;
beer &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; aggregate&lt;span class="p"&gt;(&lt;/span&gt;ausbeer&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Converting to sale of beer at yearly level&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;beer&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# plot of yearly beer sales from 1956 to 2007&lt;/span&gt;

beer_train &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; window&lt;span class="p"&gt;(&lt;/span&gt;beer&lt;span class="p"&gt;,&lt;/span&gt; end &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1994.99&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# data from 1956 till 1994&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;)&lt;/span&gt;

beer_test &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; window&lt;span class="p"&gt;(&lt;/span&gt;beer&lt;span class="p"&gt;,&lt;/span&gt; start &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1995&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# data from 1995 till 2007&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;beer_test&lt;span class="p"&gt;)&lt;/span&gt;

a10Train &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; window&lt;span class="p"&gt;(&lt;/span&gt;a10&lt;span class="p"&gt;,&lt;/span&gt; end&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2005.99&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
a10Test &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; window&lt;span class="p"&gt;(&lt;/span&gt;a10&lt;span class="p"&gt;,&lt;/span&gt; start &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2006&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Simple methods for the BEER data&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;f1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; meanf&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
f2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; rwf&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
f3 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; rwf&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;,&lt;/span&gt; drift &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; h &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

plot&lt;span class="p"&gt;(&lt;/span&gt;f1&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;f2&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;f3&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;In-sample accuracy&lt;/h3&gt;
&lt;p&gt;This will give you in-sample accuracy but that is not of much use. It just gives you an idea how will the model fit into the data. Chances are that the model may not fit well into the test data. So we should always look at the accuracy from the test data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Out-of-sample accuracy&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beer_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beer_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beer_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Exponential smoothing&lt;/h3&gt;
&lt;p&gt;This method has been around since 1990s.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fit1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ets&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;,&lt;/span&gt; model &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ANN&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; damped &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k-Variable"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
fit2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ets&lt;span class="p"&gt;(&lt;/span&gt;beer_train&lt;span class="p"&gt;)&lt;/span&gt;

accuracy&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;)&lt;/span&gt;
accuracy&lt;span class="p"&gt;(&lt;/span&gt;fit2&lt;span class="p"&gt;)&lt;/span&gt;

fcast1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; forecast&lt;span class="p"&gt;(&lt;/span&gt;fit1&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
fcast2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; forecast&lt;span class="p"&gt;(&lt;/span&gt;fit2&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;fcast2&lt;span class="p"&gt;)&lt;/span&gt;

accuracy&lt;span class="p"&gt;(&lt;/span&gt;fcast1&lt;span class="p"&gt;,&lt;/span&gt; beer_test&lt;span class="p"&gt;)&lt;/span&gt;
accuracy&lt;span class="p"&gt;(&lt;/span&gt;fcast2&lt;span class="p"&gt;,&lt;/span&gt; beer_test&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;General notation&lt;/h3&gt;
&lt;p&gt;ETS(Error, Trend, Seasonal)
ETS(ExponenTial Smoothing)&lt;/p&gt;
&lt;p&gt;ETS(X, Y, Z):
'X' stands for whether you add the errors or multiply the errors on point forecasts.&lt;/p&gt;
&lt;p&gt;'Y' stands for whehter the trend component is additive or multiplicative or multiplicative damped&lt;/p&gt;
&lt;p&gt;'Z' stands for whether the seasonal component is additive or multiplicative or multiplicative damped
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Some examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ETS(A, N, N): Simple exponential smoothing with additive errors
'A'/'M' stands for whether you add the errors on or multiply the errors on the point forecsats&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ETS(A, A, N): HOlt's linear method with additive errors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ETS(A, A, A): Additive Holt-Winter's method with addtitive errors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ETS(M, A, M): Multiplicative Holt-Winter's method with multiplicative errors
&lt;br&gt;
&lt;br&gt;
There are 30 separate models in the ETS framework. However 11 of them are unstable so only 19 ETS models. So when you don't specify what model to use in &lt;em&gt;model&lt;/em&gt; parameter, it fits all the 19 models and comes out with the best model using AIC criteria.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;model1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ets&lt;span class="p"&gt;(&lt;/span&gt;a10Train&lt;span class="p"&gt;)&lt;/span&gt;
model2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ets&lt;span class="p"&gt;(&lt;/span&gt;a10Train&lt;span class="p"&gt;,&lt;/span&gt; model &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;MMM&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; damped &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k-Variable"&gt;F&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

forecast1 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; forecast&lt;span class="p"&gt;(&lt;/span&gt;model1&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
forecast2 &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; forecast&lt;span class="p"&gt;(&lt;/span&gt;model2&lt;span class="p"&gt;,&lt;/span&gt; h&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;
&lt;em&gt;There are many other parameters in the model which I suggest not to touch unless you know what you are doing&lt;/em&gt;. - Prof Hyndman&lt;/p&gt;
&lt;p&gt;If you want to have a look at the parameters that the method chose. Just type in the name of your model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You will see the values of &lt;em&gt;alpha&lt;/em&gt;, &lt;em&gt;beta&lt;/em&gt;, &lt;em&gt;gamma&lt;/em&gt;. Also,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;sigma&lt;/em&gt;: the standard deviation of the residuals&lt;/p&gt;
&lt;p&gt;&lt;em&gt;AIC&lt;/em&gt;: Akaike Information criteria. AIC gives you and idea how well the model fits the data. ets fits all the 19 models, looks at the AIC and give the model with the lowest AIC. The lower the AIC, the better the model fits.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;AICc&lt;/em&gt;: Corrected Akaike Information criteria
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;BIC&lt;/em&gt;: Bayesian Information Criteria
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;ets() function&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Automatically chooses a model by default using the AIC, AICc, BIC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can handle any combination of trend, seasonality and damping&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Produces prediction intervals for every model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensures the parameters are admissible (equivalent to invertible)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Produces an object of class ets
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;ets objects&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Methods&lt;/strong&gt;: coef(), plot(), summary(), residuals(), fitted(), simulate() and forecast()&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;plot()&lt;/em&gt; function shows the time plots of the original series along with the extracted components (level, growth and seasonal)
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Automatic forecasting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why use automatic forecasting?&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Most users are not very expert at fitting time series models&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Most experts cannot beat the best automatic algorithms. Prof. Hyndman accepted this fact for himself as well. He has been doing forecasting for the last 20 years.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Most busines need thousands of forecasts every week/month and they need it fast. You have to do it automatically.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some multivariate forecasting methods depend on many univariate forecasts.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Box-Cox transformations&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Transformations to stabilize the variance&lt;/strong&gt;
If the data show different variation at different levels of the series, then a transformation can be useful.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="photo of a10_plot.png" src="http://manishbarnwal.com/images/time_series_basics/a10_plot.png" /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the variation is increasing with the level of the series and the variation is multiplicative. If we take a log of the series, we will see that the variation becomes a little stable.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xlab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Time&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ylab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Log&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;a10&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;a10&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="photo of a10_log_plot.png" src="http://manishbarnwal.com/images/time_series_basics/a10_log_plot.png" /&gt;
&lt;br&gt;
&lt;br&gt;
Box-Cox transformations gives you value of parameter, &lt;em&gt;lambda&lt;/em&gt;. And based on this value you decide if any transformation is needed or not.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;lambda = 1 ; No substantive transformation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lambda = 1/2 ; Square root plus linear transformation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lambda = 0 ; Natural logarithm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;lambda = -1; Inverse plus 1
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Back-transformation&lt;/h3&gt;
&lt;p&gt;We must reverse the transformation (or back transform) to obtain forecasts on the original scale.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;a10 &lt;span class="c1"&gt;# Sale of pharmaceuticals at monthly level from 1991 to 2008&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;a10&lt;span class="p"&gt;)&lt;/span&gt;
lam &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; BoxCox.lambda&lt;span class="p"&gt;(&lt;/span&gt;a10&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 0.131&lt;/span&gt;
lam
fit &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; ets&lt;span class="p"&gt;(&lt;/span&gt;a10&lt;span class="p"&gt;,&lt;/span&gt; additive &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; lambda &lt;span class="o"&gt;=&lt;/span&gt; lam&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# &amp;#39;additive = T&amp;#39; implies we only want to consider additive models&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;forecast&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;))&lt;/span&gt;
plot&lt;span class="p"&gt;(&lt;/span&gt;forecast&lt;span class="p"&gt;(&lt;/span&gt;fit&lt;span class="p"&gt;),&lt;/span&gt; include&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;ARIMA forecasting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;R functions&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The arima() function in the &lt;em&gt;stats&lt;/em&gt; package provides seasonal and non-seasonal ARIMA model estimation including covariates&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However, it does not allow a constant unless the model is stationary&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It does not return everything required for forecast()&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It does not allow re-fitting a model to new data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the &lt;em&gt;Arima()&lt;/em&gt; function in the &lt;em&gt;forecast&lt;/em&gt; package which acts as a wrapper to &lt;em&gt;arima()&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Or use &lt;em&gt;auto.arima()&lt;/em&gt; function in the forecast package and it will find the model for you
&lt;br&gt;
&lt;br&gt;
This post was just a starter to time series. I will talk more about time series and forecasting in future posts. I plan to cover each of these methods - ses(), ets(), and Arima() in detail in future posts.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>Do you believe in yourself?</title><link href="http://manishbarnwal.github.io/blog/2017/04/03/believe_in_yourself/" rel="alternate"></link><updated>2017-04-03T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-04-03:blog/2017/04/03/believe_in_yourself/</id><summary type="html">&lt;p&gt;Are you a confident person? The mind asked itself. I replied with an excited &lt;em&gt;Yes!&lt;/em&gt;. The mind asked again, &lt;em&gt;are you sure?&lt;/em&gt;. This time I thought for a while but again after a pause, I said &lt;em&gt;yes&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Why was there a pause the second time?&lt;/strong&gt;
&lt;br&gt;
We humans doubt our capabilities a lot. And what makes us doubt our strengths? A single failure sometimes makes you feel cringed. You feel you are worthless. Life before this failure seems all happy and confident. If you have been confident all your life, how can a single failure shake away the temple of your confidence.
&lt;br&gt;
&lt;br&gt;
You know the reason? I will tell you. We humans fear failure. A lot. The thought of failure often surpasses the strength of trying it. We are taught from childhood that if you try something and if it doesn’t see the light of success then it is a bad thing. Why can’t we accept and celebrate failures?Now I am not talking about any failure. I am talking about failure where you gave your best. You tried all your means. You presented the best version of yourself.
&lt;br&gt;
&lt;br&gt;
Recently, I was talking to one of senior colleagues. He has two beautiful daughters. On mundane days, our discussions mostly revolve around deep learning and the new world it is going to present. Today was different. It was a Friday evening.
&lt;br&gt;
&lt;br&gt;
We happen to discuss about life and the lessons it teaches us. How sports shape your character? How teamwork is not to be seen just on the field playing football. Teamwork is everywhere were more than one person is involved. A person having a team game on his resume gets a plus point according to him. I instantly realized the reason behind this. It is not just about playing in a team. If you play sports, the chances of you failing increases manifold. Every match is either a win or a loss (read failure).
&lt;br&gt;
&lt;br&gt;
It is important to embrace failure gracefully. That way, you won’t fear the thought of failure. A fear can haunt you only till you run away from it. The moment you face it, the fear will be gone. The fear doesn’t know what to do once you accept it.
&lt;br&gt;
&lt;br&gt;
Do you feel low when you fail at achieving a goal or fail at delivering a task? Of course! We all do. Imagine you are famished, the first first few swallows of food are divine, right? That’s what success tastes like. It is tantalizing. On the contrary, failure is bland. There is always a feeling of something missing.
&lt;br&gt;
&lt;br&gt;
Now, if you are a confident person, you would have developed your confidence from some of your accomplishments in the past. Some examples of such accomplishments could be- you stood first in your class, you presented in front of a large audience despite all the fear of stammering and dry mouth, you learnt something from scratch and you are very proud of it.
&lt;br&gt;
&lt;br&gt;
Now the above examples are personal. Try to remember the tasks and achievements you are proud of. This achievements could be as small as fixing your bathroom door without the help of a carpenter. You fixed it on your own. Know that feeling when you do something uncertain all on your own? If yes, I am talking about such feelings.
&lt;br&gt;
&lt;br&gt;
How does these thoughts make you feel like? You feel happy about it. A sense of pride erupts in your body. The neurons in your brain all fire at once like sky shot fireworks. Remember these accomplishments and make this your fuel to lighten you up. Fight back at failure. Roar back at life. You have all the ingredients for the perfect recipe to success.
&lt;br&gt;
&lt;br&gt;
Treat failure as another chapter of life. And life doesn’t end at just one chapter. It is a long story with indefinite number of pages. Your life can’t be written in one book or 10 books. It is a saga of millions of books. Don’t stop at that chapter. You are the author of the next chapter as well. Decide what you want it to be.
&lt;br&gt;
&lt;br&gt;
So next time, when failure knocks at your door and gets in your home, welcome it. Let is sit in your home for a weekend. Talk to it. Try to understand it. And then one afternoon gather your ingredients to prepare the delicious meal of success. Failure will never eat that lunch and will jump off your balcony. Now that the failure is gone. Begin the new chapter. You already know the title to that chapter—&lt;em&gt;Believe in yourself&lt;/em&gt;.&lt;/p&gt;</summary><category term="life"></category><category term="increase confidence"></category><category term="life lessons"></category></entry><entry><title>Diving into H2O with R</title><link href="http://manishbarnwal.github.io/blog/2017/03/28/h2o_with_r/" rel="alternate"></link><updated>2017-03-28T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-03-28:blog/2017/03/28/h2o_with_r/</id><summary type="html">&lt;p&gt;Do you understand the pain when you have to train advanced machine learning algorithms like Random Forest on huge datasets? When there is a factor column that has way too many number of levels? When the time taken to train the model is so huge that you went to your pantry for snacks and came back, you are even done browsing 9gag but your model is still training, the code is still running? Fear no more, we will talk about these problems and how we can address them.
&lt;br&gt;
&lt;br&gt;
We will first try to understand why it takes so much time to train models on huge datasets in R. And then the solution to this - build models using H2O in R.
&lt;br&gt;
&lt;br&gt;
Our laptop has multiple cores in it. Most of the laptops these days come with at least 4 cores. R by default uses only one of the cores of your laptop. Say your laptop has 4 cores, then the remaining 3 cores are unused or may be partially used by other processes that your computer is running. Using just one core would obviously be slower than if R could use multiple cores in parallel.  &lt;/p&gt;
&lt;p&gt;What if R could use the other cores as well? This would definitely make the R codes run faster. The solution is H2o.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is H2O?&lt;/h3&gt;
&lt;p&gt;H2O is &lt;em&gt;The Open Source In-Memory, Prediction Engine for Big Data Science&lt;/em&gt;. H2o enables R to use the other cores of the laptop as well. R then runs on multiple cores of your laptop. You laptop is now like a standalone cluster. The power of you laptop has just increased by H2o. You know what your laptop is thinking right now - &lt;em&gt;With great power, comes great responsibility&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Do you need to worry about how H2O converts your laptop to a cluster?&lt;/h3&gt;
&lt;p&gt;Not at all. It is as simple as running this - h2o.init(). We will come to this in later part of the post. The whole idea why h2o was built was keeping in mind that most of its users would be Data Scientists and it should be easy for them to build these models using h2o without any hassle of worrying about distributing the code across the cluster. And the developers of h2o have achieved this with flying colors.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Initializing the H2O cluster&lt;/h3&gt;
&lt;p&gt;So we now understand what is the use of h2o and how it can make our life easier. Let us now talk about execution.
You would first need to install the h2o package using
&lt;pre style=*font-size:60%; padding:3px; margin:0em;*&gt;
&lt;code class=*bash*&gt;install.packages(&lt;em&gt;h2o&lt;/em&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Next, we load the installed package and launch h2o from R.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;library&lt;span class="p"&gt;(&lt;/span&gt;h2o&lt;span class="p"&gt;)&lt;/span&gt;
local_h2o &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; h2o.init&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above will initialize your h2o cluster. Now there are other parameter in &lt;em&gt;h2o.init()&lt;/em&gt; but for now let's go with the default settings. Few important parameters to &lt;em&gt;h2o.init()&lt;/em&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;nthreads&lt;/em&gt;: Number of cores of the computer you want to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;max_mem_size&lt;/em&gt;: The total RAM size allocated to the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;
If you are interested in the other input parameters that can be passed, you can read for help by typing &lt;code&gt;?h2o.init()&lt;/code&gt; in R.
&lt;br&gt;
&lt;br&gt;
Once you have initiated the cluster, check if your connection is working using &lt;code&gt;h2o.clusterInfo()&lt;/code&gt;.
You should see this line- &lt;em&gt;R is connected to the H2O cluster along with other details of your initiated cluster&lt;/em&gt;. Now your cluster is ready and you are good to start your coding workflow in h2o.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Working with H2O&lt;/h3&gt;
&lt;p&gt;There are two ways you can work with h2o. Either with the flow or writing the code in R editor. The h2o flow is like an user interface that can be accessed at &lt;a href="http://localhost:54321"&gt;http://localhost:54321&lt;/a&gt; after you have initiated the h2o instance using &lt;code&gt;h2o.init()&lt;/code&gt;. The other way is to write the code in R. The serious work gets done by writing the code in R.
&lt;br&gt;
&lt;br&gt;
I normally use h2o flow just to create the first base model to see how the base model is doing. H2o flow is easy to get started with. There is no coding involved here. The image below shows the look of h2o flow. You parse in your datasets using the &lt;strong&gt;Data&lt;/strong&gt; tab in the top right. The &lt;strong&gt;Model&lt;/strong&gt; tab gives you a list of models you can train on your parsed dataset. The flow is pretty easy to go through.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="photo of h2o_flow" src="http://manishbarnwal.com/images/h2o_flow.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
Almost all of the machine learning models are supported in H2o. Some of them being Deep Learning, Generalized Linear Models (GLM), Gradient Boosted Regression (GBM), K-Means, Naive Bayes, Principal Components Analysis (PCA), Principal Components Regression (PCR), Random Forest (RF) and few others.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Interaction of R with the cluster&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The data is not saved in the R workspace&lt;/em&gt;. All data munging occurs in the h2o instance. By the look and feel it is easy to believe that all the processing is taking place in R but that is not true.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;You are now &lt;strong&gt;not&lt;/strong&gt; limited by R's ability&lt;/em&gt; to handle the data but by the amount of memory allocated to your h2o instance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the user makes a request, R queries the server via the REST API, which returns a JSON file with the relevant information that R then displays in the console.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Demonstration on a dataset&lt;/h3&gt;
&lt;p&gt;We will try to understand the working of h2o using a classification problem. The problem is simple. The government of a country wants to understand if its citizens are happy or not.&lt;/p&gt;
&lt;p&gt;There are various independent features like &lt;em&gt;WorkStatus&lt;/em&gt;, &lt;em&gt;Residence_Region&lt;/em&gt;, &lt;em&gt;income&lt;/em&gt;, &lt;em&gt;Gender&lt;/em&gt;, &lt;em&gt;Unemployed10&lt;/em&gt;, &lt;em&gt;Alcohol_Consumption&lt;/em&gt; and a few others. There is a response column called &lt;em&gt;Happy&lt;/em&gt;. Your task is to classify a citizen as happy or not happy. I will write a follow up post that will involve the code for the above problem statement.
&lt;br&gt;
&lt;br&gt;
As always, feedbacks and comments are welcomed. Share this with people who are interested in learning machine learning and data science. Let us talk more of Data Science.&lt;/p&gt;</summary><category term="big data"></category><category term="R"></category></entry><entry><title>An illustrated introduction to adversarial validation part 2</title><link href="http://manishbarnwal.github.io/blog/2017/02/16/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-16T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-16:blog/2017/02/16/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;In the last post we talked about the &lt;a href="http://manishbarnwal.com/blog/2017/02/15/introduction_to_adversarial_validation/"&gt;idea of adversarial validation&lt;/a&gt; and how it helps the problem when your validation set result doesn't comply with that of test set result. In this post, I will share the R code to help achieve the idea of adversarial validation. The data used would be from &lt;a href="https://numer.ai/"&gt;Numerai competition&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Loading required packages&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
library(randomForest)
library(glmnet)
library(data.table)
library(MLmetrics)
getwd()
dir()
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Reading train and test data set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train &lt;- fread("Data/numerai_training_data.csv")
train &lt;- as.data.frame(train)
train$target &lt;- as.factor(train$target)

str(train)
dim(train) # has close to 136000 rows and having no missing values
head(train)

test &lt;- fread("Data/numerai_tournament_data.csv")
test &lt;- as.data.frame(test)
dim(test) # has close to 150000 rows and having no missing values
head(test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Creating the target variable to distinguish between train and test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train$isTest &lt;- 0 # assigning 0 for train and 1 for test data
test$isTest &lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Combining train and test data into a single data frame&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
combi &lt;- rbindlist(list(train[, -51], test[, -1])) # removing 'target' from train data and 't_id' from test data
combi$isTest &lt;- as.factor(combi$isTest)
combi &lt;- as.data.frame(combi)
str(combi)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Train a classifier to identify whether data comes from the train or test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
logitMod &lt;- glm(formula = isTest~. , data = combi, family = 'binomial')
summary(logitMod)
head(logitMod$fitted.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Predict on the training data to see which rows resembles most to the test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
pred &lt;- predict(logitMod, newdata = train, type = 'response')
head(pred)

trainData &lt;- train
head(trainData)
trainData$predictTest &lt;- pred
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Sort the training data by it’s probability of being in the test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
trainData &lt;- trainData[order(trainData$predictTest, decreasing = T), ]

valIndx &lt;- 1:(0.2*nrow(trainData))
colsToKeep &lt;- names(trainData)[!names(trainData) %in% c('isTest', 'predictTest')]

trainFinal &lt;- trainData[-valIndx, colsToKeep]
valData &lt;- trainData[valIndx, colsToKeep]

write.csv(trainFinal, 'trainfinal.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Build a random forest classifier to predict the 'target' variable&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
set.seed(1) # setting seed for reproducibility of the result

matX &lt;- trainFinal[, -grep('target', names(trainFinal))]
response &lt;- trainFinal[, 'target']
table(response)

rfMod &lt;- randomForest(x = matX, y = response, ntree = 200, mtry = 7) # training randomForest model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on validation set&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
rfValPreds &amp;lt;- predict(rfMod, newdata = valData, type="prob")
head(valPreds)
LogLoss(rfLassoValPreds, as.numeric(as.character(valData$target))) # LogLoss function from MLmetrics package
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;
The validation set gives a LogLoss of 0.699. Let us see how does this come out on test data set. For this we will predict on the test data and upload the predictions to the site.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on actual test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
testPreds &lt;- predict(rfMod, newdata = test, type = 'prob')
testPreds &lt;- testPreds[, 2]

submission &lt;- data.frame(t_id = test$t_id, probability = testPreds)
head(submission)
write.csv(submission, 'submission.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;
The predictions on test data shows a LogLoss of 0.694 which is same as that of the validation set. We can now hope to have same result on both validation set and test set.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>An illustrated introduction to adversarial validation part 2</title><link href="http://manishbarnwal.github.io/blog/2017/02/16/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-16T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-16:blog/2017/02/16/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;In the last post we talked about the &lt;a href="http://manishbarnwal.com/blog/2017/02/15/introduction_to_adversarial_validation/"&gt;idea of adversarial validation&lt;/a&gt; and how it helps the problem when your validation set result doesn't comply with that of test set result. In this post, I will share the R code to help achieve the idea of adversarial validation. The data used would be from &lt;a href="https://numer.ai/"&gt;Numerai competition&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Loading required packages&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
library(randomForest)
library(glmnet)
library(data.table)
library(MLmetrics)
getwd()
dir()
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Reading train and test data set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train &lt;- fread("Data/numerai_training_data.csv")
train &lt;- as.data.frame(train)
train$target &lt;- as.factor(train$target)

str(train)
dim(train) # has close to 136000 rows and having no missing values
head(train)

test &lt;- fread("Data/numerai_tournament_data.csv")
test &lt;- as.data.frame(test)
dim(test) # has close to 150000 rows and having no missing values
head(test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Creating the target variable to distinguish between train and test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train$isTest &lt;- 0 # assigning 0 for train and 1 for test data
test$isTest &lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Combining train and test data into a single data frame&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
combi &lt;- rbindlist(list(train[, -51], test[, -1])) # removing 'target' from train data and 't_id' from test data
combi$isTest &lt;- as.factor(combi$isTest)
combi &lt;- as.data.frame(combi)
str(combi)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Train a classifier to identify whether data comes from the train or test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
logitMod &lt;- glm(formula = isTest~. , data = combi, family = 'binomial')
summary(logitMod)
head(logitMod$fitted.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Predict on the training data to see which rows resembles most to the test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
pred &lt;- predict(logitMod, newdata = train, type = 'response')
head(pred)

trainData &lt;- train
head(trainData)
trainData$predictTest &lt;- pred
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Sort the training data by it’s probability of being in the test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
trainData &lt;- trainData[order(trainData$predictTest, decreasing = T), ]

valIndx &lt;- 1:(0.2*nrow(trainData))
colsToKeep &lt;- names(trainData)[!names(trainData) %in% c('isTest', 'predictTest')]

trainFinal &lt;- trainData[-valIndx, colsToKeep]
valData &lt;- trainData[valIndx, colsToKeep]

write.csv(trainFinal, 'trainfinal.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Build a random forest classifier to predict the 'target' variable&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
set.seed(1) # setting seed for reproducibility of the result

matX &lt;- trainFinal[, -grep('target', names(trainFinal))]
response &lt;- trainFinal[, 'target']
table(response)

rfMod &lt;- randomForest(x = matX, y = response, ntree = 200, mtry = 7) # training randomForest model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on validation set&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
rfValPreds &amp;lt;- predict(rfMod, newdata = valData, type="prob")
head(valPreds)
LogLoss(rfLassoValPreds, as.numeric(as.character(valData$target))) # LogLoss function from MLmetrics package
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;
The validation set gives a LogLoss of 0.699. Let us see how does this come out on test data set. For this we will predict on the test data and upload the predictions to the site.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on actual test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
testPreds &lt;- predict(rfMod, newdata = test, type = 'prob')
testPreds &lt;- testPreds[, 2]

submission &lt;- data.frame(t_id = test$t_id, probability = testPreds)
head(submission)
write.csv(submission, 'submission.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;
The predictions on test data shows a LogLoss of 0.694 which is same as that of the validation set. We can now hope to have same result on both validation set and test set.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>How to use Git and Github [draft]</title><link href="http://manishbarnwal.github.io/blog/2017/02/15/git_and_github/" rel="alternate"></link><updated>2017-02-15T10:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-15:blog/2017/02/15/git_and_github/</id><summary type="html">&lt;p&gt;I had taken this course - How to use git and github some time last year. This post is an amalgamation of the course notes and other tutorials I have completed in understanding git. I will talk about the most frequently used commands.&lt;br /&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git init&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Initializes an empty git repository in the directory. Creates a hidden folder .git in your directory. If you click on the .git folder, you will see many sub-directories and files in side this folder. But, you will hardly need to know what these files are. These folders are the guts of Git where all the magic happens.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git status&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gives you the status of the snapshot of your repository. Suppose you made some changes on your local disk, then this change doesn’t automatically gets reflected on GIT. git status tells you what all were modified since the last commit. It’s healthy to run git status often. Sometimes things change and you don’t notice it. Status of your repository could be one of the following:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;staged&lt;/em&gt;: Files are ready to be committed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;unstaged&lt;/em&gt;: Files with changes that have not been prepared to be committed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;untracked&lt;/em&gt;: Files aren’t tracked by Git yet. This usually indicates a newly created file.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;deleted&lt;/em&gt;: File has been deleted and is waiting to be removed from Git.
&lt;br&gt;
&lt;br&gt;
In a nutshell, you run git status to see if anything has been modified and/or staged since your last commit so you can decide if you want to commit a new snapshot and what will be recorded in it.
&lt;br&gt;
&lt;br&gt;
Staging Area:
A place where we can group files together before we “commit” them to Git. Once, you add the files to Git using git add, the files come to staging area. Staged files are files we have told git that are ready to be committed. The files listed here are in the &lt;em&gt;Staging area&lt;/em&gt;, and they are not in our repository yet.
&lt;br&gt;
&lt;br&gt;
To store our staged changes we run the commit command with a message describing what we’ve changed.
After using git add, make sure to use git status to see what all files are there in the staged area and be sure that you want to commit these files only.
&lt;br&gt;
&lt;br&gt;
You can unstage files by using the git reset command. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git reset filename.txt&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;em&gt;git reset&lt;/em&gt; merely unstage the files but these files will still be there in your local. So if you want to get rid of these unstaged files, you will have to use git checkout to the commit after which you added these files.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git — version&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gives you the version of git you are using&lt;/p&gt;
&lt;h3&gt;Commits&lt;/h3&gt;
&lt;p&gt;Commits are part of GIT VCS (version control systems) which lets you save meaningful changes of your code. It is because of commit that you will be able to go to any of the previous versions of your code.
&lt;br&gt;
&lt;br&gt;
A commit is a snapshot of every file in your repository at the time of commit. Commits are Git’s way of saving versions, so to save two different versions, you would create two commits.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Commits with multiple files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You will often work with multiple files and not just one code and these file may or may not be directly related. Any collection of such files is called a repository. Now, GIT keeps track of all the changes made to each of the files and these are carried forward i.e when you save a version of a file using commit you will save a version of all the files in that repository.
&lt;br&gt;
&lt;br&gt;
Git does not rename files when you save a new commit. Instead, Git uses the commit IDs to refer to different versions of the files, and you can use git checkout to access old versions of your files.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git commit -m “added new feature RMSE to the function”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git checkout&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is sort of like restoring previous version.&lt;/p&gt;
&lt;p&gt;Say, the commit ID of the most recent commit is 3884eab839af1e82c44267484cf2945a766081f3. You can use this commit ID to return to the latest commit after checking out an older commit.&lt;/p&gt;
&lt;p&gt;Format of git checkout&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git checkout 3884eab839af1e82c44267484cf2945a766081f3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;How often to commit&lt;/h3&gt;
&lt;p&gt;Since you can choose when to make a commit, you might be wondering how often to commit your changes. It’s usually a good idea to keep commits small. As the diff between two versions gets bigger, it gets harder to understand and less useful. However, you don’t want to make your commits too small either. If you always save a commit every time you change a line of code, your history will be harder to read since it will have a huge number of commits over a short time period.
&lt;br&gt;
&lt;br&gt;
A good rule of thumb is to make one commit per logical change. For example, if you fixed a typo, then fixed a bug in a separate part of the file, you should use one commit for each change since they are logically separate. If you do this, each commit will have one purpose that can be easily understood. Git allows you to write a short message explaining what was changed in each commit, and that message will be more useful if each commit has a single logical change.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Reflect: Manual Commits&lt;/h3&gt;
&lt;p&gt;What do you think are the pros and cons of manually choosing when to create a commit, like you do in Git, vs having versions automatically saved, like Google Docs does?
&lt;br&gt;
&lt;br&gt;
Manually choosing when to create a commit like in Git:&lt;/p&gt;
&lt;p&gt;Pros: You won’t have to wait for some time or lines of code and then that commit will be made. It depends on you and you get to decide whether the code change that you have done deserves or needs to be mentioned in the commit.&lt;/p&gt;
&lt;p&gt;Cons: Getting you to decide when to commit is a little subjective and different people will have different answer to it and so the size of commits will be different for different user and if you are not used to it, understanding the differences in the commits can prove to be problematic for you&lt;/p&gt;
&lt;p&gt;Having versions automatically saved like Google Docs does:&lt;/p&gt;
&lt;p&gt;Pros: You don’t need to worry about manually commiting. You know this gets committed automatically&lt;/p&gt;
&lt;p&gt;Cons: Too many commits or too less commits will be generated depending on what settings are there for automatic commits
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Branching&lt;/h3&gt;
&lt;p&gt;When developers are working on a feature or bug they’ll often create a copy (aka. branch) of their code they can make separate commits to. Then when they’re done they can merge this branch back into their main master branch.
&lt;br&gt;
&lt;br&gt;
Branches are what naturally happens when you want to work on multiple features at the same time. You wouldn’t want to end up with a master branch which has Feature A half done and Feature B half done.
&lt;br&gt;
&lt;br&gt;
Rather you’d separate the code base into two “snapshots” (branches) and work on and commit to them separately. As soon as one was ready, you might merge this branch back into the master branch and push it to the remote server.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Remove all the things!&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;git rm&lt;/strong&gt; command will not only remove the actual files from disk, but will also stage the removal of the files for us. Now that you’ve removed all the required files you’ll need to commit your changes. Feel free to run git status to check the changes you’re about to commit.
&lt;br&gt;
&lt;br&gt;
Removing one file is great and all, but what if you want to remove an entire folder? You can use the recursive option on git.
This will recursively remove all folders and files from the given directory.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git rm -r folder_of_cats&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git log&lt;/strong&gt;
Think of Git’s log as a journal that remembers all the changes we’ve committed so far, in the order we committed them. Gives you the list of all commits that have been made to the code. It gives you the commit number and an associated message that was added to the commit.
&lt;br&gt;
&lt;br&gt;
Exiting git log: To stop viewing git log output, press q (which stands for quit).
&lt;br&gt;
&lt;br&gt;
Getting Colored Output: To get colored diff output, run &lt;em&gt;git config — global color.ui auto&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;git log — stat&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Gives you the statistics of all the commits that has been made, with information like which file changed and whether lines were added or deleted for each commit.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git push&lt;/strong&gt;
The push command tells Git where to put our commits when we’re ready. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git push -u origin master&lt;/p&gt;
&lt;p&gt;git push&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The -u tells Git to remember the parameters, so that next time we can simply run git push and Git will know what to do.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Pulling Remotely&lt;/h3&gt;
&lt;p&gt;Let’s pretend some time has passed. We’ve invited other people to our GitHub project who have pulled your changes, made their own commits, and pushed them. We can check for changes on our GitHub repository and pull down any new changes by running this command.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git pull origin master&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;HEAD&lt;/h3&gt;
&lt;p&gt;The HEAD is a pointer that holds your position within all your different commits. By default HEAD points to your most recent commit.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git diff&lt;/strong&gt;
Gives you the difference between the commits that you have performed in your code. If you want to understand the differences between 2 commits (say comNo.1 and comNo.2) you just do this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git diff comNo.1 comNo.2&lt;/p&gt;
&lt;p&gt;git diff&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Without any extra arguments, a simple &lt;em&gt;git diff&lt;/em&gt; will display in unified diff format (a patch) what code or content you’ve changed in your project since the last commit that are not yet staged for the next commit snapshot.
&lt;br&gt;
&lt;br&gt;
So where git status will show you what files have changed and/or been staged since your last commit, git diff will show you what those changes actually are, line by line. It’s generally a good follow-up command to git status
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is a README?&lt;/h3&gt;
&lt;p&gt;Many projects contain a file named “README” that gives a general description of what the project does and how to use it. It’s often a good idea to read this file before doing anything with the project, so the file is given this name to make users more likely to read it.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Cloning a Repository&lt;/h3&gt;
&lt;p&gt;There is a difference between downloading and cloning a repository. When you clone a repository, you don’t just download the files i.e. the latest commit file but the entire commit history as well. To clone a repository, run git clone followed by a space and the repository URL.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;repo url&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Example: Use the following url to clone the Asteroids repository: https://github.com/udacity/asteroids.git&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone https://github.com/udacity/asteroids.git&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Git Errors and Warnings Solution&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Should not be doing an octopus&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Octopus is a strategy Git uses to combine many different versions of code together. This message can appear if you try to use this strategy in an inappropriate situation.
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;You are in ‘detached HEAD’ state&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;HEAD is what Git calls the commit you are currently on. You can “detach” the HEAD by switching to a previous commit. Despite what it sounds like, it’s actually not a bad thing to detach the HEAD. Git just warns you so that you’ll realize you’re doing it.
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;Panic! (the ‘impossible’ happened)&lt;/em&gt;
This is a real error message, but it’s not output by Git. Instead it’s output by GHC, the compiler for a programming language called Haskell. It’s reserved for particularly surprising errors!
&lt;br&gt;
&lt;br&gt;
Takeaway I hope these errors and warnings amused you as much as they amused me! Now you know what kind of errors Git can throw.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Git command review&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Compare two commits, printing each line that is present in one commit but not the other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git diff will do this. It takes two arguments — the two commit ids to compare.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a copy of an entire Git repository, including the history, onto your own computer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git clone will do this. It takes one argument — the url of the repository to copy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Temporarily reset all files in a directory to their state at the time of a specific commit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git checkout will do this. It takes one argument — the commit ID to restore.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Show the commits made in this repository, starting with the most recent.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git log will do this. It doesn’t take any arguments.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Behavior of git clone&lt;/h3&gt;
&lt;p&gt;If someone else gives you the location of their directory or repository, you can copy or clone it to your own computer.
This is true for both copying a directory and cloning a repository.
&lt;br&gt;
&lt;br&gt;
If you have a URL to a repository, you can copy it to your computer using git clone. For copying a directory, you weren’t expected to know this, but it is possible to copy a directory from one computer to another using the command scp, which stands for “secure copy”. The name was chosen because the &lt;strong&gt;scp&lt;/strong&gt; command lets you securely copy a directory from one computer to another.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The history of changes to the directory or repository is copied.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true for cloning a repository, but not for copying a directory. The main reason to use git clone rather than copying the directory is because git clone will also copy the commit history of the repository. However, copying can be done on any directory, whereas git clone only works on a Git repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you make changes to the copied directory or cloned repository, the original will not change.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true for both copying a directory and cloning a repository. In both cases, you’re making a copy that you can alter without changing the original.&lt;/p&gt;
&lt;p&gt;- The state of every file in the directory or repository is copied.&lt;/p&gt;
&lt;p&gt;This is true for both copying a directory and cloning a repository. In both cases, all the files are copied.
 &lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="git"></category><category term="github"></category></entry><entry><title>An illustrated introduction to adversarial validation part 1</title><link href="http://manishbarnwal.github.io/blog/2017/02/15/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-15T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-15:blog/2017/02/15/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;You'd have heard about cross-validation - a common technique used in data-science process to avoid overfitting and many a times to tune the optimal parameters. Overfitting is when the model does well on training data but fails drastically on test data. The reason could be one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The model is trying to map the exact findings of training data to test data instead of generalizing the patterns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The train data and test data are significantly different from each other i.e. they have not been derived from the same population.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;We will try to understand the second issue. What is the problem with the second issue? If you have participated in Kaggle like competitions, then you would know the way these competitions work. You are given a training data set and test dataset. You train your model on training data, predict on the test data and upload the predictions on Kaggle to get your rank.
&lt;br&gt;
&lt;br&gt;
What we typically do is divide the training data into train and validation data set. Validation data is used to get an idea of how your model will work on the test data. Now imagine if your train data and test data are different in terms of the population from where they've been derived. You won't see the same result in validation and test data. You see the problem here?
&lt;br&gt;
&lt;br&gt;
The use of validation data is to understand how the model is expected to perform on test data. But if train and test are not identically distributed, validation and test data would show different results.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;The solution&lt;/h3&gt;
&lt;p&gt;The solution is &lt;strong&gt;adversarial validation&lt;/strong&gt;. I got to know about this recently when I started participating in &lt;a href="https://numer.ai/"&gt;Numerai&lt;/a&gt; competition and read about this technique on fastml. Here's how it is done.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
1. &lt;strong&gt;Build a classifier to distinguish between training and test data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Combine your train and test data into one data. Create a response variable say isTest and assign it as 0 to all the rows in training data and 1 to all the rows in test data. Now your task is to build a classification model that will distinguish between the training and test data. This could be any classification model - logistic or random forest.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
2. &lt;strong&gt;Sort the predicted probabilities of training data in decreasing order&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have the model built, use this model to predict on the training data. You will get the fitted probabilities. Sort the probabilities in decreasing order i.e. the row having highest probability of being classified as test data comes to the top.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
3. &lt;strong&gt;Take the starting few rows as your validation set&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The starting few rows are now those rows of training data that resembles the most to test data. Take the starting few rows say 30% as your validation set. And the remaining as your train data to train your model.
&lt;br&gt;
&lt;br&gt;
Now, the accuracy metrics on your validation set should be similar to that on the test data. If your model works well on validation data, it should work well on test data. If you are interested in the implementation of what we just talked, head out to &lt;em&gt;&lt;a href="http://manishbarnwal.com/blog/2017/02/16/introduction_to_adversarial_validation/"&gt;this post&lt;/a&gt;&lt;/em&gt; for the part 2 where the code is written in R.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>An illustrated introduction to adversarial validation part 1</title><link href="http://manishbarnwal.github.io/blog/2017/02/15/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-15T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-15:blog/2017/02/15/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;You'd have heard about cross-validation - a common technique used in data-science process to avoid overfitting and many a times to tune the optimal parameters. Overfitting is when the model does well on training data but fails drastically on test data. The reason could be one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The model is trying to map the exact findings of training data to test data instead of generalizing the patterns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The train data and test data are significantly different from each other i.e. they have not been derived from the same population.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;We will try to understand the second issue. What is the problem with the second issue? If you have participated in Kaggle like competitions, then you would know the way these competitions work. You are given a training data set and test dataset. You train your model on training data, predict on the test data and upload the predictions on Kaggle to get your rank.
&lt;br&gt;
&lt;br&gt;
What we typically do is divide the training data into train and validation data set. Validation data is used to get an idea of how your model will work on the test data. Now imagine if your train data and test data are different in terms of the population from where they've been derived. You won't see the same result in validation and test data. You see the problem here?
&lt;br&gt;
&lt;br&gt;
The use of validation data is to understand how the model is expected to perform on test data. But if train and test are not identically distributed, validation and test data would show different results.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;The solution&lt;/h3&gt;
&lt;p&gt;The solution is &lt;strong&gt;adversarial validation&lt;/strong&gt;. I got to know about this recently when I started participating in &lt;a href="https://numer.ai/"&gt;Numerai&lt;/a&gt; competition and read about this technique on fastml. Here's how it is done.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
1. &lt;strong&gt;Build a classifier to distinguish between training and test data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Combine your train and test data into one data. Create a response variable say isTest and assign it as 0 to all the rows in training data and 1 to all the rows in test data. Now your task is to build a classification model that will distinguish between the training and test data. This could be any classification model - logistic or random forest.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
2. &lt;strong&gt;Sort the predicted probabilities of training data in decreasing order&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have the model built, use this model to predict on the training data. You will get the fitted probabilities. Sort the probabilities in decreasing order i.e. the row having highest probability of being classified as test data comes to the top.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
3. &lt;strong&gt;Take the starting few rows as your validation set&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The starting few rows are now those rows of training data that resembles the most to test data. Take the starting few rows say 30% as your validation set. And the remaining as your train data to train your model.
&lt;br&gt;
&lt;br&gt;
Now, the accuracy metrics on your validation set should be similar to that on the test data. If your model works well on validation data, it should work well on test data. If you are interested in the implementation of what we just talked, head out to &lt;em&gt;&lt;a href="http://manishbarnwal.com/blog/2017/02/16/introduction_to_adversarial_validation/"&gt;this post&lt;/a&gt;&lt;/em&gt; for the part 2 where the code is written in R.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>The curse of bias and variance [draft]</title><link href="http://manishbarnwal.github.io/blog/2017/02/08/the_curse_of_bias_and_variance/" rel="alternate"></link><updated>2017-02-08T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-08:blog/2017/02/08/the_curse_of_bias_and_variance/</id><summary type="html">&lt;p&gt;Statistics is the field of study where we try to draw conclusions about the population from a sample. Why do we talk about sample? Why can't we get the conclusions about the population directly from the population? Let me illustrate this by an example.
&lt;br&gt;
&lt;br&gt;
Let us say we want to understand which brand of beer do the people of Bangalore prefers? An interesting question. If I ask you this question, how would you approach this problem?
&lt;br&gt;
&lt;br&gt;
You can't go around asking each and every person their favorite beer? Or can we? No, we can't cover each and every individual because the 'population' is huge. One thing you can do is you may ask among your circle of friends their preference of beer and get an overall idea of the population. But we have a problem with this analysis. Do you see the problem? Your estimation is suffering from bias or we say your sample is biased. Biased sample is when it is not random. There is some form of personal preference in the choice of picking data.
&lt;br&gt;
&lt;br&gt;
In your case, chances are most of your friends will be of same age as you. Intuitively, I feel that your age also decides what kind of beer you like. Say, when I was in college I'd never heard of Corona and Kingfisher was the best beer I had tasted. So we can't estimate the best brand of beer that Bangalore prefers from the sample of your friends.
&lt;br&gt;
&lt;br&gt;
So population is a broader set of data that covers all the data points in the whole universe. Sample is a subset of that data. We try to infer the characteristics of the population from the sample or try to answer questions about the population from the sample. Getting or collecting population data is tough as explained in the above example (The favorite beer example).
&lt;br&gt;
&lt;br&gt;
I hope the concept of population and sample is clear now. It's normally not feasible to get the data for the complete population so we try to estimate parameters or findings of the population from a sample.
&lt;br&gt;
&lt;br&gt;
Another example. Not the beer but a rather completely different one - marriages.
&lt;br&gt;
&lt;br&gt;
Say, now we want to understand what are the factors that affect the age at which one gets married. Some of the factors that came to me without much thought are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Gender&lt;/li&gt;
&lt;li&gt;Love or arranged marriage&lt;/li&gt;
&lt;li&gt;Plan for higher studies&lt;/li&gt;
&lt;li&gt;Company type - Government of Private&lt;/li&gt;
&lt;li&gt;Salary&lt;/li&gt;
&lt;li&gt;Region&lt;/li&gt;
&lt;li&gt;Religion
&lt;br&gt;
&lt;br&gt;
Now we don't know the true relationship between marital age and the variables listed above. There should be a true function that maps response variable to the predictors but we don't know what that function is.
&lt;br&gt;
&lt;br&gt;
Let's say that true function is f.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;f: (gender, love-marriage, higher-studies, ...) --&amp;gt; marital age&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
Now we don't know the true f(). But we can estimate the true function using the data we may have from the past. It's easy to collect the data for the respective variables and the marital age. We are trying to come up with a function say fcap that resembles closely to the true function f.
&lt;br&gt;
&lt;br&gt;
Whenever we try to estimate true f from the data in hand, we will obviously get some error. This error can be categorized in two types:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reducible error&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the name suggests, this is something that the analyst has some control over. This can be reduced based on the kind of data you collect and the models (not all models are same right?)one uses to estimate the true f.
   This error can arise from a combination of 'bias' and 'variance'. We will talk about bias and variance in coming few paragraphs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Irreducible error&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the name suggests, this error is something that the analyst has not control over. There is always some information that is difficult to capture in data. There is always some randomness in the data and that is difficult to explain. This error can't be reduced using any model whatsoever.&lt;/p&gt;
&lt;p&gt;Let us dig deeper into reducible error. We will talk about bias and variance here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When we talk about estimating the true f, there are various models that can be used. Now not all models are same. Each has some characteristics of its own. A linear regression, the simplest model is different from say a random forest model. Bias is the error that captures how far is the predicted value (say predicted marital age given the variables like gender, love or arranged marriage, etc.) from the true value (actual marital age).
&lt;br&gt;
&lt;br&gt;
Now you may ask is there a relationship between the bias and the type of models used? Yes there is one.
Bias tends to decrease as the complexity of model increases i.e it is expected that the model error will decrease if we use a more complex model instead of a simple model. Now this is intuitive isn't it?
&lt;br&gt;
&lt;br&gt;
You may ask what is meant by complexity of a model or an example of it? Linear regression is a very simple model whereas random forest is a more complex model. Linear regression tries to fit just a line to the actual data and it assumes a linear relationship. A random forest model is more complex is the sense that it uses an ensemble of decision trees and is able to explain non-linear relationship as well.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Variance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you estimate the true f, you use some data to train the model, that data is called training data. The data that the machine uses to train on, to find the patterns.
Now once your model is trained, you want to use this to predict on unseen data (the data that the model has not been trained on) that data is called test data.
&lt;br&gt;
&lt;br&gt;
There is always some variability in the training data and the test data. You can't expect both these data to be exactly same.
It is important to note that when you train your model, the model doesn't learn the exact values but instead try to find patterns so that when this pattern is seen on test data, the model is able to predict correctly.
Many complex model overfits the data i.e models that perform well on training data but fails drastically on test data.
&lt;br&gt;
&lt;br&gt;
How is variance related to the complexity of models?
As the complexity increases, the chances of overfitting increases i.e the variance increases. Coming to random forest and linear regression example, a random forest's variance is expected to be higher than that from a linear regression model.
&lt;br&gt;
&lt;br&gt;
So if we talk about complexity, bias and variance together, this is the relationship between them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As the complexity of model increases, the bias decreases but the variance increases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
So there is a trade-off between bias and variance. You can't get both low bias and low variance at the same time. You will have to accept a trade-off. Do you now understand why we call it the curse of bias and variance? I hope you do.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="bias"></category><category term="variance"></category></entry><entry><title>Visualization in ML is under-rated</title><link href="http://manishbarnwal.github.io/blog/2017/01/27/visualization_is_under-rated/" rel="alternate"></link><updated>2017-01-27T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-01-27:blog/2017/01/27/visualization_is_under-rated/</id><summary type="html">&lt;p&gt;Visualization is one of the most important pillars of data science. Every one wants to learn Machine learning but if you explain them the little tasks that involve the overall workflow of the process, it turns them off. Everyone just wants to do the cool stuff. They want to build models and be done with it. And I was one of them. I understand that feeling when you get the data and without much understanding of the features in the dataset, we just want to throw in the data to a model and hope that something good comes out.
&lt;br&gt;
&lt;br&gt;
I have participated in many hacks relating to ML and I used to just hope the trained model would do the task. Sadly, this thought always betrayed me. Even ensemble of various models may not work. Sometimes there are patterns in data which one gets to know only when one does EDA, when one plots a few graphs. One tries to understand the relationship of one feature with the other or do univariate analysis of the columns. An ML model is not always sufficient to understand the patterns in data.
&lt;br&gt;
&lt;br&gt;
After a point, the learning of a model becomes saturated. It can only perform to a certain point. If there is a pattern that you have identified, it would definitely help the model to better train on it.
&lt;br&gt;
&lt;br&gt;
I have learnt this hard way - You can't ignore EDA, visualization if you want to come in the top 1% of the leaderboard. Anyone and everyone can run a random forest model. Tuning the parameters is a little tedious but that too can be done with little practice. But finding the hidden patterns in the dataset, finding the relationship, understanding the little nuances in the data is an art. It's a skill. It takes practice. A lot of practice.
&lt;br&gt;
&lt;br&gt;
I have always wondered, how does the winners go about finding those patterns. Isn't there a course that could teach me these hacks to find the unseen patterns. Unfortunately, there are courses that teaches EDA, they teach you how to use ggplot2. But the thing I am talking about takes altogether a different mindset. One needs to be patient with this chunk of work. There are no defined paths to it. You just keep doing the EDA, observe the patterns, try to create new features based on this and once you have done this a hundred times, then you realize you finally understand this thing.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;So how do you master the art of EDA?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The short answer is practice. But then how do you practice EDA? You take shorter assignments, try to write the code snippet of common plots like histogram, scatterplot, bar-plot. And when you plot these graphs, I would strongly recommend to use &lt;em&gt;ggplot2&lt;/em&gt; if you are using R. In one of the talks, Hadley Wickham has strongly asserted that one should start learning visualization using &lt;em&gt;ggplot2&lt;/em&gt;. The syntax is very intuitive and makes plots interesting and beautiful. And when a person like Hadley advises a thing, you just follow it.
&lt;br&gt;
&lt;br&gt;
I have learnt more about R by looking at other people's codes. Look at the Kernels shared on Kaggle by the top performers of Kaggle. The kind of analysis they do is just mind-blowing. And the best part is they share the code for everyone.
&lt;br&gt;
&lt;br&gt;
In this post, I would just share the code snippets for the most common visualization tasks. Below are some of the most common plots to know about your data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Barplot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_bar(mapping = aes(x = as.factor(class))) +
  ggtitle(label = 'Plot of class of vehicle') +
  xlab('Class of vehicle') + ylab('Count')
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Scatterplot&lt;/h3&gt;
&lt;p&gt;The scatterplot is useful for displaying the relationship between two continuous variables, although it can also be used with one continuous and one categorical variable, or two categorical variables.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Linechart plot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_line(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;
I will add a tutorial on visualization in R using &lt;strong&gt;ggplot2&lt;/strong&gt; soon. Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>The filmy Secret Santa</title><link href="http://manishbarnwal.github.io/blog/2016/12/22/filmy_secret_santa/" rel="alternate"></link><updated>2016-12-22T07:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-12-22:blog/2016/12/22/filmy_secret_santa/</id><summary type="html">&lt;p&gt;Ever heard of secret Santa game? If not, you may not appreciate this article. Wait till you join the corporate world. But if you have, secret Santa may ring some bell.
&lt;br&gt;
&lt;br&gt;
Dialogues define our cinema. Ever imagined this twist. What if the Secret Santa gets introduced to Bollywood? Presenting, &lt;em&gt;Secret Santa meets Bollywood!&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;img alt="bade_bade" src="http://manishbarnwal.com/images/bade_bade.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Bade bade companies me aise chhote chhote tasks milte rehte hai&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="thappad" src="http://manishbarnwal.com/images/thappad.jpeg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Gift se dar nahi lagta saheb, task se lagta hai&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="rahul_naam" src="http://manishbarnwal.com/images/rahul_naam.jpeg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Secret Santa, naam toh suna hi hoga&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="sahensha" src="http://manishbarnwal.com/images/sahensha.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Rishte me toh hum tumhare Secret Santa lagte hai, naam hum hints ke through bataenge&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="lion" src="http://manishbarnwal.com/images/lion.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Saara office mujhe Secret Santa ke naam se jaanta hai&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="tareekh" src="http://manishbarnwal.com/images/tareekh.png" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Tasks pe tasks, tasks pe tasks, tasks pe tasks milti rahi hai magar hints na mila. Mile toh bas tasks&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="feke_paise" src="http://manishbarnwal.com/images/feke_paise.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Main aaj bhi feke hue hints nahi uthata&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="picture_abhi" src="http://manishbarnwal.com/images/picture_abhi.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Tasks abhi baaki hai mere dost&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="babu_moshai" src="http://manishbarnwal.com/images/babu_moshai.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Babu moshai, hints interesting hone chahiye lambe nahi&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="batman" src="http://manishbarnwal.com/images/batman.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;It's not the gift that matters...but the task given by your Secret Santa that defines you&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;img alt="winter" src="http://manishbarnwal.com/images/winter.jpeg" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;Secret Santa is coming&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;br&gt;
That is all for now, folks. &lt;/p&gt;
&lt;p&gt;Is your favorite Bollywood dialogue in this list? Drop down your Secret Santa dialogues in comments.   &lt;/p&gt;</summary><category term="Christmas"></category><category term="office"></category></entry><entry><title>Coldplay-Fix you | Ghar aa jaao</title><link href="http://manishbarnwal.github.io/blog/2016/11/24/ghar_aa_jaao/" rel="alternate"></link><updated>2016-11-24T07:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-11-24:blog/2016/11/24/ghar_aa_jaao/</id><summary type="html">&lt;p&gt;Last month on a mundane weekend, I was practicing this song - Fix you by Coldplay when my friend, Dipu came to the room. He stood there for some time to understand what I was trying to play. I am not that pro on guitar yet. Chances are you may want to throw me and my guitar out of your sight.
&lt;br&gt;
&lt;br&gt;
After some trouble, Dipu finally guessed the song and randomly uttered some lines in Hindi along the tune of the original song. And when he was bored, he left the room. I continued my attempt to make the song perfect on the guitar. It was then when I thought what if I write the complete song in Hindi along the tune of the original.
&lt;br&gt;
&lt;br&gt;
Without much thought, I picked up my notebook and started scribbling few lines. Later, Dipu also joined me in this attempt and below is the final song that we came up with.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Verse 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab har koshish pe ho jaati hai haar&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab milta nahi jo ho tera pyaar&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab saath nahin ho mera wo yaar&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Akela hoon main...&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Verse 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab lagta hai sab gaya hai bikhar&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab lagta hai suna, bin tere sheher&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jab aati nahi ho tum nazar&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sab kuch hai soo... oo... na&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ghar aa jaao tum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sath seh lenge hum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kal fir aayega&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Verse 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sadkon pe hoon, ab main khada&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aakhon me hain, bas chehra tera&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aur hoton pe hai, bas vaada tera&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tujh sang hamesha...aaa&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ghar aa jaao tum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sath seh lenge hum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kal fir aayega&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Bridge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aasoon bahein, tere chehre pe&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aur main kuch na kar paaya&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aasoon bahein, tere chehre pe&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aur main...&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;Aasoo bahein tere chehre pe&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aur main bebas hoon yahan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aasoo bahein tere chehre pe&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aur main...&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ghar aa jaao tum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sath seh lenge hum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kal fir aayega&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Chorus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ghar aa jaao tum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sath seh lenge hum&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kal fir aayega&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
Here is how the song sounds. Give it a listen &lt;a href="https://www.youtube.com/watch?v=8NDH2L5R27w"&gt;here&lt;/a&gt;.&lt;/p&gt;</summary><category term="Coldplay"></category></entry><entry><title>Random Forests explained intuitively</title><link href="http://manishbarnwal.github.io/blog/2016/10/18/random_forests_explained_intuitively/" rel="alternate"></link><updated>2016-10-18T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-18:blog/2016/10/18/random_forests_explained_intuitively/</id><summary type="html">&lt;p&gt;&lt;img src="http://manishbarnwal.com/images/random_forest_explained/rf_pic.jpg" alt="Photo of a forest"
width="1100" height="10" border="3"&gt;
&lt;br&gt;
&lt;br&gt;
Random Forests algorithm has always fascinated me. I like how this algorithm can be easily explained to anyone without much hassle. One quick example, I use very frequently to explain the working of random forests is the way a company has multiple rounds of interview to hire a candidate. Let me elaborate.
&lt;br&gt;
&lt;br&gt;
Say, you appeared for the position of Statistical analyst at WalmartLabs. Now like most of the companies, you don't just have one round of interview. You have multiple rounds of interviews. Each one of these interviews is chaired by independent panels. Each panel assesses the candidate separately and independently. Generally, even the questions asked in these interviews differ from each other. &lt;strong&gt;Randomness&lt;/strong&gt; is important here.
&lt;br&gt;
&lt;br&gt;
The other thing of utmost importance is &lt;strong&gt;diversity&lt;/strong&gt;. The reason we have a panel of interviews is that we assume a committee of people generally takes better decision than a single individual. Now this committee is not any collection of people. We make sure that the interview panel is a little diversified in terms of topics to be covered in each interview, the type of questions asked, and many other details. You don't go about asking same question in each round of interviews.
&lt;br&gt;
&lt;br&gt;
After having all the rounds of interviews, the final call whether to select or reject the candidate is based on the majority of the decision from each panel. If out of 5 panel of interviewers, 3 recommends a hire and two against a hire, we tend to go ahead with selecting the candidate. I hope you get the gist.
&lt;br&gt;
&lt;br&gt;
If you have heard about decision tree, then you are not very far from understanding what random forests are. There are two keywords here - random and forests. Let us first understand what forest means. Random forests is a collection of many decision trees. Instead of relying on single decision tree, you build many decision trees say 100 of them. And you know what a collection of trees is called - a forest. So you now understand why is it called forest.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Why is it called random then?&lt;/h3&gt;
&lt;p&gt;Say our dataset has 1,000 rows and 30 columns.&lt;/p&gt;
&lt;p&gt;There are two levels of randomness in this algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At row level&lt;/strong&gt;: Each of these decision trees gets a random sample of the training data (say 10%) i.e. each of these trees will be trained independently on 100 randomly chosen rows out of 1,000 rows of data. Keep in mind that each of these decision trees is getting trained on 100 randomly chosen rows from the dataset i.e they are different from each other in terms of predictions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At column level&lt;/strong&gt;: The second level of randomness is introduced at the column level. Not all the columns are passed into training each of the decision trees. Say we want only 10% of columns to be sent to each tree. This means a randomly selected 3 column will be sent to each tree. So for first decision tree, may be column C1, C2 and C4 were chosen. The next DT will have C4, C5, C10 as chosen columns and so on.
&lt;br&gt;
&lt;br&gt;
Let me draw an analogy now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let us now understand how interview selection process resembles a random forest algorithm. Each panel in the interview process is actually a decision tree. Each panel gives a result whether the candidate is a pass or fail and then a majority of these results is declared as final. Say there were 5 panels, 3 said yes and 2 said no. The final verdict will be yes.
&lt;br&gt;
&lt;br&gt;
Something similar happens in random forest as well. The results from each of the tree is taken and final result is declared accordingly. Voting and averaging is used to predict in case of classification and regression respectively.
&lt;br&gt;
&lt;br&gt;
With the advent of huge computational power at our disposal, we hardly think for even a second before we apply random forests. And very conveniently our predictions are made. Let us try to understand other aspects of this algorithm.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;When is a random forest a poor choice relative to other algorithms?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random forests doesn't train well on smaller datasets&lt;/strong&gt; as it fails to pick on the pattern. To simplify, say we know that 1 pen costs INR 1, 2 pens cost INR 2, 3 pens cost INR 6. In this case linear regression will easily estimate the cost of 4 pens but random forests will fail to come up with a good estimate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;There is a problem of interpretability with random forest.&lt;/strong&gt; You can't see or understand the relationship between the response and the independent variables. Understand that random forest is a predictive tool and not a descriptive tool. You get variable importance but this may not suffice in many analysis of interests where the objective might be to see the relationship between response and the independent features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;time taken to train random forests&lt;/strong&gt; may sometimes be too huge as you train multiple decision trees. Also, in case of categorical variable, the time complexity increases exponentially. For a categorical column with n levels, RF tries split at 2^n -1 points to find the maximal splitting point. However, with the power of H2O we can now train random forests pretty fast. You may want to read about H2O at &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2017/03/28/h2o_with_r/"&gt;H2O in R explained&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case of regression problem, &lt;strong&gt;the range of values response variable can take&lt;/strong&gt; is determine by the values already available in the training dataset. Unlike linear regression, decision trees and hence random forest can't take values outside the training data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What are the advantages of using random forest?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Since we are using multiple decision trees, &lt;strong&gt;the bias remains same as that of a single decision tree&lt;/strong&gt;. However, the variance decreases and thus we decrease the chances of overfitting. I have explained bias and variance intuitively at &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2017/02/08/the_curse_of_bias_and_variance/"&gt;The curse of bias and variance&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When all you care about is the predictions and &lt;strong&gt;want a quick and dirty way-out&lt;/strong&gt;, random forest comes to the rescue. You don't have to worry much about the assumptions of the model or linearity in the dataset.
&lt;br&gt;
&lt;br&gt;
I will add in the R code snippets as well to get an idea of how this is executed soon.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="random forest"></category><category term="ml"></category><category term="data-science"></category></entry><entry><title>Improve runtime of Random Forest in R</title><link href="http://manishbarnwal.github.io/blog/2016/10/13/improve_runtime_random_forest_r/" rel="alternate"></link><updated>2016-10-13T10:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-13:blog/2016/10/13/improve_runtime_random_forest_r/</id><summary type="html">&lt;p&gt;There are two ways one can write the code to train a random forest model in R.
Both the ways are listed below.
&lt;br&gt;
&lt;br&gt;
A normal and frequent way of writing the command to train the random forest model is something like this.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;rfModel &amp;lt;- randomForest(Survived~. , data = trainSample[, -c(6, 8, 9)])
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Notice the &lt;strong&gt;~&lt;/strong&gt; sign. We call this the formula way of writing.
&lt;br&gt;
&lt;br&gt;
Another way of writing the command to train the random forest model is shown below.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;rfModel &amp;lt;- randomForest(y=trainSample$Survived, x= trainSample[, -c(6, 8, 9)], data = trainSample)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Here we explicitly mention the y-variable and the x variables.
&lt;br&gt;
&lt;br&gt;
Recently, I was working on a huge dataset where the task was to predict a variable based on some 12 independent variables. The dataset had close to 1.3 million rows. I tried train the model using the first method i.e the formula way. Sadly, I had to kill the task as it was taking a lot of time.
&lt;br&gt;
&lt;br&gt;
It was then, I got to know that if you train the model using the second format command, the code runs relatively faster. When investigated further, I got to know that the reason for the difference in time is that the code for random forest is written in C and when one writes in the &lt;em&gt;formula format&lt;/em&gt;, the x and y variables are explicitly converted into proper format and this is what takes time. &lt;a href="http://stats.stackexchange.com/questions/116952/improving-randomforest-running-time"&gt;ref&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
Even the help page of randomForest in R says the same thing.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;?randomForest
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For large data sets, especially those with large number of variables, calling randomForest via the formula interface is not advised: There may be too much overhead in handling the formula.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
Look just before the 'Authors' section.&lt;/p&gt;
&lt;p&gt;The time difference is not significant for smaller datasets. However, for larger datasets I would suggest using the second format.&lt;/p&gt;
&lt;p&gt;To illustrate the time difference, I have taken a dataset having 2,23,874 rows and 6 columns.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;system.time( rfModel1 &lt;- randomForest(DepDelay~. , data = subset(df, select = c(1,2,3,4,5, 13))) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;user  system elapsed&lt;/p&gt;
&lt;p&gt;94.582  11.356 &lt;strong&gt;109.311&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;system.time( rfModel2 &lt;-randomForest(y=df$DepDelay, x=df[,c(1,2,3,4,5)], data=subset(df, select = c(1,2,3,4,5, 13))) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;user  system elapsed&lt;/p&gt;
&lt;p&gt;93.499  10.248 &lt;strong&gt;106.205&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This shows a time difference of 3 seconds. Note that all of these columns were numeric and if one takes categorical columns as well, the time difference would be more.
&lt;br&gt;
&lt;br&gt;
Also, this dataset is not huge. One would appreciate this more on larger datasets.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="life"></category><category term="learnings"></category><category term="thoughts"></category></entry><entry><title>How to install a package of a particular version in R</title><link href="http://manishbarnwal.github.io/blog/2016/10/05/install_a_package_particular_version_in_R/" rel="alternate"></link><updated>2016-10-05T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-05:blog/2016/10/05/install_a_package_particular_version_in_R/</id><summary type="html">&lt;p&gt;I recently tried installing &lt;em&gt;caret&lt;/em&gt; package in R using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.packages('caret', dependencies=T)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;Normally this installation of package works and I continue to work with the functions associated with the package.
When I tried including the package using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;library(caret)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;I got the following error.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;Error in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :there is no package called 'pbkrtest'
In addition: Warning message:package 'caret' was built under R version 3.2.5
Error: package or namespace load failed for 'caret'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R was not able to install this dependency package- &lt;em&gt;pbkrtest&lt;/em&gt;. So I tried installing it separately, again using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.package('pbkrtest', dependencies=T)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;This too didn't work. And the error again this time was
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;Error in loadNamespace(j &amp;lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : there is no package called ‘pbkrtest’ In addition: Warning message: package ‘pbkrtest’ was built under R version 3.2.3
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;So this particular package is not available for the R version that I am using.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;What is the way out?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We install an older version of this package.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you install an older version of some package in R?&lt;/h3&gt;
&lt;p&gt;I will go ahead with package-&lt;em&gt;pbkrtest&lt;/em&gt; to illustrate the steps.&lt;/p&gt;
&lt;p&gt;Open this &lt;a href="https://cran.r-project.org/src/contrib/Archive/pbkrtest"&gt;url&lt;/a&gt;. You will see various versions of the package listed there. Choose the version you want to install. Let's say I want to install the &lt;em&gt;pbkrtest_0.4-5.tar.gz&lt;/em&gt; version.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We now create a variable &lt;em&gt;packageUrl&lt;/em&gt; which contains all this information and assign it the url of the page.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;packageUrl&amp;lt;- "https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-5.tar.gz"
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;You then install this version of the package using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.packages(packageUrl, repos=NULL, type='source')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;And we are done!
&lt;br&gt;
&lt;br&gt;
Now if you have some other package to install, just replace the last word &lt;em&gt;pbkrtest&lt;/em&gt; with the package name you want to install and you will be able to see all the older versions of that package. For &lt;em&gt;Rcpp&lt;/em&gt; the link would be &lt;a href="https://cran.r-project.org/src/contrib/Archive/Rcpp"&gt;url&lt;/a&gt;. Choose the version you want to install and follow the steps stated above.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Manish&lt;/p&gt;</summary><category term="R"></category></entry><entry><title>Don’t introduce that bias in your child</title><link href="http://manishbarnwal.github.io/blog/2016/10/02/bias_in_child/" rel="alternate"></link><updated>2016-10-02T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-02:blog/2016/10/02/bias_in_child/</id><summary type="html">&lt;p&gt;A thought crossed my mind yesterday and just when it was about to get lost in the cloud of many thoughts that scatters in mind…I caught hold of it. I felt I had gathered the maximum out of this thought but I wanted to write about it so that the mind has to think more on it and may be, just may be, I will be able to know more.
Here’s the thought.
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every child learns something or the other from every human it spends time with.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
This statement sounds more promising when we think about it in this context. We tend to become more like our parents because we spend maximum time with them. Our characters and behaviors are imitated from theirs.
&lt;br&gt;
&lt;br&gt;
How does it happen that a &lt;em&gt;baniya&lt;/em&gt;’s son is almost always good in calculations and Maths? Why is that? Why doesn’t it tend to behave more like a musician?
&lt;br&gt;
&lt;br&gt;
Because, the child spends the maximum time with his parents. He sees the kind of work his father does or the daily chores that his mother gets involved it. He might not be paying much attention to all of this throughout his life. But the brain keeps on peeping through. The unconscious learning happens. Or what we call an &lt;strong&gt;unsupervised&lt;/strong&gt; learning in data-science.
&lt;br&gt;
&lt;br&gt;
In the long run, the best learning happens when it is not enforced. When the child learns on his own, at his own rate.
You want your child to be interested in reading? Make your home a library. Not so fancy or anything. Just surround the room and tables with books. Read it to your baby even when you think the baby doesn’t understand a single word. Let the unsupervised learning happen. 
&lt;br&gt;
&lt;br&gt;
Let the brain enjoy the feel of reading. The idea of living many lives through these books. Every character in these stories will add to the learning. And by the time the child grows, the brain is already hooked to this habit of reading. It has tasted the palatable food of reading. And it can’t survive without it now.&lt;/p&gt;
&lt;p&gt;Let’s say, you want him to be a musician. You already know what you need to do.
&lt;br&gt;
&lt;br&gt;
Do you realize, what have you done by reading out everyday to your child?
&lt;br&gt;
&lt;br&gt;
You have introduced a bias in your child’s character. A good bias, I would say. How interesting is that we can relate machine-learning and a child’s learning behaviors. And, we all know what happens to a biased model. It doesn’t work well. There is a &lt;a href="http://scott.fortmann-roe.com/docs/BiasVariance.html"&gt;trade-off&lt;/a&gt; between bias and variance you know.
&lt;br&gt;
&lt;br&gt;
While, it’s good to have the child spend time with good people and have a nurtured learning. The child would know about the right things. But don’t stop your child from interacting with someone who is not in your good books. Let us call that someone Asura.
&lt;br&gt;
&lt;br&gt;
Now Asura may not be on good terms with you and you may have legitimate reasons for that. Maybe you feel that Asura is immoral and unscrupulous. But you have seen only a part of Asura’s character. Maybe Asura is an excellent guitar player or a seasoned writer or he could be an expert in other domains. You don’t know.
&lt;br&gt;
&lt;br&gt;
Let his learning be flexible. Let the bad data be part of his life. Let him enjoy the outliers as well, be it a good or a bad one. With the bad outlier — Asura, make sure you enforce a little of your experience and monitor the child’s learning of good and the evil. Let the supervised learning come into play.
&lt;br&gt;
&lt;br&gt;
Don’t let your child have limited experiences of just the right things. Don’t limit the datasets in his life or the variance will be huge which again is not a good thing.
&lt;br&gt;
&lt;br&gt;
Let the learning be both a mix of unsupervised and supervised. A semi-supervised learning we call it in data-science.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="life"></category><category term="learnings"></category><category term="thoughts"></category></entry><entry><title>Shell commands come in handy for a data scientist</title><link href="http://manishbarnwal.github.io/blog/2016/09/30/shell_commands_for_data_scientist/" rel="alternate"></link><updated>2016-09-30T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-30:blog/2016/09/30/shell_commands_for_data_scientist/</id><summary type="html">&lt;p&gt;I am no expert of shell commands. I have been using them for quite some time and thought I give an attempt to list down the most common commands. I am writing these mostly from the perspective of a data-science guy. Let us get started.
&lt;br&gt;
&lt;br&gt;
I will use the file- ‘data.txt’ to illustrate these commands. ‘data.txt’ is a file having 200 rows and 8 columns. You can access the &lt;a href="https://github.com/ManishBarnwal/LearnNew/blob/master/LearnShellCommands/data.txt"&gt;data&lt;/a&gt; here.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;cat&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Throws the the contents of the entire file at your terminal
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;cat data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
We don’t want to bombard our terminal with the complete content of the file. Instead, if you want to have a complete look at the file, open the file in vim editor&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;vim&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Opens the file in the editor  &lt;br /&gt;
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;vim data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;head&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Gives the top 10 rows of the text file at your terminal
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;head data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;tail&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Gives the bottom 10 rows of the text file at your terminal
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;tail -n 2 data.txt -- This will give you the bottom 2 rows of the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;The piping operator&lt;/h4&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;cat data.txt | head
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Notice the &lt;strong&gt;|&lt;/strong&gt; operator. This is called pipe operator. Piping is a concept wherein you can perform a sequence of operations in a single command.
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;So what exactly is piping?&lt;/h3&gt;
&lt;p&gt;A pipe is a facility of the shell that makes it easy to chain together multiple commands. When used between two Unix commands, it means that output from the first command should become the input to the second command. Just read the &lt;strong&gt;|&lt;/strong&gt; in the command as &lt;em&gt;pass the data onto&lt;/em&gt;. More on this operator later in the post.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;wc&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;wc&lt;/strong&gt; is a fairly useful shell command that lets you count the number of lines(-l), words(-w) or characters(-c) in a given file
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wc -l data.txt -- gives you the number of lines in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wc -w data.txt -- gives you the number of words in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;wc -c data.txt -- gives you the number of characters in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;head -n 1 data.txt| wc -w -- gives you the number of columns in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;grep&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider ‘grep’ as a command to filter on the results you get. You may want to print all the lines in your file which have a particular phrase. Say for example you want to see people who are ‘Very Happy’. You simply pass this to grep command.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;grep 'Very Happy' data.txt | head
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let us say, we want to count the number of users who are ‘Not Happy’
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;grep 'Not Happy' data.txt | wc -l
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;sort&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to sort the data based on some column, say ‘Score’; ‘Score’ is the 3rd column in the file- data.txt&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;sort -t ',' -k 3 -n -r data.txt |head -5
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Explanation: -t is used to specify the delimiter; ‘,’ in this case.&lt;/p&gt;
&lt;p&gt;If the delimiter is ‘\t’, we don’t need to specify -t argument. Space is taken as delimiter by default.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k is used to specify the column based on which you want to sort the data; 3 in this case&lt;/li&gt;
&lt;li&gt;n is to specify that sorting is to be done numerically&lt;/li&gt;
&lt;li&gt;r is to imply that the sorting is descending
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;cut&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This command gives you only specific column. Say you want to see only the 4th column of the file.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 4 |head
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Explanation: ‘,’ is the delimiter. 4 is the column number that you want to see.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;uniq&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do not confuse this command for ‘unique’. It is slightly different. This &lt;strong&gt;removes sequential duplicates&lt;/strong&gt;.
So if you want to get unique values from a column, you need to first sort the data and then use this &lt;em&gt;uniq&lt;/em&gt; command in sequence.&lt;/p&gt;
&lt;p&gt;To get the unique of a column, say the 2nd column
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 2 |sort|uniq
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
This command could be used with argument -c to count the occurrence of these distinct values. Something like to count distinct in SQL.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 2 |sort|uniq-c
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;tr&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;strong&gt;tr&lt;/strong&gt; stands for translate&lt;/p&gt;
&lt;p&gt;‘Find and Replace’ function that we have in excel. Yes we have that in UNIX as well.
A typical use of this command that I use on regular basis is that I get the file from HIVE which are tab delimited. And say I want to convert it to ‘,’ delimited.&lt;/p&gt;
&lt;p&gt;You may also want to replace certain characters in file with something else using the &lt;em&gt;tr&lt;/em&gt; command.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cat data.txt | tr ',' '\t'  -- Changed ',' delimited to '\t' delimited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;strong&gt;Save to a new file or append to an existing file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;gt;&lt;/strong&gt; and &lt;strong&gt;&amp;gt;&amp;gt;&lt;/strong&gt; operator
Say you want to save the output of operations to some file. You use ‘&amp;gt;’ or ‘&amp;gt;&amp;gt;’ depending on whether you want it to be a new file or you want to append it to an existing file.
&lt;br&gt;
&lt;br&gt;
I will update this list as and when I see a command deserving enough to be in a data scientist’s toolbox.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="shell-commands"></category></entry><entry><title>ROC and AUC - The three lettered acronyms</title><link href="http://manishbarnwal.github.io/blog/2016/09/26/three_letter_acronym_roc_and_auc/" rel="alternate"></link><updated>2016-09-26T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-26:blog/2016/09/26/three_letter_acronym_roc_and_auc/</id><summary type="html">&lt;p&gt;I don't feel bad to confess this that &lt;em&gt;ROC curve&lt;/em&gt;, &lt;em&gt;AUC&lt;/em&gt;, &lt;em&gt;True-positive&lt;/em&gt; and related terms took quite some time for me to understand. If today I contemplate on the reasons why I found this topic confusing. The first would be there are not many resources that explains intuitively what these mean. They just jump to the terms and the mathematical formula for them. The second being I had not used them even in my project work. You see the project work is never enough for all your learnings.
&lt;br&gt;
&lt;br&gt;
In this post, I will try to explain my understanding both intuitively and mathematically.&lt;/p&gt;
&lt;p&gt;I will illustrate this concept with the help of an example.
&lt;br&gt;
&lt;br&gt;
There is a bank say &lt;em&gt;SBI&lt;/em&gt; that wants to understand which of its future customers would default on a loan granted by the bank. The bank would already have historical data from the past years that says how many of the customers have defaulted, what type of customer were they and many other information about the past loans.
&lt;br&gt;
&lt;br&gt;
It is very rare that customers default. To give an example, say the bank has data having 1000 rows that contains information like &lt;em&gt;age&lt;/em&gt;, &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;income&lt;/em&gt;, &lt;em&gt;marital-status&lt;/em&gt;, other related columns and then a variable named &lt;em&gt;default&lt;/em&gt; that says whether the customer has defaulted or not.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, very few customers default on their loans. So a realistic example would be say 100 customers defaulting out of a total of 1000 customers.
&lt;br&gt;
&lt;br&gt;
The bank is interested in knowing if a customer would default or not. This is a typical binary classification problem. The bank would really be interested in the customers who are likely to default. Let us now try to understand terms like &lt;em&gt;True-positive&lt;/em&gt;, &lt;em&gt;True-negative&lt;/em&gt;, &lt;em&gt;False-positive&lt;/em&gt; and other related terms.
&lt;br&gt;
&lt;br&gt;
There are two levels in the &lt;em&gt;default&lt;/em&gt; variable that we are trying to predict-&lt;strong&gt;default&lt;/strong&gt; and &lt;strong&gt;not-default&lt;/strong&gt;. One has to define first whether &lt;strong&gt;default&lt;/strong&gt; will be treated as positive or &lt;strong&gt;not-default&lt;/strong&gt; as positive. It is just a convention. Normally, the class of interest is treated as positive. So in the bank's case, what do you think we should take as positive? You might have guessed it correctly, we will treat &lt;strong&gt;default&lt;/strong&gt; as positive and &lt;strong&gt;not-default&lt;/strong&gt; as negative.
&lt;br&gt;
&lt;br&gt;
We train a binary classification model on this dataset having 1000 rows. The predictions that the model makes for the training data will either be correct or incorrect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for incorrect predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was negative i.e classifying a customer as default when in actual he is not-default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was positive i.e classifying a customer as not-default when in actual he is default&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for correct predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was positive i.e correctly classifying default as default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was negative i.e correctly classifying not-default as not-default
&lt;br&gt;
&lt;br&gt;
Now, FP, FN are incorrect predictions (notice False in the name) as the name suggests and TP, TN are the correct predictions. Don't be in a hurry here. Take some time to digest these 2 lettered acronyms. Read them loud. Take a notebook and write them down on your own.
&lt;br&gt;
&lt;br&gt;
Once we are comfortable with these terms we will discuss about something called confusion-matrix. Don't get confused yet. If you understood TP, TN, FP, FN then confusion-matrix is just a matrix having these values. The diagonal elements contain the count of correct predictions (TP, TN) whereas the off-diagonal contain the count of incorrect predictions (FP, FN). The rows are the predicted and the columns are the actual.
This looks something like this.
&lt;Insert a picture&gt;
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why we need TPR, FPR if we already have mis-classification error?&lt;/h3&gt;
&lt;p&gt;The data I describe above is a typical case of imbalanced data wherein one of the class is having majority of observations (90% non-defaulters (negatives) in our data) and the remaining class is a minority (only 10% defaulters (positives)). In such cases, the predictions on new dataset will be skewed towards negatives i.e the model will classify a lot of defaulters (positives) into negatives. The bank can't afford to have such predictions. The bank wants to know for sure the defaulters (positives). Imagine, the loss to bank if the model classify a probable defaulter (positive) to non-defaulter.
&lt;br&gt;
&lt;br&gt;
In such cases, accuracy corresponding to mis-classification alone is not acceptable. The bank would be more interested in correctly classifying the positives into positive i.e the bank wants to classify the defaulters into defaulters without fail.
&lt;br&gt;
&lt;br&gt;
Comes into picture &lt;em&gt;TPR&lt;/em&gt;, &lt;em&gt;TNR&lt;/em&gt;, &lt;em&gt;FPR&lt;/em&gt;, &lt;em&gt;FNR&lt;/em&gt;. These 3 lettered acronyms are nothing but the rates of TP, TN, FP and FN respectively.
Below is the formula. To digest the formula, let's move to our data having 1000 rows - 100 defaulters (positives) and 900 non-defaulters(negatives). Suppose we employed a logistic regression that classified 80 defaulters correctly and incorrectly classified 90 non-defaulters as defaulters.
TPR = 80 / 100 = 0.8
FPR = 90/ 900 = 0.1
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How does the TPR and FPR gets calculated?&lt;/h3&gt;
&lt;p&gt;Whenever you do any classification, the model always gives you probabilities of each observation getting classified in each of the classes.
Based on what cut-off you choose, you will get different predictions for the data and hence different TPR and FPR overall. You can choose whatever probability cut-off between [0, 1] and you will get different tuples of TPR and FPR.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TPR and FPR are be generated for each of the probability value one chooses. And these values are then plotted on an ROC curve.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
One can plot these tuples (probability, FPR, TPR) on a graph. You know what this graph is called? ROC-Receiver Operating Characteristics.
There is a trade-off between TPR and FPR. Depending on the requirement one can choose the probability cut-off that best fulfills their purpose. For instance, in the bank's case, the bank wants to not miss a single defaulter(positives) i.e the bank wants a higher TPR. The ROC curve looks something like this.&lt;/p&gt;
&lt;p&gt;Alright, all this is clear to me.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What about AUC?&lt;/h3&gt;
&lt;p&gt;AUC is nothing but the area under curve of ROC curve. Let's say we built a logistic regression model that gave us the probabilities for each row. Now we try with probability cut-offs from 0.1 to 1.0 with step size of 0.1 i.e we would have 10 probabilites to try with and corresponding to each of the 10 values we would have corresponding (FPR, TPR). If we plot these values on a graph we would get a graph having 10 points. This 10 point graph is what we call an ROC curve and the area under this graph is called AUC.
&lt;br&gt;
&lt;br&gt;
The AUC is a common evaluation metric for binary classification problems. Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1: if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5.
&lt;br&gt;
&lt;br&gt;
One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets.
&lt;br&gt;
&lt;br&gt;
The larger the area the better. If we have to choose between two classifiers having different AUCs, we choose the one having larger AUC.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you decide what probability cut-off should you choose to classify them into either of the classes?&lt;/h3&gt;
&lt;p&gt;Some say they take a cut-off of 0.5 i.e observation having probability greater than 0.5 will be classified as positive or else negative. Do you see the problem in here? Try thinking.
&lt;br&gt;
&lt;br&gt;
Come into picture ROC curve. Look at the ROC curve and depending on what value of TPR or FPR you want from your model, you take probability corresponding to that point.&lt;/p&gt;
&lt;p&gt;ROC, AUC can easily be plotted and calculated using modern analytical tools like R or Python. But I would suggest for better understanding of this topic, try writing your own code.
&lt;br&gt;
&lt;br&gt;
I will make an attempt of the same soon and will share it on this space. Keep learning and sharing.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="roc"></category><category term="auc"></category></entry><entry><title>Vim/Vi editor shortcuts</title><link href="http://manishbarnwal.github.io/blog/2016/09/22/vim_shortcuts/" rel="alternate"></link><updated>2016-09-22T04:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-22:blog/2016/09/22/vim_shortcuts/</id><summary type="html">&lt;p&gt;Repetitive tasks should be done using as many shortcuts as possible. You are not doing  anything new and hence not even an extra minute should be spent on doing the same. This post refers to the shortcuts that come in handy when working on the vi/vim editor.&lt;/p&gt;
&lt;p&gt;This is not an exhaustive list. These are the ones I use frequently. Feel free to comment down your favorite shortcuts.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Navigation keys&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves cursor to the start of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves cursor to the end of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;w&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves forward one word&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;b&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves backward one word&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;G&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves to the end of the file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1 + G&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves to the beginning of file
 &lt;br&gt;
 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Delete text&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;dw&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes a word ahead of the cursor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;db&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes a word behind the cursor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;d0&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line till beginning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;d$&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line till the end&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dd&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10dd&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the following 5 lines&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dG&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes till the end of the file
 &lt;br&gt;
 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Undo/Redo operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;u&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Undo the last operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + r&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Redo the last undo
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Search and Replace keys&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;/search_text&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds 'search_text' in file going forward&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;?search_text&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds 'search_text' in file going backward&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds the next occurrence of 'search_text'&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds the previous occurrence of 'search_text'&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;:%s/replace_what/replace_with&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Replaces first occurrence&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:%s/replace_what/replace_with/g&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Replaces all occurrences globally&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:%s/replace_what/replace_with/c&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Asks for confirmation
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Save and quit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;:q!&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Force quit without saving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:wq&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Saves the changes made to the document&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:wq!&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Forcefully saves the changes made to the document&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:w new_file_name&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Saves the file to a new file named new_file_name
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Command line shortcuts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + a&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Brings to the beginning of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + e&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Brings to the end of the line&lt;/li&gt;
&lt;/ul&gt;</summary><category term="vi/vim"></category></entry><entry><title>Hadoop Streaming</title><link href="http://manishbarnwal.github.io/blog/2016/08/29/hadoop_streaming/" rel="alternate"></link><updated>2016-08-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-08-29:blog/2016/08/29/hadoop_streaming/</id><summary type="html">&lt;p&gt;A few days ago, I had written a post on &lt;strong&gt;The Big Data Problem&lt;/strong&gt; which attempted to understand why we need big data and what the fuss is all about. You may want to read it &lt;a href="http://manishbarnwal.com/blog/2016/06/29/big_data_problem/"&gt;here&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;
Having understood why we need big data, let’s understand how we can go about analyzing the same. What is the way out to do analysis on big data? The solution is Streaming…Hadoop Streaming. &lt;em&gt;James Bond style&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
To recount a personal experience, I was faced with the following Data Analysis task — Scaling a particular analytics technique on retail data from one store to all stores across the US. Sounds interesting so far, doesn’t it? The catch however, was that the program took 2 days just to compute the results for 1 department.
&lt;br&gt;
&lt;br&gt;
Imagine, the time it would take if we were restricted to a single machine/Rstudio.
&lt;br&gt;
&lt;br&gt;
Let’s understand the gravity of the problem and get an idea of a rough time estimate. 
So for one department the time taken was two days. Let us assume, each store has close to 100 departments and we have a total of 5000 stores. So the total time taken for all of the stores would be roughly 2 X 100 X 5000 = 1 million days = 2740 years. This is just a rough estimate assuming a linear relationship between the number of departments and the time taken.
&lt;br&gt;
&lt;br&gt;
Obviously, using RStudio stand alone was not a feasible solution. We had at our disposal Hadoop cluster. We wanted to see if we could exploit the number of machines in our cluster to solve this Herculean challenge of the Big Data world.
&lt;br&gt;
&lt;br&gt;
A fresh college graduate, I was a complete stranger to the world of Hadoop and out of anxiety, I purchased a copy of &lt;em&gt;Hadoop-The definitive guide&lt;/em&gt;. I started going through the chapters. It was in no time that I realized this book was meant for someone with an understanding of Java. Unfortunately, I was not one of them. Saddened and almost defeated. What other choice do I have? I came across this wonderful concept — Hadoop streaming.
&lt;br&gt;
&lt;br&gt;
Wow! This looks like it could solve the problem. I started reading more about it on the internet. There were not many resources on this technique. Fortunately, a colleague of mine had worked on Hadoop Streaming earlier and with his help we were able to accomplish the task successfully.
&lt;br&gt;
&lt;br&gt;
Now that you have the context, let me try to answer some basic technical questions.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;So what exactly is Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;Hadoop streaming — ‘hadoop’ and ‘streaming’. We use the availability of machines in the cluster to process data in parallel. Let me elaborate. When we do our analysis on Rstudio, we just have one machine (our laptop). Now imagine we have 100s of such machines in our cluster. We can take advantage of this and distribute our input data in such a way that each machine gets some portion of your input data and they are processed in parallel.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Now, you may ask what is streaming here.&lt;/h3&gt;
&lt;p&gt;The input data is sent to each of the machines in the cluster via stdin() (standard input) and the analyzed output is thrown at stdout() (standard output). Your input data is streamed via stdin and the output gets flushed out to stdout.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What languages can I use for Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;You can use any scripting language — R, Python, Ruby. Care should be taken to choose your scripting language. The analysis you are planning to do on your data should decide your choice of language. Say, we want to do a forecasting of sales at item level for a retail store. We know that R is the suitable choice to use when it comes to do forecasting as we have well-developed packages in R which is not the case with Ruby. So do put some thought into choosing the language.
&lt;br&gt;
&lt;br&gt;
I hope we are now clear on the basics of Hadoop Streaming and its benefits.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hadoop Streaming is not the answer to all your Big Data analysis problems. It can be used only for cases where each machine can independently perform its analysis with a small portion of input data that is fed to it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming can be used&lt;/h3&gt;
&lt;p&gt;Sales forecasting at item level. Say, we have weekly data for 2 years at item level. And say there are 10,000 items for which we want to do the forecasting. Each item has close to 104 rows (2 years weekly data). So our input data has close to 104 X 10,000 = 1,040,000 ~ 1 million rows. 
&lt;br&gt;
&lt;br&gt;
Assume we have 100 machines in our cluster. What we do next is pretty intuitive. We distribute our data such that each machine receives 1 item at a time, and once it is done processing that item, we send the next item to it. So, in a single go, we will have 100 items processed across the cluster. In, the next go, again more 100 items will be processed and this goes on until all the 10,000 items are processed.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming won’t work&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/K-means_clustering"&gt;Clustering&lt;/a&gt;. In clustering we need to find the hidden patterns that are there in the complete dataset. A smaller portion of the dataset can’t be sent to a machine to do the analysis because the purpose will not be served.
&lt;br&gt;
&lt;br&gt;
I think this is worth mentioning.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In Hadoop Streaming, you are not writing an &lt;strong&gt;org.apache.hadoop.mapred.Mapper class&lt;/strong&gt;! This is just a simple script that reads rows from stdin (columns separated by ‘\t’ or any delimiter) and should write rows to stdout (again, columns separated by ‘\t’ or other delimiter). It’s probably worth mentioning this again but you shouldn’t be thinking in traditional map-reduce &lt;strong&gt;Key Value&lt;/strong&gt; terms, you need to think about columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
You can write your script in any language you want, but it needs to be available on all machines in the cluster. Any easy way to do this is to take advantage of the Hadoop &lt;strong&gt;distributed cache support&lt;/strong&gt;, and just use add file /path/to/script within hive. The script will then be distributed and can be run as just ./script (assuming it is executable)
Enough theoretical stuff. The interesting part is the code framework of mapper and reducer that I will explain in the next post. 
&lt;br&gt;
&lt;br&gt;
I hope you feel more educated on big data after reading this. Please leave a comment if you have questions/insights. I will reply as soon as possible.&lt;/p&gt;</summary><category term="big data"></category><category term="analytics"></category><category term="hadoop"></category></entry><entry><title>When R package is not available across the cluster</title><link href="http://manishbarnwal.github.io/blog/2016/08/02/r_packages_not_in_cluster/" rel="alternate"></link><updated>2016-08-02T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-08-02:blog/2016/08/02/r_packages_not_in_cluster/</id><summary type="html">&lt;p&gt;When deploying R codes across the cluster, many a times the reason for the failure of the task is unavailability of a particular package across all nodes of the cluster. We wait for someone to get the package installed across all the nodes. This may take some days. Do we wait for them? &lt;em&gt;Naah!&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
Presenting a temporary solution that one of my colleague came up with. I have used this technique and this works smoothly.
&lt;br&gt;
&lt;br&gt;
The following are the steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the package you require on one of the edge nodes into a local directory&lt;ul&gt;
&lt;li&gt;Create a local directory. Let's say our directory name is &lt;em&gt;rPackages&lt;/em&gt;&lt;br /&gt;
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;mkdir rPackages
    &lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Install the required package, say 'randomForest' in the directory just created
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="r"&gt;install.packages('randomForest', repos=’repo_name', lib='rPackages/')
    Note that you need to choose the appropriate repo_name. The one that your company allows.
    &lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;
2. Check if you can load the package from this local directory
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;library(randomForest, lib.loc='rPackages/')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
3. Create zip file of “dir_location” using command
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;zip -r rPackages.zip rPackages/
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
4. Add this zip file in your HIVE hql (or anything else)
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;add file rPackages.zip;
Don’t forget the semicolon
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
5. Unzip the file inside R script i.e. each reducer will have &lt;em&gt;rPackages&lt;/em&gt; directory now
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;unzip('rPackages.zip', overwrite=TRUE)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
6. Load the package now
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;library(randomForest, lib.loc='rPackages/')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And you’re done! Remember, you have to build the package where you want to use, because built packages are OS dependent.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="big data"></category><category term="R"></category></entry><entry><title>The most inspiring talk I ever attended live</title><link href="http://manishbarnwal.github.io/blog/2016/07/26/the_most_inspiring_talk_live/" rel="alternate"></link><updated>2016-07-26T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-07-26:blog/2016/07/26/the_most_inspiring_talk_live/</id><summary type="html">&lt;p&gt;Last week &lt;a href="https://www.linkedin.com/company/walmartlabs-india"&gt;WalmartLabs&lt;/a&gt; India was one of the sponsors of IIT Bombay’s alumni meet and I along with few of my colleagues got a chance to attend the meet-up. I didn’t have much idea about the speakers who were going to present. All I knew was, we wanted to get some good talent for our company and we had set up a booth where we were presenting the work we do at WalmartLabs.
&lt;br&gt;
&lt;br&gt;
Of all the speakers that presented their talk, one was mesmerizing. I was impressed by this speaker. The way he confidently took the stage. Stood casually by the dais. He didn’t carry any presentation with him unlike the other speakers. There was a charm and calmness in his presence. It was as if he knew that we were eagerly waiting to hear him out.
&lt;br&gt;
&lt;br&gt;
He introduced himself as &lt;a href="https://www.linkedin.com/in/ravi-venkatesan-ba15b820"&gt;Ravi Venkatesan&lt;/a&gt;. He gave a brief background about his college, the works he had done, and the great experiences he had in his career of 30 years. Nothing unusual or impressive so far. But when he mentioned the different fields in which he had done his work, I was awestruck. I was not able to digest the fact that he had worked as someone who established Microsoft in India during its early years after having manufactured engines in his previous job. See the difference in the two kinds of work.
&lt;br&gt;
&lt;br&gt;
Many had asked him not to take this job at Microsoft. That it was not safe to make such a change from working on engines to IT sector. Even his most trusted mentors were not in favor of this change. But this guy wanted to wear this new hat and take this risk. Not to prove anyone wrong but to ensure that later in his life he never had the guilt of not trying.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;People generally don’t try new things because they fear the idea of failing. The moment one conquers this feeling and see beyond…all one can see is opportunity, learning, and experience.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
He was in no hurry to finish up his talk. Slowly, he went on describing his journey of ups-and-downs. His talk was mostly on advices on career.
I remember him mentioning this.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The age of 40 is a magical year. Once you reach this, you start getting serious. You have in front of you, the 40 years you have already lived, you have the list of things you wanted to accomplish, and the things you have actually conquered.&lt;/p&gt;
&lt;p&gt;The next thought that comes to your mind is you don’t have much time left. A few more years and your career will come to a halt. In the next few years, you would like to get the most out of your life. Do not wait to reach this magical year.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Do things you want to do NOW. Get out of your comfort zone. Take risks. His words were powerful and convincing.
Another anecdote from his life. He had applied for Harvard just after graduating from IIT Bombay. Sadly, he was rejected. Dejected and shattered, he called upon the office asking why was he rejected?
&lt;br&gt;
&lt;br&gt;
The person on the other side replied — &lt;em&gt;you don’t have anything different&lt;/em&gt;. Your grades are good and all but how are you different? He had no answer.
Few years passed by. Ravi liked reading about various things. He started writing about these things. His interest towards writing increased exponentially and fortunately at the age of 26, he became the youngest man to have his article published in the &lt;strong&gt;Harvard Business Review&lt;/strong&gt;. Obviously, he got admitted to Harvard the next time.
&lt;br&gt;
&lt;br&gt;
My eyes glistened with awe.
Here was a guy who had not given up. He continued doing things he liked and gave an answer to how he was different from others.
&lt;br&gt;
&lt;br&gt;
The auditorium echoed with claps from the audience. The speaker opened up for questions from the audience which he patiently answered. There were many in the audience who had more questions to be answered. Time was running short and he had to leave the stage for the next speaker to come.
&lt;br&gt;
&lt;br&gt;
Did you find the post useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="life"></category><category term="career"></category></entry><entry><title>Things I have learned (So Far)</title><link href="http://manishbarnwal.github.io/blog/2016/07/15/things_learned_so_far/" rel="alternate"></link><updated>2016-07-15T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-07-15:blog/2016/07/15/things_learned_so_far/</id><summary type="html">&lt;p&gt;Today when I opened my blog on Quora, I saw a draft of this post that I had written some six months post to my first job. The learnings I had moving from college to corporate.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Things I have learned (So Far)&lt;/strong&gt;
&lt;br&gt;
&lt;br&gt;
I graduated from my college almost six months back. The last six months have been exciting in many ways. The transition from being a college student to a full-time corporate employee has been enlightening. In this post I will talk about some of the things which I've learnt... so far.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Travel-On&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Just few days after my college, I traveled for some 20 days and those days were some of the good days I'll always cheer. Uttarakhand is beautiful.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Talk to people&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;People are interesting. Every person has something interesting to share, something that makes you feel happy. Try to live the happiness of others by experiencing how their happiness makes you feel. You would be able to relate something from their reasons for happiness.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Trekkings&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, I won’t say I am a hardcore trekker- someone how has got a all his gadgets ready to hop on for next trek. But I’ve trekked around Bangalore a few times. And man! You get to know so much about people. If you want to know about a person, know some of his truest qualities, go on a long trek with him.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Smile and laugh&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Office gets boring sometimes, the work gets bland. You get the feeling right? You don’t hate your job but sometimes you just feel &lt;em&gt;aaj kaam na ho paaega yaar office me&lt;/em&gt; &lt;I don’t feel like working today&gt;, that time, a thought of only a few of your office friends make you come to the office. Those people are mostly who smile and talk to people.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;The&lt;/strong&gt; &lt;em&gt;Guitarest&lt;/em&gt; &lt;strong&gt;oh sorry I meant The Greatest!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Guitar is happiness. Think about your happiness. What makes you happy? There would be some activity in your daily life which you truly adore doing. Know the feeling I’m talking about. Guitar is that feeling to me.
&lt;br&gt;
&lt;br&gt;
More on this soon.&lt;/p&gt;</summary><category term="life"></category><category term="life lessons"></category></entry><entry><title>The Big Data Problem</title><link href="http://manishbarnwal.github.io/blog/2016/06/29/big_data_problem/" rel="alternate"></link><updated>2016-06-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-29:blog/2016/06/29/big_data_problem/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Big data&lt;/strong&gt; has become a sensation these days. Anyone and everyone wants to use this in their discussions. When I was still in my college and preparing for campus placements, I had attended almost all the pre-placement talks that companies gave to its prospective candidates.
&lt;br&gt;
&lt;br&gt;
American Express was one such company that had talked extensively about big data and hadoop in their presentation. I remember clearly, the blank faces that all of us had. We were not able to follow a single term. &lt;strong&gt;Hadoop&lt;/strong&gt;, &lt;strong&gt;clusters&lt;/strong&gt;, &lt;strong&gt;big data&lt;/strong&gt;, &lt;strong&gt;distributed environment&lt;/strong&gt; and many other terms were just bouncers for me. I did try to google them up later but these terms were still alien to me.
&lt;br&gt;
&lt;br&gt;
I have worked at WalmartLabs for close to 2 years now and the work has mostly been on the big data side. Walmart is one of the largest companies when it comes to capturing the transactions data that we have on daily basis both at the stores and at the e-commerce. Having worked on the big data technologies a little, I felt I would give it a try to make big data and related questions a little easy to understand.
&lt;br&gt;
&lt;br&gt;
Following are few of the questions that I will try to explain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data? And why should I care about it?&lt;/li&gt;
&lt;li&gt;What is the problem with a single laptop?&lt;/li&gt;
&lt;li&gt;What is a cluster?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The data is distributed across the cluster&lt;/em&gt;. What is meant by this?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is Hadoop and Spark?
&lt;br&gt;
&lt;br&gt;
Everyone is generating data. Data is the side-product of any work we do. &lt;em&gt;Just like smoke is emitted when things burn, data is created when machines work or interact with each other&lt;/em&gt;. These machines used to emit data in the past as well, but we were not advanced enough to collect the data. The technologies weren’t capable of capturing these.
&lt;br&gt;
&lt;br&gt;
Today, every other thing is emitting data. Be it the signals from your refrigerator, the air-conditioners in your room, the cars on the street, the videos that you like or dislike on YouTube, your transactions at the nearby Super-market, the posts you make on Facebook. These are all emissions of your day-to-day work.
&lt;br&gt;
&lt;br&gt;
Companies and analysts want to understand you and your behavior using the data you generate. To give you an idea of how much data is generated, let’s look at the ridiculous amount of data some of the big giants produce on daily basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Facebook’s daily logs are more than &lt;strong&gt;60 terabytes&lt;/strong&gt; every day.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Google’s web index is more than &lt;strong&gt;10 petabytes&lt;/strong&gt; of information.&lt;/li&gt;
&lt;li&gt;Millions of customers visit Walmart stores on daily basis. Imagine the &lt;strong&gt;size of these transactions data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do we need this data? Of course!&lt;/p&gt;
&lt;p&gt;I strongly believe this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every data has a story. With proper analysis and techniques, we can get a lot from data. Now, there will be times when you won’t get anything insightful from it. I consider this also good. Maybe, there is something wrong with the way we are collecting the data and we need to have a look at it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
So far we know that every other thing is puking data. And they are puking in bulk. To get something good out of these pukes, we need to store them somewhere, do our analysis on them and get insights. How do we go about it?&lt;/p&gt;
&lt;p&gt;Storage of this data has never been a problem. The disks are relatively cheap. A terabyte of disk costs ~ $35. So storing them on physical disks is not an issue. What is the problem then?&lt;/p&gt;
&lt;p&gt;Do you have an idea how fast the data can be read from a disk? It is still in Mbs/sec. Let us assume a speed of 100 Mbs/sec. With this speed, the time taken to read 1 terabyte of data would be ~ 3 hours! Below is the calculation for 3 hours.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1 terabytes = 1000,000 Mb. So the total time ~ 1000,000/100 ~ 10000 seconds ~ 10000/3600 ~ 3 hours&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3 hours is too huge a time to read 1 terabyte of data given the amount of data giants like Facebook generates on daily basis. Also, a single machine won’t be able to handle this much data. So what is the solution?
&lt;br&gt;
&lt;br&gt;
What if we could distribute the data across multiple machines? Yes, that sounds promising. The data that is big enough to handle by one machine can be distributed across multiple machines such that each machine has some portion of the actual data. Now these machines are interconnected with each other and so there is no issue of interactions between these machines. And that is exactly what a cluster is.
&lt;br&gt;
&lt;br&gt;
A &lt;em&gt;cluster&lt;/em&gt; is a combination of many machines connected to each other. Think of it like this. There is a process by which, my computer and your computer can be connected to each other and we can have a cluster with two nodes or machines.&lt;/p&gt;
&lt;p&gt;Now, that you have your machines connected with each other in the cluster, you can take advantage of the computation power of each of the machines. And thus, the task which one couldn’t even think of accomplishing by a single machine can now be very easily be done by a cluster of machines.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is hadoop and Spark? And how are they different?&lt;/h3&gt;
&lt;p&gt;Hadoop is a framework that supports the storage and processing of large data sets in a distributed computing environment.It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs.&lt;/p&gt;
&lt;p&gt;Spark is a powerful analytics engine. It is a fast and general engine for large-scale data processing.&lt;/p&gt;
&lt;p&gt;Hadoop provides both the data storage and processing power whereas Spark is meant for doing analysis and processing of big data.&lt;/p&gt;
&lt;p&gt;One thing to notice is, Spark is about 10 to 100 times faster than the Hadoop MapReduce framework by making use of in-memory processing compared to persistence storage used by Hadoop.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Spark is a Swiss army knife of analytics world&lt;/em&gt;. There are various APIs- Python, R, Scala thorough which one can interact with the Spark framework. For machine learning algorithms, there is MlLib which can be used to perform some common analysis like regression, K-means clustering, and classification.
&lt;br&gt;
&lt;br&gt;
We will talk more about Spark in future posts. I am exploring it rigorously and plan to write out my understanding of it soon.&lt;/p&gt;
&lt;p&gt;I hope you enjoyed reading this post and feel a little more familiar with big data now. Hit the share button if you would like your friends to read this.&lt;/p&gt;</summary><category term="big data"></category><category term="analytics"></category><category term="hadoop"></category><category term="spark"></category></entry><entry><title>My first attempt at public speaking</title><link href="http://manishbarnwal.github.io/blog/2016/06/27/first_attempt_at_public%20speaking/" rel="alternate"></link><updated>2016-06-27T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-27:blog/2016/06/27/first_attempt_at_public speaking/</id><summary type="html">&lt;p&gt;Having graduated from college, it was just 6 months into my new job that I was asked to give a tech-talk on &lt;em&gt;Introduction to Data Science&lt;/em&gt;. Imagine the thrill and fright of a fresher when he is presented with a situation like this. The first thing I asked my manager was — how many people are we expecting in the audience? Notice the fright in the question. I was scared that I might screw up in front of a large audience. I may not be able to present the talk eloquently, the audience may laugh at me and all those crappy thoughts that quickly clouds your mind with fear and uncertainty.
&lt;br&gt;
&lt;br&gt;
Without much thought and ignoring the above questions, I straight away agreed to present the talk. &lt;strong&gt;The plan was simple&lt;/strong&gt;. Commit to the task and then learn how to do it. In a few days, after a lot many iterations I had the presentation ready with me. I had not prepared any script or dialogues for the talk. The reason being, I wanted to give an impromptu talk. Days passed and I got busy in routine office work and meeting project deadlines.
&lt;br&gt;
&lt;br&gt;
Writing and public-speaking was something that has always fascinated me. I had read many a posts on effective public-speaking. Now was the time to apply whatever I had read about it.
&lt;br&gt;
&lt;br&gt;
The day had come when I had to give the tech-talk. That morning, I woke up a little early and I looked at a few videos on dealing with the stage-fear. I found this video in particular to be helpful.
&lt;br&gt;
&lt;br&gt;
It’s not that I had not practiced at all. We had a few dry runs. Frankly, I didn’t had the exact lines that I intended to say. I had just planned the overall flow of the talk. As I had read on various blogs on public-speaking, I reached the conference room a little early, walked down the room confidently, talked to some of my colleagues about random topics so as to divert the attention and calm the nerves. 
&lt;br&gt;
&lt;br&gt;
Within a few minutes, the room got packed with folks from various teams. I hadn’t expected these many people to turn around. Nevertheless, I was happy and confident. I kept saying this to myself — &lt;em&gt;you are going to rock today! So many people have come just to listen to you. You know this stuff. This is your bread and butter&lt;/em&gt;. I took a few deep breaths and started the talk. 
&lt;br&gt;
&lt;br&gt;
After first few minutes of the talk, I started reading some of the faces in the audience, trying to understand if they are following what was being spoken. A good proxy for this include people asking questions, nodding to your statements in agreement, and making notes. I could see the audience doing these. So the feedback was good and I continued with the talk.
&lt;br&gt;
&lt;br&gt;
It was not all that easy. I stammered in between, fell short of the right words, the throat felt thirsty, some of the faces in audience looked scary but somehow the confidence in me was at peak. &lt;strong&gt;I was happy talking about something that I enjoyed doing&lt;/strong&gt;.
&lt;br&gt;
&lt;br&gt;
It has been almost a year since I had presented this talk. Now when I think about it, I feel good. I feel accomplished. I know I could have screwed up badly. I had to start somewhere. And today I am happy that I did take that chance.
&lt;br&gt;
&lt;br&gt;
Next time, don’t let that chance pass away. Take that risk, prepare well and make it your first attempt at public speaking. We all need to start somewhere.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="life"></category><category term="thoughts"></category><category term="nostalgia"></category></entry><entry><title>Understanding HIVE for data science people</title><link href="http://manishbarnwal.github.io/blog/2016/06/22/understanding_hive_data_science/" rel="alternate"></link><updated>2016-06-22T04:43:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-22:blog/2016/06/22/understanding_hive_data_science/</id><summary type="html">&lt;p&gt;I have been working as Statistical analyst for the last 1.5 years and fortunately I got to work on Hadoop on one of my initial projects. Hadoop sounds scary to a lot of people and I am no exception. In this post, I would make an attempt to explain HIVE-the data warehouse for Hadoop ecosystem. &lt;/p&gt;
&lt;p&gt;What is HIVE? And why as an analyst I should care about it?&lt;/p&gt;
&lt;p&gt;HIVE is the part of the Hadoop infrastructure where data gets stored and you can write your query to fetch data from HIVE (tables). Now when I had started working on Hadoop, I knew a little SQL and a tad little of data science. I had take a few courses on Statistics during my college but then who really studies in college! Having allocated to a project which required an understanding of Hadoop, I planned how I need to understand this system so that I can contribute my best to the project.&lt;/p&gt;
&lt;p&gt;The first thing you need to build any model is data. Comes to picture-HIVE. As I said, I knew SQL and as I started working on HIVE I got to know that if one knows SQL then one knows HIVE, it’s just that he is not aware of it yet. &lt;/p&gt;
&lt;p&gt;Why do I say so? 
Because, HIVE was built with the sole purpose that database people who are comfortable working on SQL need not require to learn a completely new language to fetch and interact with data in the Hadoop ecosystem. And I believe the founders of Hadoop have achieved this successfully. &lt;/p&gt;
&lt;p&gt;HIVE hides the map-reduce processes from the user and all you need to worry about is writing your SQL query. You don’t need to worry at all about mappers and reducers. All these information is hidden from the user and if one wants to know more on these, there are ways to know that as well (advanced users of HIVE).&lt;/p&gt;
&lt;p&gt;I will cover most common queries that we deal with when working as an analyst on Hadoop- creating a table (external/internal), getting information about table, setting common HIVE settings and a few others.&lt;/p&gt;
&lt;p&gt;Creating a table
Table in HIVE can be of 2 types: Internal or External
Stay with me for next few lines and I will explain the differences. 
By default, the table that we create in most of the cases is internal table. However, external tables come into picture when you want to build a table over some file.&lt;/p&gt;
&lt;p&gt;The difference between external and internal table is in terms of what happens when we drop a table. 
External table:
 —  If you drop the table, the table and the metadata of the data is dropped but not the data 
 The data is located in hdfs (and not in local file system) and since this data is accessed by many tables we don’t want the data to get dropped.&lt;/p&gt;
&lt;p&gt;Just add the keyword external to specify that we want to build an external table.
 —  CREATE EXTERNAL TABLE external_table_name&lt;/p&gt;
&lt;p&gt;Internal table:
 —  If you drop the table, the table, metadata and even the data is dropped
 —  If the data for the table resides in the local file system, you should go for creating internal table&lt;/p&gt;
&lt;p&gt;—  CREATE INTERNAL TABLE internal_table or just CREATE TABLE internal_table
Once the table gets created, we want to get the data from the table for our analysis. Before we dive into getting the data from the table, it is a good idea to get an idea about the overall structure of the table — column names, column types, whether external or internal, owner of the table.&lt;/p&gt;
&lt;p&gt;All of these information can be retrieved by using this: describe formatted table_name; This gives you a lot of information in a formatted manner. If you are in a hurry and just want to see the column names and no other details, use desc table_name;&lt;/p&gt;
&lt;p&gt;External tables are created over some file. This file should be located in hdfs-file system for hadoop. A few command lines would easily move the file to hdfs.&lt;/p&gt;
&lt;h3&gt;How to move a file to hdfs?&lt;/h3&gt;
&lt;p&gt;Below are the steps:
- Create a directory where you want to move the file
  hadoop fs -mkdir directory_name
- Check if the directory got created
  hadoop fs -ls ; you should see your directory name here
  This directory is empty right now. You can check this using
  hadoop fs -ls directory_name; It's empty
- Move the file from local to hdfs
  hadoop fs -copyFromLocal 'path where file is stored' directory_name
- Check if the file has been copied to directory
  hadoop fs -ls directory_name; You should see the file now.&lt;/p&gt;
&lt;p&gt;Open vim editor and write the below codes in any file. I will name my file create_table.hql
vim create_table.hql
Creating table. Code below&lt;/p&gt;
&lt;p&gt;drop external table if exists db_name.table_name;
create external table if not exists db_name.table_name (
ID    string,
WorkStatus     string,
Score    int,
Residence_Region    string,
income    string,
Gender    smallint,
Alcohol_Consumption    string,
Happy    string
)
row format delimited
fields terminated by ‘\t’
stored as textfile
location ‘/user/mbarnwa/data1’
;&lt;/p&gt;
&lt;p&gt;Dropping a database
Many a times, you may want to drop a database that you don’t need anymore. Say, the database to be dropped is ‘userdb’, then if you do this: drop database userdb; 
Now the above command will work if your database is empty i.e. the database, ‘userdb’ has no tables in it. 
But that is rarely a case, so either you can go ahead on deleting each table in the database by using this command drop table ‘tableName’; or addition of a simple keyword — CASCADE will solve your purpose. So the final command to drop a database (even if it has tables) is 
drop database ‘userdb’ cascade;&lt;/p&gt;</summary><category term="Hive"></category><category term="hadoop"></category><category term="big data"></category></entry><entry><title>Pre-Placement Talk Snippets</title><link href="http://manishbarnwal.github.io/blog/2014/09/07/pre-placement_talk_snippets/" rel="alternate"></link><updated>2014-09-07T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-09-07:blog/2014/09/07/pre-placement_talk_snippets/</id><summary type="html">&lt;p&gt;I'd attended &lt;em&gt;PPTs&lt;/em&gt; (Pre-placement Talks) of almost all the companies I'd interest in during my placement time. Many students find attending PPTs a waste of time. I was not one of them.
&lt;br&gt;
&lt;br&gt;
I liked attending PPTs. I felt they gave me continuous motivation to be consistent in my preparation. I remember the first PPT I'd attended. It was EXL and it felt like almost every final year student was in that room. Not a single chair in &lt;em&gt;Kalidas Auditorium&lt;/em&gt; was unoccupied.
&lt;br&gt;
&lt;br&gt;
Looking at such a big crowd of students in that auditorium, me and my friend looked at each other with confused looks. We exchanged these words.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Me: "Yaar Rahul, yaha toh bahot log hai be. Competition tagda hai be."&lt;/p&gt;
&lt;p&gt;Rahul: "Ho jaaega be. Naukri hi toh hai."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Yes the competition is going to be tough. So be prepared. I'm not trying to scare you. I'm just saying give your best shot at preparing for the companies you're interested in. It's just a matter of few months. I remember I'd studied more in that semester than all the previous semesters combined. This semester is an opportunity to learn the maximum out of your college life. Make it count.
&lt;br&gt;
&lt;br&gt;
I always made it a practice to take notes of the presentations I was attending. And then when needed I would look at them and try to understand what exactly is the company looking for. Be the boring one and take notes.
&lt;br&gt;
&lt;br&gt;
Following are the snippets of some of the companies I could find in my mobile.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FICo&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Define score card for banking and lending institutions&lt;/li&gt;
&lt;li&gt;Clients include DB, HSBC, ICICI bank, Coca-Cola, JP Morgan Chase, Healthways&lt;/li&gt;
&lt;li&gt;Project in diverse fields&lt;/li&gt;
&lt;li&gt;FiCo score - Number 1 score in North America&lt;/li&gt;
&lt;li&gt;Hiring 25 to 30 people across India&lt;/li&gt;
&lt;li&gt;Custom analytic modeling for Maths and Computing&lt;/li&gt;
&lt;li&gt;Informal structure and informal culture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Miebach consulting&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supply chain Company&lt;/li&gt;
&lt;li&gt;What is supply chain - Everything involving from the start of manufacture of product till delivery at right time at right price&lt;/li&gt;
&lt;li&gt;Started in 1973, started in India in 1996, FOUNDED by Dr. Miebach&lt;/li&gt;
&lt;li&gt;Consulting in automotive, spare parts, fashion, FMCG, retail and          e-commerce&lt;/li&gt;
&lt;li&gt;Hiring for Associate consultant&lt;/li&gt;
&lt;li&gt;Close to 60 employees in India&lt;/li&gt;
&lt;li&gt;You need to be a team player, result oriented, a leader&lt;/li&gt;
&lt;li&gt;Procedure for hiring: CV submission, test, GD/case interview&lt;/li&gt;
&lt;li&gt;Test quantitative, analytical&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Capital one&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Credit cards, auto finance, home loans, consumer and commercial banking...50 percent bigger than SBI in terms of bank labs&lt;/li&gt;
&lt;li&gt;Us, UK, Canada&lt;/li&gt;
&lt;li&gt;Fortune 500 company, ranked 127&lt;/li&gt;
&lt;li&gt;Only 22 years old but is comparable to top banks&lt;/li&gt;
&lt;li&gt;Credit card interest rate in India? Look for it.&lt;/li&gt;
&lt;li&gt;Making decisions based on data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Times internet&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PLP, product leader program&lt;/li&gt;
&lt;li&gt;Analytical ability, General aptitude&lt;/li&gt;
&lt;li&gt;Basic aptitude test followed by pseudo algorithm test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Goldman Sachs&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Market maker, provides liquidity&lt;/li&gt;
&lt;li&gt;Advisory services eg. Merger and acquisition&lt;/li&gt;
&lt;li&gt;Financier eg. For a growing company like Facebook&lt;/li&gt;
&lt;li&gt;Asset manager&lt;/li&gt;
&lt;li&gt;Principal, find the best companies to be invested in&lt;/li&gt;
&lt;li&gt;Founded in 1869, 32400 people in 65 offices in 34 countries&lt;/li&gt;
&lt;li&gt;Survived 2008 crisis&lt;/li&gt;
&lt;li&gt;Financial holding company&lt;/li&gt;
&lt;li&gt;Bangalore is global hub of GS with representatives from all divisions&lt;/li&gt;
&lt;li&gt;Our client's interests always come first&lt;/li&gt;
&lt;li&gt;We anticipate the rapidly changing needs of our clients&lt;/li&gt;
&lt;li&gt;GS culture:   Excellence, integrity team work, meritocracy, entrepreneurial spirit client device&lt;/li&gt;
&lt;li&gt;Quantitative roles&lt;/li&gt;
&lt;li&gt;Investment management, automated trading, developing algorithms to run trade on electronic markets like NYSE, NASDAQ&lt;/li&gt;
&lt;li&gt;Risk modeling building models to quantify market risk, liquidity risk. Post trade activities, surveillance analytics&lt;/li&gt;
&lt;li&gt;What is risk? Why measure risk? Relevant to whom?&lt;/li&gt;
&lt;li&gt;One possible measure: How much can we loose in one day?&lt;/li&gt;
&lt;li&gt;Historical simulation&lt;/li&gt;
&lt;li&gt;Value at risk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AmEx, American Express&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Started in 1850 as courier firm&lt;/li&gt;
&lt;li&gt;Moved to charge and credit card business in 1958&lt;/li&gt;
&lt;li&gt;Transactions worth 900 billion dollars by volume globally&lt;/li&gt;
&lt;li&gt;Interesting projects like shopping through Twitter, tweet to buy launched in US last year&lt;/li&gt;
&lt;li&gt;AmEx is all three parties combined in online transaction&lt;/li&gt;
&lt;li&gt;500 plus people based in Gurgaon and Bangalore&lt;/li&gt;
&lt;li&gt;Decision sciences, campaign management, big data.&lt;/li&gt;
&lt;li&gt;Newly involved in e-commerce&lt;/li&gt;
&lt;li&gt;Generate different kinds of score&lt;/li&gt;
&lt;li&gt;Wipro&lt;/li&gt;
&lt;li&gt;Wipro, applying thought&lt;/li&gt;
&lt;li&gt;WIPRO: western India vegetable products limited&lt;/li&gt;
&lt;li&gt;Got Listed on NYSE in 2001&lt;/li&gt;
&lt;li&gt;Provider of Consulting services, BPO, IT, R&amp;amp;D services&lt;/li&gt;
&lt;li&gt;Largest independent R&amp;amp;D services provider in the world&lt;/li&gt;
&lt;li&gt;Six strategic SBU (small business units)&lt;/li&gt;
&lt;li&gt;Banking &amp;amp; financial services: healthcare, life science and services, retail consumer products, global media &amp;amp; telecom, energy natural resources &amp;amp; utilities, manufacturing and hi-tech&lt;/li&gt;
&lt;li&gt;One of the most ethical companies by Ethisphere institute&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Futures First&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Propriety trading, most challenging work in financial services&lt;/li&gt;
&lt;li&gt;You don't take calls from clients...you invest on your own&lt;/li&gt;
&lt;li&gt;GHF group&lt;/li&gt;
&lt;li&gt;Trading...buy low, sell high...having an expectation and trying to trade on that expectation&lt;/li&gt;
&lt;li&gt;Discipline, risk taking ability, decision making, street smartness, flexibility and adaptability, mental toughness, analytical ability&lt;/li&gt;
&lt;li&gt;Trades in: Interest rates, commodities, foreign exchange, equity index futures&lt;/li&gt;
&lt;li&gt;In terms of volume based market share, we are in top 10&lt;/li&gt;
&lt;li&gt;Group of 700 trader and analysts&lt;/li&gt;
&lt;li&gt;Started in 1993 in London, in India in 2004&lt;/li&gt;
&lt;li&gt;Performance bonus on an average of 1.2 lakhs&lt;/li&gt;
&lt;li&gt;Bonus can be zero or explode to multiple times of your salary&lt;/li&gt;
&lt;li&gt;Work week from Monday to Friday, working hours depend on your product market&lt;/li&gt;
&lt;li&gt;The best part is "even I don't know what is the maximum amount I can make" - one of the employees in the video&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Citi Bank&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PPT. presented by Raj Kurucheti&lt;/li&gt;
&lt;li&gt;Global bank, 200 years of history&lt;/li&gt;
&lt;li&gt;Leader in the financial sector with presence globally&lt;/li&gt;
&lt;li&gt;Our primary business in citi corp&lt;/li&gt;
&lt;li&gt;Citi technology centre: Pune&lt;/li&gt;
&lt;li&gt;Test analytical, logical reasoning, language agnostics, coding
&lt;br&gt;
&lt;br&gt;
I hope you found this post helpful. Share it with your friends if you found it useful. And as always, drop a comment if you have any queries and I will respond back to you as soon as possible.
&lt;br&gt;
&lt;br&gt;
Prepare well for your placements. All the best!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="placement"></category><category term="college"></category><category term="interview"></category></entry><entry><title>The Placement Season</title><link href="http://manishbarnwal.github.io/blog/2014/07/31/the_placement_season/" rel="alternate"></link><updated>2014-07-31T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-07-31:blog/2014/07/31/the_placement_season/</id><summary type="html">&lt;p&gt;The placement season is back. Nothing new, it comes every year but may be...this is new for you and I'm talking to those who're about to witness probably the most important phase of their college life.
&lt;br&gt;
&lt;br&gt;
First things first. I'll start with a disclaimer - "Everything written on this space is my personal experience which is definitely not a statistically significant sample size"
&lt;br&gt;
&lt;br&gt;
The very first question that comes to a typical final year student- What should I start my preparation with? You've got to do a lot, you know that but you don't know what to start with?
&lt;br&gt;
&lt;br&gt;
I'll help you out here. I'd started with the &lt;strong&gt;TnPedia&lt;/strong&gt; (this is specific to just students from IIT Kharagpur; others please ignore this).
&lt;br&gt;
&lt;br&gt;
Heard of TnPedia? I'm sure you'd have. If not, just download it from dc++. Ohh yes! dc is used for useful stuff as well apart from the 'general daily stuff'. Download the last three years TnPedia and read through the companies you're targeting. This'll give you an overview of what is expected in an interview, what procedures are followed in different companies, you may also gain some insight to what kind of preparation is needed for a particular company.
&lt;br&gt;
&lt;br&gt;
Now that you have given a thorough read to your target companies. Start preparing accordingly. I'll try to give a brief introduction as to what preparation is needed for different profiles.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;CONSULTING&lt;/h3&gt;
&lt;p&gt;Victor Cheng is your friend. Super-smart guy. Download his video lectures(again dc++)  to get an understanding of what solving a case-study looks like. I'd suggest you to subscribe to his email newsletters as well. Those will help you in slowly gaining the overall idea of consulting.
&lt;br&gt;
&lt;br&gt;
Preparing for this takes time and you'll have to be persistent. Practise with your wingies, your department friends, anyone...but practise!
&lt;br&gt;
&lt;br&gt;
Do the role play.
Some of you become the interviewer and one of you could be the interviewee. I know it sounds weird. But trust me you'll learn a lot. You'll overcome your anxiety of facing the interviewers. At first the role-play thing won't go that fine but with time it'll be smooth and everyone involved will come out beneficial.
&lt;br&gt;
&lt;br&gt;
Share your feedback about each other. I remember I used to get a lot aggressive while participating in mock group discussions with my friends. You sure will get to know about your weaknesses. Work on these weaknesses before it's too late. By doing this, you get a chance to commit all your mistakes before the real interview and that's a good thing.
Enough words. It's time to execute.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Being pro-active is the key."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Resources for preparation&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the video lectures from dc++. Some 10 videos are there&lt;/li&gt;
&lt;li&gt;Subscribe to &lt;a href="http://www.caseinterview.com/homepage-new?utm_expid=6333140-12.n1MK6GEfT5qHocwp2COITg.1&amp;amp;utm_referrer=https%3A%2F%2Fstuffedtomato.quora.com%2FThe-placement-season"&gt;case interview emails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Case in point. The pdf is easily available on dc++ or online as well&lt;/li&gt;
&lt;li&gt;Practise your basic math skills. You can't afford to make calculation mistakes in interview. &lt;a href="https://www.caseinterview.com/math/home.php"&gt;CaseInterview&lt;/a&gt; will improve your calculation speed and accuracy
&lt;br&gt;
&lt;br&gt;
Case-interview is all about practise. After going through some frameworks you'll feel like you've earned the mastery over this but I guarantee you'll find this wrong once you actually solve a case-study problem in a real interview set-up. So practise, practise and practise.
&lt;br&gt;
&lt;br&gt;
That's all for now. I'll add about other job profiles soon as time permits. No more a free bird like I used to be in Kgp. Next update would be on Coding.
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;CODING&lt;/h3&gt;
&lt;p&gt;Let's talk about coding preparation. Coding was one thing which I'd started learning and practicing only during my placement preparation. Prior to that I never really liked it. Actually I'd never really put much effort into it. And let me tell you this - Coding is interesting stuff to say the least.&lt;/p&gt;
&lt;p&gt;Some of the resources that I'd found helpful.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/"&gt;GeeksForGeeks&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/c-arrays/"&gt;Arrays Archives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/c-strings/"&gt;Strings Archives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/linked-list/"&gt;Linked Lists Archives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/tree/"&gt;Trees Archives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/bit-magic/"&gt;Bit Magic Archives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.geeksforgeeks.org/category/program-output/"&gt;Output Archives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://quiz.geeksforgeeks.org/"&gt;GeeksQuiz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.in/Cracking-Coding-Interview-Laakmann-McDowell/dp/0984782869/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1468958412&amp;amp;sr=1-1&amp;amp;keywords=cracking+the+coding+interview"&gt;Cracking the Coding interview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.in/Data-Structures-Algorithms-Made-Easy/dp/145654988X/ref=sr_1_5?s=books&amp;amp;ie=UTF8&amp;amp;qid=1468958573&amp;amp;sr=1-5&amp;amp;keywords=Data+Structures+And+Algorithms+Made+Easy"&gt;Data Structures And Algorithms Made Easy&lt;/a&gt;. You get this book in Java, C and C++ version. Get the one in which you're comfortable with. For a starter like me, I'd read the C version.
&lt;br&gt;
&lt;br&gt;
The above should suffice for a normal coding preparation. I'll add more soon. I'll ask some of my coder friends.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That's all for now. Next update would be on puzzles and brainteasers.&lt;/p&gt;
&lt;p&gt;All the best for the placements. Prepare well.
Any help needed? Drop me an email or comment down and I will get back to you as soon as possible.&lt;/p&gt;</summary><category term="interview"></category><category term="placement"></category><category term="preparation"></category><category term="college"></category></entry><entry><title>Beautiful Sadness</title><link href="http://manishbarnwal.github.io/blog/2014/06/03/beautiful_sadness/" rel="alternate"></link><updated>2014-06-03T10:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-06-03:blog/2014/06/03/beautiful_sadness/</id><summary type="html">&lt;p&gt;As I woke up this morning, it took me a few moments to realize where I was. I looked around, gained my consciousness and a thought thundered into my mind –I was not in Kgp. Not anymore. Life at Kgp is now a past, a past which will always be cherished and remembered as the guide of all my future endeavors. I had heard that time flies but this fast I didn’t had even an iota of idea. The five years passed by in a blink.
&lt;br&gt;
&lt;br&gt;
When  you leave Kgp, you find yourself juggling with thoughts. You experience mixed emotions of both happiness and sadness or rather a term –‘beautiful sadness’ perfectly contain these feelings. You feel sad for the life of peace is over –the phenomenal five years. At the same time you find yourself smiling that Kgp happened to you, that you had your best time here, that you couldn’t have imagined a better college life.
&lt;br&gt;
&lt;br&gt;
Thank you Kgp for making me what I’m today and know that you’ll play a prominent role in whatever I aspire to become in my future. You are one of the best things that has happened to me in my past five years. Kgp, you have introduced me to some of the most amazing people, well some weird ones as well and a fewer limited editions.
&lt;br&gt;
&lt;br&gt;
Now that we’re separated, I feel the void that has been created between us. But know that no one can ever fill this space of yours. You will always be special and I want you to know that just a thought of yours will automatically bring a smile on my lips. Thank you for the wonderful time we had together. Kgp, thou shall always be missed.
&lt;br&gt;
&lt;br&gt;
Yours forever,&lt;/p&gt;
&lt;p&gt;--Just another Kgpian&lt;/p&gt;
&lt;p&gt;&lt;img alt="photo of kgp" src="http://manishbarnwal.com/images/beatiful_sadness.jpg" /&gt;&lt;/p&gt;</summary><category term="college"></category><category term="life"></category><category term="thoughts"></category><category term="nostalgia"></category></entry><entry><title>My stay at IIT Kharagpur -A synopsis</title><link href="http://manishbarnwal.github.io/blog/2014/04/16/stay_at_IIT_Kharagpur/" rel="alternate"></link><updated>2014-04-16T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-04-16:blog/2014/04/16/stay_at_IIT_Kharagpur/</id><summary type="html">&lt;p&gt;Mathematics has always been my favorite and when I got Mathematics &amp;amp; Computing, IIT Kharagpur I was on cloud nine. I did a quick Google and landed to the official site of our institute. That was the first time and also the last time when I had paid close attention to each of the tabs in the site. The five years of stay at this land has been phenomenal. I know time flies but this fast, I had no idea.
&lt;br&gt;
&lt;br&gt;
So I started my first year with full innocence, attended almost all my classes the first few days after which I came to know about this beautiful concept--proxy. The first year passed mostly in making new friends, understanding DC++, giving auditions for societies I had no idea about whatsoever, celebrating birthdays--GPLand attending a few classes. Needless to say, I had screwed my CGPA badly.
&lt;br&gt;
&lt;br&gt;
Something happened in the very beginning of 2nd year. I got serious with my academics, a little and then I met Professor G.P Raja Sekhar. Well, he needs no introduction. We had 9 credits under him, such a strict professor he was that I managed to attend almost all his classes even the 7:30 ones. That helped and I ended up improving my CGPA. Thank you Sir. I owe this one to you.
&lt;br&gt;
&lt;br&gt;
Third year went normal –attending a few more classes, this time in our department mostly. I had my internship at IDRBT–an RBI institute in Hyderabad. I feel this internship opportunity is highly under-rated. I urge all you juniors to have a look at this opportunity. I met some juniors and a few seniors of our department. One can find a Kgpian in almost every city. Two months internship got over like a breeze.
&lt;br&gt;
&lt;br&gt;
Fourth year, this is the time when you get to know the real competition –internship season. Some 10 students could manage to get an internship offer from TnP. The rest of us tried our luck off-campus. I landed up in Mumbai at CRISIL Limited for internship. Mumbai is a city full of life and happenings.
&lt;br&gt;
&lt;br&gt;
Then came the final year –the placement season. You end up attending more PPTs than your classes. This green site–TnP automatically becomes your homepage. You study more in these four months than all of your first four years combined. A typical day consists of getting CV reviews from your seniors, discussing puzzles, brushing up your coding skills, solving case studies and attending some PPTs. Being proactive is the key. Be well prepared and you will bag your dream job quite easily. Talk to relevant seniors –they know a lot.
&lt;br&gt;
&lt;br&gt;
I was offered the job of statistical analyst at WalmartLabs. The interview revolved mostly around Statistics, my internships and a few puzzles. Having placed, the last semester is a total delight. You are about to leave your institute so all your emotional chords get stroked.  You wonder doing all things which you might have missed during your four years stay at IIT Kharagpur.
&lt;br&gt;
&lt;br&gt;
I wish all the very best to my junior batch in their future endeavors.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You won't get a life like Kgp anywhere else. Make it count.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="life"></category><category term="college"></category></entry><entry><title>Happy Women's Day</title><link href="http://manishbarnwal.github.io/blog/2014/03/08/happy_woman_day/" rel="alternate"></link><updated>2014-03-08T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-03-08:blog/2014/03/08/happy_woman_day/</id><summary type="html">&lt;p&gt;Today is 8th March, just another day except that today is women’s day. Women’s day started as a Socialist political movement but over time it has changed to a day when it’s celebrated as a sign of respect and love to women across the world. Women’s day is all about respecting and caring for women around you. Take some time to wish and appreciate women in your life. Not that you shouldn’t love them on any ordinary day. All I’m saying is –&lt;em&gt;It’s their day. Make it special just like you do on someone’s birthday&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;
Give a call or pay a visit to your mother, wife, daughter, girlfriend, colleague or any women who has stood by you and supported you through the ups and downs of your life. &lt;em&gt;Don’t let this pass like just another day&lt;/em&gt;. Wish them. Tell them that you are thankful to them; I guarantee you their smile –a beautiful smile in return.
&lt;br&gt;
&lt;br&gt;
I called my mother, wished her, and made her smile. &lt;strong&gt;Did you?&lt;/strong&gt;&lt;/p&gt;</summary><category term="life"></category><category term="women's day"></category><category term="happy"></category></entry><entry><title>The Walmart Interview</title><link href="http://manishbarnwal.github.io/blog/2014/03/04/walmart_interview/" rel="alternate"></link><updated>2014-03-04T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2014-03-04:blog/2014/03/04/walmart_interview/</id><summary type="html">&lt;p&gt;Now that you are reading this, I’ll assume that you are someone looking for an opportunity of placement or an internship. Go ahead and read. You have a long journey to cover.
&lt;br&gt;
&lt;br&gt;
I was offered a job of Statistical Analyst at WalmartLabs. Let me guide you through their placement procedure. Sometime in October, we had the Pre-placement talk (we call it PPT here) where we were introduced to the company’s fundamentals, its businesses and some awesomeness of Walmart. The guy presenting the PPT looked geeky without paying much heed to if we were at all listening. But frankly, he had covered almost everything.
&lt;br&gt;
&lt;br&gt;
A month later, we had the online test which was based on statistical questions covering mostly topics such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hypothesis Testing&lt;/li&gt;
&lt;li&gt;R square&lt;/li&gt;
&lt;li&gt;Regression&lt;/li&gt;
&lt;li&gt;Basic probability distribution functions&lt;/li&gt;
&lt;li&gt;Confidence interval
&lt;br&gt;
&lt;br&gt;
It also had 2 coding questions out of which I somehow managed to write the logic of one of them. The duration of the test was 90 minutes. I had started with statistical questions first and later moved to the coding part. My friends who had started with the coding questions were not able to attempt some very easy statistics questions. My advice to you is, go for the objective statistics questions first.  I managed to attempt some 17 questions out of a total of 23 questions. Based on the performance in the test a further shortlist of 7 students was declared. Needless to say I was one of them.
&lt;br&gt;
&lt;br&gt;
So I was selected for the personal interview of WalmartLabs. A lot had to be done. I didn’t know where to start. The first thing I did was to talk to one of my seniors, Manikanta Dinesh. His advice and suggestions helped. A lot. I kept myself surrounded with books for quite a few days. I had interviews for a few other companies as well. I couldn't put all my energy into this one company.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;“Don’t keep all your fruits in one basket. Diversify.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
It was day 1, 1st December. I was selected in only one company.  CGPA plays a crucial part. If you have a low CGPA, chances are that you will see students getting shortlisted and your name won’t grace any of the lists. I know that feeling. You see, CGPA plays its part. A strong one.
&lt;br&gt;
&lt;br&gt;
Next day, Day 2, the only company to appear for – WalmartLabs. So I took all the interview funda from a friend, actually a mentor, Tushar – well, he knew more about interviews at that time, Day 1 placed at Credit Suisse. Having all well-prepared, suited up, drove my cycle to Nalanda Complex. On my way, I kept blabbering and preparing my answers, mostly the introduction part. You see, how you open the interview is important.
&lt;br&gt;
&lt;br&gt;
Though there were 2 HR guys from Walmart, the 1st round was a Skype interview. I had the experience of talking to my girlfriend at Skype. So 1st round well. That was just said in jest. The 1st round lasted for some 50 minutes. I was asked mostly about my internship at &lt;a href="http://www.idrbt.ac.in/"&gt;IDRBT&lt;/a&gt;. A few puzzles – 3 doors puzzle or the famous &lt;a href="http://www.mathsisfun.com/puzzles/monty-hall.html"&gt;Monty hall&lt;/a&gt;. I was able to answer almost all the puzzles. The interviewer was a CS graduate from IIT Bombay working at WalmartLabs for the past 6 years. How do I know? You ask. Well, I had asked him about it towards the end of the interview where you get to ask questions to the interviewer.
“Never miss this opportunity. Always ask questions. Period.”
&lt;br&gt;
&lt;br&gt;
So the 1st round went well. Two of my friends couldn’t make to the 2nd round. This round was a Skype interview as well. This guy, the interviewer was a dude. We talked – I didn’t feel like it was an interview for some 90 minutes. Some very basic concepts of statistics were talked about. Some of which I recall:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How would you explain probability to your grandmother?&lt;/li&gt;
&lt;li&gt;How would you teach statistics to a layman?&lt;/li&gt;
&lt;li&gt;What are assumptions of linear regression? What are their implications?&lt;/li&gt;
&lt;li&gt;A few puzzles.&lt;/li&gt;
&lt;li&gt;Explain your work at CRISIL Ltd.
&lt;br&gt;
&lt;br&gt;
Confident of my grasp at Statistics, the interviewer moved towards courses like Real Analysis and Measure Theory.
Question: Tell me about Cauchy’s law of convergence?
I was blank. Total blank. Few seconds passed. I muttered a few words vaguely. Finally I reiterated confidently. “Sir, I don’t remember this as of now. I didn’t like these courses much. But if you want me to know these subjects, I can very well be versed in them given a day or two.”
Guess what? He was impressed by my say. He said – “Not a problem Manish! I was just curious. You are good in Statistics and that is what mostly matters here.”
&lt;br&gt;
&lt;br&gt;
The 3rd round started with only two of us in the game now. The interviewer was a senior executive from US. He asked some general questions from my CV. No more puzzles. This round too lasted for some 50 minutes. Finishing this round, I expected they would announce their result. But they had more in their basket. The other friend of mine couldn’t clear the 3rd round.
&lt;br&gt;
&lt;br&gt;
The 4th round was an HR round with one of the 2 guys who were present there. This lasted for some 15 minutes. By this time, I was determined – determined to be selected. Cleared it with a smile.
Yes there was a 5th round –a telephonic interview with the senior HR, Bangalore office. Again a 30 minutes interview. Some of the questions I recall:&lt;/li&gt;
&lt;li&gt;Why Walmart?&lt;/li&gt;
&lt;li&gt;Who is your best buddy? Worst buddy?&lt;/li&gt;
&lt;li&gt;What is your greatest weakness?&lt;/li&gt;
&lt;li&gt;Any questions you would like to ask?
&lt;br&gt;
&lt;br&gt;
The time now was 7:00 p.m. I was waiting for the result of longest interview of my life, walking restlessly outside the room. Waited for some 15 minutes. I could see the HR guys talking and discussing intensely over phone. A few more minutes passed. The wait was getting heavy now. The HR guy finally came out and with a broad smile, he said –“Congratulations! Welcome to the team.”
&lt;br&gt;
&lt;br&gt;
That moment I knew what success tasted like. I was happy, excited. Overwhelmed! All of a sudden, I felt like a hero. They clicked a photo of me wearing the Walmart cap. They gave me a bag of gifts as a sign of welcome. On my way to hall, I gave a quick call to my father and then to my maa –She said she had been worshipping the whole day. Her prayers had played their charms and she was thankful to her Gods.
&lt;br&gt;
&lt;br&gt;
Reached my wing, told my friends and then they congratulated –GPL. The night was great. It called for a celebration. We headed to the bar. Time for some drinks.
&lt;br&gt;
&lt;br&gt;
One final thought:&lt;blockquote&gt;
&lt;p&gt;“Ek din hota hai, ek job hota hai aur ek tum hote ho.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="analytics"></category><category term="statistics"></category><category term="interview"></category><category term="walmart"></category></entry></feed>