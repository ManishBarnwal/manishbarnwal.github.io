<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Manish Barnwal</title><link href="http://manishbarnwal.github.io/" rel="alternate"></link><link href="http://manishbarnwal.github.io/feeds/ml-data-science.atom.xml" rel="self"></link><id>http://manishbarnwal.github.io/</id><updated>2017-03-28T07:00:00-03:00</updated><entry><title>Diving into H2O with R</title><link href="http://manishbarnwal.github.io/blog/2017/03/28/h2o_with_r/" rel="alternate"></link><updated>2017-03-28T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-03-28:blog/2017/03/28/h2o_with_r/</id><summary type="html">&lt;p&gt;Do you understand the pain when you have to train advanced machine learning algorithms like Random Forest on huge datasets? When there is a factor column that has way too many number of levels? When the time taken to train the model is so huge that you went to your pantry for snacks and came back, you are even done browsing 9gag but your model is still training, the code is still running? Fear no more, we will talk about these problems and how we can address them.
&lt;br&gt;
&lt;br&gt;
We will first try to understand why it takes so much time to train models on huge datasets in R. And then the solution to this - build models using H2O in R.
&lt;br&gt;
&lt;br&gt;
Our laptop has multiple cores in it. Most of the laptops these days come with at least 4 cores. R by default uses only one of the cores of your laptop. Say your laptop has 4 cores, then the remaining 3 cores are unused or may be partially used by other processes that your computer is running. Using just one core would obviously be slower than if R could use multiple cores in parallel.  &lt;/p&gt;
&lt;p&gt;What if R could use the other cores as well? This would definitely make the R codes run faster. The solution is H2o.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is h2o?&lt;/h3&gt;
&lt;p&gt;H2O is &lt;em&gt;The Open Source In-Memory, Prediction Engine for Big Data Science&lt;/em&gt;. H2o enables R to use the other cores of the laptop as well. R then runs on multiple cores of your laptop. You laptop is now like a standalone cluster. The power of you laptop has just increased by H2o. You know what your laptop is thinking right now - &lt;em&gt;With great power, comes great responsibility&lt;/em&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Do you need to worry about how H2o converts your laptop to a cluster?&lt;/h3&gt;
&lt;p&gt;Not at all. It is as simple as running this - h2o.init(). We will come to this in later part of the post. The whole idea why h2o was built was keeping in mind that most of its users would be Data Scientists and it should be easy for them to build these models using h2o without any hassle of worrying about distributing the code across the cluster. And the developers of h2o have achieved this with flying colors.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Initializing the H2o cluster&lt;/h3&gt;
&lt;p&gt;So we now understand what is the use of h2o and how it can make our life easier. Let us now talk about execution.
You would first need to install the h2o package using
&lt;pre style=*font-size:60%; padding:3px; margin:0em;*&gt;
&lt;code class=*bash*&gt;install.packages(&lt;em&gt;h2o&lt;/em&gt;)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Next, we load the installed package and launch h2o from R.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;library&lt;span class="p"&gt;(&lt;/span&gt;h2o&lt;span class="p"&gt;)&lt;/span&gt;
local_h2o &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; h2o.init&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above will initialize your h2o cluster. Now there are other parameter in &lt;em&gt;h2o.init()&lt;/em&gt; but for now let's go with the default settings. Few important parameters to &lt;em&gt;h2o.init()&lt;/em&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;nthreads&lt;/em&gt;: Number of cores of the computer you want to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;max_mem_size&lt;/em&gt;: The total RAM size allocated to the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;
If you are interested in the other input parameters that can be passed, you can read for help by typing &lt;code&gt;?h2o.init()&lt;/code&gt; in R.
&lt;br&gt;
&lt;br&gt;
Once you have initiated the cluster, check if your connection is working using &lt;code&gt;h2o.clusterInfo()&lt;/code&gt;.
You should see this line- &lt;em&gt;R is connected to the H2O cluster along with other details of your initiated cluster&lt;/em&gt;. Now your cluster is ready and you are good to start your coding workflow in h2o.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Working with H2o&lt;/h3&gt;
&lt;p&gt;There are two ways you can work with h2o. Either with the flow or writing the code in R editor. The h2o flow is like an user interface that can be accessed at &lt;a href="http://localhost:54321"&gt;http://localhost:54321&lt;/a&gt; after you have initiated the h2o instance using h2o.init(). The other way is to write the code in R. I normally use h2o flow just to create the first base model to see how the base model is doing. H2o flow is easy to get started with. There is no coding involved here. The serious work gets done by writing the code in R.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="photo of h2o_flow" src="http://manishbarnwal.com/images/h2o_flow.png" /&gt;&lt;/p&gt;
&lt;p&gt;Almost all of the machine learning models are supported in H2o. Some of them being Deep Learning, Generalized Linear Models (GLM), Gradient Boosted Regression (GBM), K-Means, Naive Bayes, Principal Components Analysis (PCA), Principal Components Regression (PCR), Random Forest (RF) and few others.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Interaction of R with the cluster&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The data is not saved in the R workspace&lt;/em&gt;. All data munging occurs in the h2o instance. By the look and feel it is easy to believe that all the processing is taking place in R but that is not true.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;You are now &lt;strong&gt;not&lt;/strong&gt; limited by R's ability&lt;/em&gt; to handle the data but by the amount of memory allocated to your h2o instance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When the user makes a request, R queries the server via the REST API, which returns a JSON file with the relevant information that R then displays in the console.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Demonstration on a dataset&lt;/h3&gt;
&lt;p&gt;We will try to understand the working of h2o using a classification problem. The problem is simple. The government of a country wants to understand if its citizens are happy or not.&lt;/p&gt;
&lt;p&gt;There are various independent features like &lt;em&gt;WorkStatus&lt;/em&gt;, &lt;em&gt;Residence_Region&lt;/em&gt;, &lt;em&gt;income&lt;/em&gt;, &lt;em&gt;Gender&lt;/em&gt;, &lt;em&gt;Unemployed10&lt;/em&gt;, &lt;em&gt;Alcohol_Consumption&lt;/em&gt; and a few others. There is a response column called &lt;em&gt;Happy&lt;/em&gt;. Your task is to classify a citizen as happy or not happy. I will write a follow up post that will involve the code for the above problem statement.
&lt;br&gt;
&lt;br&gt;
As always, feedbacks and comments are welcomed. Share this with people who are interested in learning machine learning and data science. Let us talk more of Data Science.&lt;/p&gt;</summary><category term="big data"></category><category term="R"></category></entry><entry><title>An illustrated introduction to adversarial validation part 2</title><link href="http://manishbarnwal.github.io/blog/2017/02/16/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-16T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-16:blog/2017/02/16/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;In the last post we talked about the &lt;a href="http://manishbarnwal.com/blog/2017/02/15/introduction_to_adversarial_validation/"&gt;idea of adversarial validation&lt;/a&gt; and how it helps the problem when your validation set result doesn't comply with that of test set result. In this post, I will share the R code to help achieve the idea of adversarial validation. The data used would be from &lt;a href="https://numer.ai/"&gt;Numerai competition&lt;/a&gt;.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Loading required packages&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
library(randomForest)
library(glmnet)
library(data.table)
library(MLmetrics)
getwd()
dir()
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Reading train and test data set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train &lt;- fread("Data/numerai_training_data.csv")
train &lt;- as.data.frame(train)
train$target &lt;- as.factor(train$target)

str(train)
dim(train) # has close to 136000 rows and having no missing values
head(train)

test &lt;- fread("Data/numerai_tournament_data.csv")
test &lt;- as.data.frame(test)
dim(test) # has close to 150000 rows and having no missing values
head(test)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Creating the target variable to distinguish between train and test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
train$isTest &lt;- 0 # assigning 0 for train and 1 for test data
test$isTest &lt;- 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Combining train and test data into a single data frame&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
combi &lt;- rbindlist(list(train[, -51], test[, -1])) # removing 'target' from train data and 't_id' from test data
combi$isTest &lt;- as.factor(combi$isTest)
combi &lt;- as.data.frame(combi)
str(combi)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Train a classifier to identify whether data comes from the train or test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
logitMod &lt;- glm(formula = isTest~. , data = combi, family = 'binomial')
summary(logitMod)
head(logitMod$fitted.values)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Predict on the training data to see which rows resembles most to the test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
pred &lt;- predict(logitMod, newdata = train, type = 'response')
head(pred)

trainData &lt;- train
head(trainData)
trainData$predictTest &lt;- pred
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Sort the training data by it’s probability of being in the test set&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
trainData &lt;- trainData[order(trainData$predictTest, decreasing = T), ]

valIndx &lt;- 1:(0.2*nrow(trainData))
colsToKeep &lt;- names(trainData)[!names(trainData) %in% c('isTest', 'predictTest')]

trainFinal &lt;- trainData[-valIndx, colsToKeep]
valData &lt;- trainData[valIndx, colsToKeep]

write.csv(trainFinal, 'trainfinal.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Build a random forest classifier to predict the 'target' variable&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
set.seed(1) # setting seed for reproducibility of the result

matX &lt;- trainFinal[, -grep('target', names(trainFinal))]
response &lt;- trainFinal[, 'target']
table(response)

rfMod &lt;- randomForest(x = matX, y = response, ntree = 200, mtry = 7) # training randomForest model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on validation set&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
rfValPreds &amp;lt;- predict(rfMod, newdata = valData, type="prob")
head(valPreds)
LogLoss(rfLassoValPreds, as.numeric(as.character(valData$target))) # LogLoss function from MLmetrics package
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;
The validation set gives a LogLoss of 0.699. Let us see how does this come out on test data set. For this we will predict on the test data and upload the predictions to the site.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Prediction on actual test data&lt;/h3&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;
testPreds &lt;- predict(rfMod, newdata = test, type = 'prob')
testPreds &lt;- testPreds[, 2]

submission &lt;- data.frame(t_id = test$t_id, probability = testPreds)
head(submission)
write.csv(submission, 'submission.csv', row.names = F, quote = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;
The predictions on test data shows a LogLoss of 0.694 which is same as that of the validation set. We can now hope to have same result on both validation set and test set.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>How to use Git and Github [draft]</title><link href="http://manishbarnwal.github.io/blog/2017/02/15/git_and_github/" rel="alternate"></link><updated>2017-02-15T10:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-15:blog/2017/02/15/git_and_github/</id><summary type="html">&lt;p&gt;I had taken this course - How to use git and github some time last year. This post is an amalgamation of the course notes and other tutorials I have completed in understanding git. I will talk about the most frequently used commands.&lt;br /&gt;
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git init&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Initializes an empty git repository in the directory. Creates a hidden folder .git in your directory. If you click on the .git folder, you will see many sub-directories and files in side this folder. But, you will hardly need to know what these files are. These folders are the guts of Git where all the magic happens.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git status&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gives you the status of the snapshot of your repository. Suppose you made some changes on your local disk, then this change doesn’t automatically gets reflected on GIT. git status tells you what all were modified since the last commit. It’s healthy to run git status often. Sometimes things change and you don’t notice it. Status of your repository could be one of the following:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;staged&lt;/em&gt;: Files are ready to be committed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;unstaged&lt;/em&gt;: Files with changes that have not been prepared to be committed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;untracked&lt;/em&gt;: Files aren’t tracked by Git yet. This usually indicates a newly created file.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;deleted&lt;/em&gt;: File has been deleted and is waiting to be removed from Git.
&lt;br&gt;
&lt;br&gt;
In a nutshell, you run git status to see if anything has been modified and/or staged since your last commit so you can decide if you want to commit a new snapshot and what will be recorded in it.
&lt;br&gt;
&lt;br&gt;
Staging Area:
A place where we can group files together before we “commit” them to Git. Once, you add the files to Git using git add, the files come to staging area. Staged files are files we have told git that are ready to be committed. The files listed here are in the &lt;em&gt;Staging area&lt;/em&gt;, and they are not in our repository yet.
&lt;br&gt;
&lt;br&gt;
To store our staged changes we run the commit command with a message describing what we’ve changed.
After using git add, make sure to use git status to see what all files are there in the staged area and be sure that you want to commit these files only.
&lt;br&gt;
&lt;br&gt;
You can unstage files by using the git reset command. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git reset filename.txt&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;em&gt;git reset&lt;/em&gt; merely unstage the files but these files will still be there in your local. So if you want to get rid of these unstaged files, you will have to use git checkout to the commit after which you added these files.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git — version&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Gives you the version of git you are using&lt;/p&gt;
&lt;h3&gt;Commits&lt;/h3&gt;
&lt;p&gt;Commits are part of GIT VCS (version control systems) which lets you save meaningful changes of your code. It is because of commit that you will be able to go to any of the previous versions of your code.
&lt;br&gt;
&lt;br&gt;
A commit is a snapshot of every file in your repository at the time of commit. Commits are Git’s way of saving versions, so to save two different versions, you would create two commits.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Commits with multiple files&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You will often work with multiple files and not just one code and these file may or may not be directly related. Any collection of such files is called a repository. Now, GIT keeps track of all the changes made to each of the files and these are carried forward i.e when you save a version of a file using commit you will save a version of all the files in that repository.
&lt;br&gt;
&lt;br&gt;
Git does not rename files when you save a new commit. Instead, Git uses the commit IDs to refer to different versions of the files, and you can use git checkout to access old versions of your files.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git commit -m “added new feature RMSE to the function”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git checkout&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is sort of like restoring previous version.&lt;/p&gt;
&lt;p&gt;Say, the commit ID of the most recent commit is 3884eab839af1e82c44267484cf2945a766081f3. You can use this commit ID to return to the latest commit after checking out an older commit.&lt;/p&gt;
&lt;p&gt;Format of git checkout&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git checkout 3884eab839af1e82c44267484cf2945a766081f3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;How often to commit&lt;/h3&gt;
&lt;p&gt;Since you can choose when to make a commit, you might be wondering how often to commit your changes. It’s usually a good idea to keep commits small. As the diff between two versions gets bigger, it gets harder to understand and less useful. However, you don’t want to make your commits too small either. If you always save a commit every time you change a line of code, your history will be harder to read since it will have a huge number of commits over a short time period.
&lt;br&gt;
&lt;br&gt;
A good rule of thumb is to make one commit per logical change. For example, if you fixed a typo, then fixed a bug in a separate part of the file, you should use one commit for each change since they are logically separate. If you do this, each commit will have one purpose that can be easily understood. Git allows you to write a short message explaining what was changed in each commit, and that message will be more useful if each commit has a single logical change.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Reflect: Manual Commits&lt;/h3&gt;
&lt;p&gt;What do you think are the pros and cons of manually choosing when to create a commit, like you do in Git, vs having versions automatically saved, like Google Docs does?
&lt;br&gt;
&lt;br&gt;
Manually choosing when to create a commit like in Git:&lt;/p&gt;
&lt;p&gt;Pros: You won’t have to wait for some time or lines of code and then that commit will be made. It depends on you and you get to decide whether the code change that you have done deserves or needs to be mentioned in the commit.&lt;/p&gt;
&lt;p&gt;Cons: Getting you to decide when to commit is a little subjective and different people will have different answer to it and so the size of commits will be different for different user and if you are not used to it, understanding the differences in the commits can prove to be problematic for you&lt;/p&gt;
&lt;p&gt;Having versions automatically saved like Google Docs does:&lt;/p&gt;
&lt;p&gt;Pros: You don’t need to worry about manually commiting. You know this gets committed automatically&lt;/p&gt;
&lt;p&gt;Cons: Too many commits or too less commits will be generated depending on what settings are there for automatic commits
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Branching&lt;/h3&gt;
&lt;p&gt;When developers are working on a feature or bug they’ll often create a copy (aka. branch) of their code they can make separate commits to. Then when they’re done they can merge this branch back into their main master branch.
&lt;br&gt;
&lt;br&gt;
Branches are what naturally happens when you want to work on multiple features at the same time. You wouldn’t want to end up with a master branch which has Feature A half done and Feature B half done.
&lt;br&gt;
&lt;br&gt;
Rather you’d separate the code base into two “snapshots” (branches) and work on and commit to them separately. As soon as one was ready, you might merge this branch back into the master branch and push it to the remote server.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Remove all the things!&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;git rm&lt;/strong&gt; command will not only remove the actual files from disk, but will also stage the removal of the files for us. Now that you’ve removed all the required files you’ll need to commit your changes. Feel free to run git status to check the changes you’re about to commit.
&lt;br&gt;
&lt;br&gt;
Removing one file is great and all, but what if you want to remove an entire folder? You can use the recursive option on git.
This will recursively remove all folders and files from the given directory.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git rm -r folder_of_cats&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git log&lt;/strong&gt;
Think of Git’s log as a journal that remembers all the changes we’ve committed so far, in the order we committed them. Gives you the list of all commits that have been made to the code. It gives you the commit number and an associated message that was added to the commit.
&lt;br&gt;
&lt;br&gt;
Exiting git log: To stop viewing git log output, press q (which stands for quit).
&lt;br&gt;
&lt;br&gt;
Getting Colored Output: To get colored diff output, run &lt;em&gt;git config — global color.ui auto&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;git log — stat&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Gives you the statistics of all the commits that has been made, with information like which file changed and whether lines were added or deleted for each commit.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git push&lt;/strong&gt;
The push command tells Git where to put our commits when we’re ready. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git push -u origin master&lt;/p&gt;
&lt;p&gt;git push&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The -u tells Git to remember the parameters, so that next time we can simply run git push and Git will know what to do.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Pulling Remotely&lt;/h3&gt;
&lt;p&gt;Let’s pretend some time has passed. We’ve invited other people to our GitHub project who have pulled your changes, made their own commits, and pushed them. We can check for changes on our GitHub repository and pull down any new changes by running this command.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git pull origin master&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;HEAD&lt;/h3&gt;
&lt;p&gt;The HEAD is a pointer that holds your position within all your different commits. By default HEAD points to your most recent commit.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;git diff&lt;/strong&gt;
Gives you the difference between the commits that you have performed in your code. If you want to understand the differences between 2 commits (say comNo.1 and comNo.2) you just do this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git diff comNo.1 comNo.2&lt;/p&gt;
&lt;p&gt;git diff&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Without any extra arguments, a simple &lt;em&gt;git diff&lt;/em&gt; will display in unified diff format (a patch) what code or content you’ve changed in your project since the last commit that are not yet staged for the next commit snapshot.
&lt;br&gt;
&lt;br&gt;
So where git status will show you what files have changed and/or been staged since your last commit, git diff will show you what those changes actually are, line by line. It’s generally a good follow-up command to git status
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is a README?&lt;/h3&gt;
&lt;p&gt;Many projects contain a file named “README” that gives a general description of what the project does and how to use it. It’s often a good idea to read this file before doing anything with the project, so the file is given this name to make users more likely to read it.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Cloning a Repository&lt;/h3&gt;
&lt;p&gt;There is a difference between downloading and cloning a repository. When you clone a repository, you don’t just download the files i.e. the latest commit file but the entire commit history as well. To clone a repository, run git clone followed by a space and the repository URL.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone &lt;repo url&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Example: Use the following url to clone the Asteroids repository: https://github.com/udacity/asteroids.git&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;git clone https://github.com/udacity/asteroids.git&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Git Errors and Warnings Solution&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Should not be doing an octopus&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Octopus is a strategy Git uses to combine many different versions of code together. This message can appear if you try to use this strategy in an inappropriate situation.
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;You are in ‘detached HEAD’ state&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;HEAD is what Git calls the commit you are currently on. You can “detach” the HEAD by switching to a previous commit. Despite what it sounds like, it’s actually not a bad thing to detach the HEAD. Git just warns you so that you’ll realize you’re doing it.
&lt;br&gt;
&lt;br&gt;
&lt;em&gt;Panic! (the ‘impossible’ happened)&lt;/em&gt;
This is a real error message, but it’s not output by Git. Instead it’s output by GHC, the compiler for a programming language called Haskell. It’s reserved for particularly surprising errors!
&lt;br&gt;
&lt;br&gt;
Takeaway I hope these errors and warnings amused you as much as they amused me! Now you know what kind of errors Git can throw.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Git command review&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Compare two commits, printing each line that is present in one commit but not the other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git diff will do this. It takes two arguments — the two commit ids to compare.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a copy of an entire Git repository, including the history, onto your own computer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git clone will do this. It takes one argument — the url of the repository to copy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Temporarily reset all files in a directory to their state at the time of a specific commit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git checkout will do this. It takes one argument — the commit ID to restore.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Show the commits made in this repository, starting with the most recent.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;git log will do this. It doesn’t take any arguments.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Behavior of git clone&lt;/h3&gt;
&lt;p&gt;If someone else gives you the location of their directory or repository, you can copy or clone it to your own computer.
This is true for both copying a directory and cloning a repository.
&lt;br&gt;
&lt;br&gt;
If you have a URL to a repository, you can copy it to your computer using git clone. For copying a directory, you weren’t expected to know this, but it is possible to copy a directory from one computer to another using the command scp, which stands for “secure copy”. The name was chosen because the &lt;strong&gt;scp&lt;/strong&gt; command lets you securely copy a directory from one computer to another.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The history of changes to the directory or repository is copied.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true for cloning a repository, but not for copying a directory. The main reason to use git clone rather than copying the directory is because git clone will also copy the commit history of the repository. However, copying can be done on any directory, whereas git clone only works on a Git repository.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you make changes to the copied directory or cloned repository, the original will not change.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is true for both copying a directory and cloning a repository. In both cases, you’re making a copy that you can alter without changing the original.&lt;/p&gt;
&lt;p&gt;- The state of every file in the directory or repository is copied.&lt;/p&gt;
&lt;p&gt;This is true for both copying a directory and cloning a repository. In both cases, all the files are copied.
 &lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="git"></category><category term="github"></category></entry><entry><title>An illustrated introduction to adversarial validation part 1</title><link href="http://manishbarnwal.github.io/blog/2017/02/15/introduction_to_adversarial_validation/" rel="alternate"></link><updated>2017-02-15T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-15:blog/2017/02/15/introduction_to_adversarial_validation/</id><summary type="html">&lt;p&gt;You'd have heard about cross-validation - a common technique used in data-science process to avoid overfitting and many a times to tune the optimal parameters. Overfitting is when the model does well on training data but fails drastically on test data. The reason could be one of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The model is trying to map the exact findings of training data to test data instead of generalizing the patterns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The train data and test data are significantly different from each other i.e. they have not been derived from the same population.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;The problem&lt;/h3&gt;
&lt;p&gt;We will try to understand the second issue. What is the problem with the second issue? If you have participated in Kaggle like competitions, then you would know the way these competitions work. You are given a training data set and test dataset. You train your model on training data, predict on the test data and upload the predictions on Kaggle to get your rank.
&lt;br&gt;
&lt;br&gt;
What we typically do is divide the training data into train and validation data set. Validation data is used to get an idea of how your model will work on the test data. Now imagine if your train data and test data are different in terms of the population from where they've been derived. You won't see the same result in validation and test data. You see the problem here?
&lt;br&gt;
&lt;br&gt;
The use of validation data is to understand how the model is expected to perform on test data. But if train and test are not identically distributed, validation and test data would show different results.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;The solution&lt;/h3&gt;
&lt;p&gt;The solution is &lt;strong&gt;adversarial validation&lt;/strong&gt;. I got to know about this recently when I started participating in &lt;a href="https://numer.ai/"&gt;Numerai&lt;/a&gt; competition and read about this technique on fastml. Here's how it is done.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
1. &lt;strong&gt;Build a classifier to distinguish between training and test data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Combine your train and test data into one data. Create a response variable say isTest and assign it as 0 to all the rows in training data and 1 to all the rows in test data. Now your task is to build a classification model that will distinguish between the training and test data. This could be any classification model - logistic or random forest.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
2. &lt;strong&gt;Sort the predicted probabilities of training data in decreasing order&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once you have the model built, use this model to predict on the training data. You will get the fitted probabilities. Sort the probabilities in decreasing order i.e. the row having highest probability of being classified as test data comes to the top.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
3. &lt;strong&gt;Take the starting few rows as your validation set&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The starting few rows are now those rows of training data that resembles the most to test data. Take the starting few rows say 30% as your validation set. And the remaining as your train data to train your model.
&lt;br&gt;
&lt;br&gt;
Now, the accuracy metrics on your validation set should be similar to that on the test data. If your model works well on validation data, it should work well on test data. If you are interested in the implementation of what we just talked, head out to &lt;em&gt;&lt;a href="http://manishbarnwal.com/blog/2017/02/16/introduction_to_adversarial_validation/"&gt;this post&lt;/a&gt;&lt;/em&gt; for the part 2 where the code is written in R.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="adversarial-validation"></category></entry><entry><title>The curse of bias and variance [draft]</title><link href="http://manishbarnwal.github.io/blog/2017/02/08/the_curse_of_bias_and_variance/" rel="alternate"></link><updated>2017-02-08T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-02-08:blog/2017/02/08/the_curse_of_bias_and_variance/</id><summary type="html">&lt;p&gt;Statistics is the field of study where we try to draw conclusions about the population from a sample. Why do we talk about sample? Why can't we get the conclusions about the population directly from the population? Let me illustrate this by an example.
&lt;br&gt;
&lt;br&gt;
Let us say we want to understand which brand of beer do the people of Bangalore prefers? An interesting question. If I ask you this question, how would you approach this problem?
&lt;br&gt;
&lt;br&gt;
You can't go around asking each and every person their favorite beer? Or can we? No, we can't cover each and every individual because the 'population' is huge. One thing you can do is you may ask among your circle of friends their preference of beer and get an overall idea of the population. But we have a problem with this analysis. Do you see the problem? Your estimation is suffering from bias or we say your sample is biased. Biased sample is when it is not random. There is some form of personal preference in the choice of picking data.
&lt;br&gt;
&lt;br&gt;
In your case, chances are most of your friends will be of same age as you. Intuitively, I feel that your age also decides what kind of beer you like. Say, when I was in college I'd never heard of Corona and Kingfisher was the best beer I had tasted. So we can't estimate the best brand of beer that Bangalore prefers from the sample of your friends.
&lt;br&gt;
&lt;br&gt;
So population is a broader set of data that covers all the data points in the whole universe. Sample is a subset of that data. We try to infer the characteristics of the population from the sample or try to answer questions about the population from the sample. Getting or collecting population data is tough as explained in the above example (The favorite beer example).
&lt;br&gt;
&lt;br&gt;
I hope the concept of population and sample is clear now. It's normally not feasible to get the data for the complete population so we try to estimate parameters or findings of the population from a sample.
&lt;br&gt;
&lt;br&gt;
Another example. Not the beer but a rather completely different one - marriages.
&lt;br&gt;
&lt;br&gt;
Say, now we want to understand what are the factors that affect the age at which one gets married. Some of the factors that came to me without much thought are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Gender&lt;/li&gt;
&lt;li&gt;Love or arranged marriage&lt;/li&gt;
&lt;li&gt;Plan for higher studies&lt;/li&gt;
&lt;li&gt;Company type - Government of Private&lt;/li&gt;
&lt;li&gt;Salary&lt;/li&gt;
&lt;li&gt;Region&lt;/li&gt;
&lt;li&gt;Religion
&lt;br&gt;
&lt;br&gt;
Now we don't know the true relationship between marital age and the variables listed above. There should be a true function that maps response variable to the predictors but we don't know what that function is.
&lt;br&gt;
&lt;br&gt;
Let's say that true function is f.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;f: (gender, love-marriage, higher-studies, ...) --&amp;gt; marital age&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
Now we don't know the true f(). But we can estimate the true function using the data we may have from the past. It's easy to collect the data for the respective variables and the marital age. We are trying to come up with a function say fcap that resembles closely to the true function f.
&lt;br&gt;
&lt;br&gt;
Whenever we try to estimate true f from the data in hand, we will obviously get some error. This error can be categorized in two types:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reducible error&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the name suggests, this is something that the analyst has some control over. This can be reduced based on the kind of data you collect and the models (not all models are same right?)one uses to estimate the true f.
   This error can arise from a combination of 'bias' and 'variance'. We will talk about bias and variance in coming few paragraphs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Irreducible error&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the name suggests, this error is something that the analyst has not control over. There is always some information that is difficult to capture in data. There is always some randomness in the data and that is difficult to explain. This error can't be reduced using any model whatsoever.&lt;/p&gt;
&lt;p&gt;Let us dig deeper into reducible error. We will talk about bias and variance here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When we talk about estimating the true f, there are various models that can be used. Now not all models are same. Each has some characteristics of its own. A linear regression, the simplest model is different from say a random forest model. Bias is the error that captures how far is the predicted value (say predicted marital age given the variables like gender, love or arranged marriage, etc.) from the true value (actual marital age).
&lt;br&gt;
&lt;br&gt;
Now you may ask is there a relationship between the bias and the type of models used? Yes there is one.
Bias tends to decrease as the complexity of model increases i.e it is expected that the model error will decrease if we use a more complex model instead of a simple model. Now this is intuitive isn't it?
&lt;br&gt;
&lt;br&gt;
You may ask what is meant by complexity of a model or an example of it? Linear regression is a very simple model whereas random forest is a more complex model. Linear regression tries to fit just a line to the actual data and it assumes a linear relationship. A random forest model is more complex is the sense that it uses an ensemble of decision trees and is able to explain non-linear relationship as well.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;Variance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you estimate the true f, you use some data to train the model, that data is called training data. The data that the machine uses to train on, to find the patterns.
Now once your model is trained, you want to use this to predict on unseen data (the data that the model has not been trained on) that data is called test data.
&lt;br&gt;
&lt;br&gt;
There is always some variability in the training data and the test data. You can't expect both these data to be exactly same.
It is important to note that when you train your model, the model doesn't learn the exact values but instead try to find patterns so that when this pattern is seen on test data, the model is able to predict correctly.
Many complex model overfits the data i.e models that perform well on training data but fails drastically on test data.
&lt;br&gt;
&lt;br&gt;
How is variance related to the complexity of models?
As the complexity increases, the chances of overfitting increases i.e the variance increases. Coming to random forest and linear regression example, a random forest's variance is expected to be higher than that from a linear regression model.
&lt;br&gt;
&lt;br&gt;
So if we talk about complexity, bias and variance together, this is the relationship between them.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As the complexity of model increases, the bias decreases but the variance increases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
So there is a trade-off between bias and variance. You can't get both low bias and low variance at the same time. You will have to accept a trade-off. Do you now understand why we call it the curse of bias and variance? I hope you do.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts in the comments. Share this post with people who you think would enjoy reading this. Let's talk more of data-science.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category><category term="overfitting"></category><category term="bias"></category><category term="variance"></category></entry><entry><title>Visualization in ML is under-rated</title><link href="http://manishbarnwal.github.io/blog/2017/01/27/visualization_is_under-rated/" rel="alternate"></link><updated>2017-01-27T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-01-27:blog/2017/01/27/visualization_is_under-rated/</id><summary type="html">&lt;p&gt;Visualization is one of the most important pillars of data science. Every one wants to learn Machine learning but if you explain them the little tasks that involve the overall workflow of the process, it turns them off. Everyone just wants to do the cool stuff. They want to build models and be done with it. And I was one of them. I understand that feeling when you get the data and without much understanding of the features in the dataset, we just want to throw in the data to a model and hope that something good comes out.
&lt;br&gt;
&lt;br&gt;
I have participated in many hacks relating to ML and I used to just hope the trained model would do the task. Sadly, this thought always betrayed me. Even ensemble of various models may not work. Sometimes there are patterns in data which one gets to know only when one does EDA, when one plots a few graphs. One tries to understand the relationship of one feature with the other or do univariate analysis of the columns. An ML model is not always sufficient to understand the patterns in data.
&lt;br&gt;
&lt;br&gt;
After a point, the learning of a model becomes saturated. It can only perform to a certain point. If there is a pattern that you have identified, it would definitely help the model to better train on it.
&lt;br&gt;
&lt;br&gt;
I have learnt this hard way - You can't ignore EDA, visualization if you want to come in the top 1% of the leaderboard. Anyone and everyone can run a random forest model. Tuning the parameters is a little tedious but that too can be done with little practice. But finding the hidden patterns in the dataset, finding the relationship, understanding the little nuances in the data is an art. It's a skill. It takes practice. A lot of practice.
&lt;br&gt;
&lt;br&gt;
I have always wondered, how does the winners go about finding those patterns. Isn't there a course that could teach me these hacks to find the unseen patterns. Unfortunately, there are courses that teaches EDA, they teach you how to use ggplot2. But the thing I am talking about takes altogether a different mindset. One needs to be patient with this chunk of work. There are no defined paths to it. You just keep doing the EDA, observe the patterns, try to create new features based on this and once you have done this a hundred times, then you realize you finally understand this thing.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;So how do you master the art of EDA?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The short answer is practice. But then how do you practice EDA? You take shorter assignments, try to write the code snippet of common plots like histogram, scatterplot, bar-plot. And when you plot these graphs, I would strongly recommend to use &lt;em&gt;ggplot2&lt;/em&gt; if you are using R. In one of the talks, Hadley Wickham has strongly asserted that one should start learning visualization using &lt;em&gt;ggplot2&lt;/em&gt;. The syntax is very intuitive and makes plots interesting and beautiful. And when a person like Hadley advises a thing, you just follow it.
&lt;br&gt;
&lt;br&gt;
I have learnt more about R by looking at other people's codes. Look at the Kernels shared on Kaggle by the top performers of Kaggle. The kind of analysis they do is just mind-blowing. And the best part is they share the code for everyone.
&lt;br&gt;
&lt;br&gt;
In this post, I would just share the code snippets for the most common visualization tasks. Below are some of the most common plots to know about your data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Barplot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_bar(mapping = aes(x = as.factor(class))) +
  ggtitle(label = 'Plot of class of vehicle') +
  xlab('Class of vehicle') + ylab('Count')
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Scatterplot&lt;/h3&gt;
&lt;p&gt;The scatterplot is useful for displaying the relationship between two continuous variables, although it can also be used with one continuous and one categorical variable, or two categorical variables.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Linechart plot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_line(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;
I will add a tutorial on visualization in R using &lt;strong&gt;ggplot2&lt;/strong&gt; soon. Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>Random Forest explained intuitively</title><link href="http://manishbarnwal.github.io/blog/2016/10/18/random_forest_explained_intuitively/" rel="alternate"></link><updated>2016-10-18T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-18:blog/2016/10/18/random_forest_explained_intuitively/</id><summary type="html">&lt;p&gt;Random Forests algorithm has always fascinated me. I like how this algorithm can be easily explained to anyone without much hassle. One quick example, I use very frequently to explain the working of random forests is the way a company has multiple rounds of interview to hire a candidate. Let me elaborate.
&lt;br&gt;
&lt;br&gt;
Say, you appeared for the position of Statistical analyst at WalmartLabs. Now like most of the companies, you don't just have one round of interview. You have multiple rounds of interviews. Each of these interviews is chaired by independent panels. Each panel assess the candidate separately and independently. Generally, even the questions asked in these interviews differ from each other. Randomness is important here.
&lt;br&gt;
&lt;br&gt;
If you have heard about decision tree, then you are not very far from understanding what random forests are. Random forests is a forest of many decision trees. So you now understand why is it called forest.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Why is it called random then?&lt;/h3&gt;
&lt;p&gt;Say our dataset has 1,000 rows and 30 columns.&lt;/p&gt;
&lt;p&gt;There are two levels of randomness in this algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At row level&lt;/strong&gt;: Each of these decision trees gets a random sample of the training data (say 10%) i.e. each of these trees will be trained independently on 100 randomly chosen rows out of 1,000 rows of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At column level&lt;/strong&gt;: The second level of randomness is introduced at the column level. Not all the columns are passed into training each of the decision trees. Say we want only 10% of columns to be sent to each tree. This means a randomly selected 3 column will be sent to each tree. So for first decision tree, may be column C1, C2 and C4 were chosen. The next DT will have C4, C5, C10 as chosen columns and so on.
&lt;br&gt;
&lt;br&gt;
Let me draw an analogy now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let us now understand how interview selection process resembles a random forest algorithm. Each panel in the interview process is actually a decision tree. Each panel gives a result whether the candidate is a pass or fail and then a majority of these results is declared as final. Say there were 5 panels, 3 said yes and 2 said no. The final verdict will be yes.
&lt;br&gt;
&lt;br&gt;
Something similar happens in random forest as well. The results from each of the tree is taken and final result is declared accordingly. Voting and averaging is used to predict in case of classification and regression respectively.
&lt;br&gt;
&lt;br&gt;
With the advent of huge computational power at our disposal, we hardly think for even a second before we apply random forests. And very conveniently our predictions are made. Let us try to understand other aspects of this algorithm.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;When is a random forest a poor choice relative to other algorithms?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random forests doesn't train well on smaller datasets&lt;/strong&gt; as it fails to pick on the pattern. To simplify, say we know that 1 pen costs INR 1, 2 pens cost INR 2, 3 pens cost INR 6. In this case linear regression will easily estimate the cost of 4 pens but random forests will fail to come up with a good estimate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;There is a problem of interpretability with random forest.&lt;/strong&gt; You can't see or understand the relationship between the response and the independent variables. Understand that RF is a predictive tool and not a descriptive tool. You get variable importance but this may not suffice in many analysis of interests where the objective might be to see the relationship between response and the independent features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;time taken to train random forests&lt;/strong&gt; may sometimes be too huge as you train multiple decision trees. Also, in case of categorical variable, the time complexity increases exponentially. For a categorical column with n levels, RF tries split at 2^n -1 points to find the maximal splitting point.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case of regression problem, &lt;strong&gt;the range of values response variable can take&lt;/strong&gt; is determine by the values already available in the training dataset. Unlike linear regression, RF can't take on value outside the training data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What are the advantages of using random forest?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Since we are using multiple decision trees, &lt;strong&gt;the bias remains same as that of a single decision tree&lt;/strong&gt;. However, the variance decreases and thus we decrease the chances of overfitting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When all you care about is the predictions and &lt;strong&gt;want a quick and dirty way-out&lt;/strong&gt;, random forest comes to the rescue.
&lt;br&gt;
&lt;br&gt;
I will add in the R code snippets as well to get an idea of how this is executed soon.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="random forest"></category><category term="ml"></category><category term="data-science"></category></entry><entry><title>Improve runtime of Random Forest in R</title><link href="http://manishbarnwal.github.io/blog/2016/10/13/improve_runtime_random_forest_r/" rel="alternate"></link><updated>2016-10-13T10:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-13:blog/2016/10/13/improve_runtime_random_forest_r/</id><summary type="html">&lt;p&gt;There are two ways one can write the code to train a random forest model in R.
Both the ways are listed below.
&lt;br&gt;
&lt;br&gt;
A normal and frequent way of writing the command to train the random forest model is something like this.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;rfModel &amp;lt;- randomForest(Survived~. , data = trainSample[, -c(6, 8, 9)])
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Notice the &lt;strong&gt;~&lt;/strong&gt; sign. We call this the formula way of writing.
&lt;br&gt;
&lt;br&gt;
Another way of writing the command to train the random forest model is shown below.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;rfModel &amp;lt;- randomForest(y=trainSample$Survived, x= trainSample[, -c(6, 8, 9)], data = trainSample)
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Here we explicitly mention the y-variable and the x variables.
&lt;br&gt;
&lt;br&gt;
Recently, I was working on a huge dataset where the task was to predict a variable based on some 12 independent variables. The dataset had close to 1.3 million rows. I tried train the model using the first method i.e the formula way. Sadly, I had to kill the task as it was taking a lot of time.
&lt;br&gt;
&lt;br&gt;
It was then, I got to know that if you train the model using the second format command, the code runs relatively faster. When investigated further, I got to know that the reason for the difference in time is that the code for random forest is written in C and when one writes in the &lt;em&gt;formula format&lt;/em&gt;, the x and y variables are explicitly converted into proper format and this is what takes time. &lt;a href="http://stats.stackexchange.com/questions/116952/improving-randomforest-running-time"&gt;ref&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;
Even the help page of randomForest in R says the same thing.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;?randomForest
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For large data sets, especially those with large number of variables, calling randomForest via the formula interface is not advised: There may be too much overhead in handling the formula.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
Look just before the 'Authors' section.&lt;/p&gt;
&lt;p&gt;The time difference is not significant for smaller datasets. However, for larger datasets I would suggest using the second format.&lt;/p&gt;
&lt;p&gt;To illustrate the time difference, I have taken a dataset having 2,23,874 rows and 6 columns.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;system.time( rfModel1 &lt;- randomForest(DepDelay~. , data = subset(df, select = c(1,2,3,4,5, 13))) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;user  system elapsed&lt;/p&gt;
&lt;p&gt;94.582  11.356 &lt;strong&gt;109.311&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;system.time( rfModel2 &lt;-randomForest(y=df$DepDelay, x=df[,c(1,2,3,4,5)], data=subset(df, select = c(1,2,3,4,5, 13))) )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;user  system elapsed&lt;/p&gt;
&lt;p&gt;93.499  10.248 &lt;strong&gt;106.205&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This shows a time difference of 3 seconds. Note that all of these columns were numeric and if one takes categorical columns as well, the time difference would be more.
&lt;br&gt;
&lt;br&gt;
Also, this dataset is not huge. One would appreciate this more on larger datasets.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="life"></category><category term="learnings"></category><category term="thoughts"></category></entry><entry><title>How to install a package of a particular version in R</title><link href="http://manishbarnwal.github.io/blog/2016/10/05/install_a_package_particular_version_in_R/" rel="alternate"></link><updated>2016-10-05T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-05:blog/2016/10/05/install_a_package_particular_version_in_R/</id><summary type="html">&lt;p&gt;I recently tried installing &lt;em&gt;caret&lt;/em&gt; package in R using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.packages('caret', dependencies=T)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;Normally this installation of package works and I continue to work with the functions associated with the package.
When I tried including the package using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;library(caret)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;I got the following error.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;Error in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :there is no package called 'pbkrtest'
In addition: Warning message:package 'caret' was built under R version 3.2.5
Error: package or namespace load failed for 'caret'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R was not able to install this dependency package- &lt;em&gt;pbkrtest&lt;/em&gt;. So I tried installing it separately, again using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.package('pbkrtest', dependencies=T)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;This too didn't work. And the error again this time was
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;Error in loadNamespace(j &amp;lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : there is no package called ‘pbkrtest’ In addition: Warning message: package ‘pbkrtest’ was built under R version 3.2.3
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;So this particular package is not available for the R version that I am using.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;What is the way out?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We install an older version of this package.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you install an older version of some package in R?&lt;/h3&gt;
&lt;p&gt;I will go ahead with package-&lt;em&gt;pbkrtest&lt;/em&gt; to illustrate the steps.&lt;/p&gt;
&lt;p&gt;Open this &lt;a href="https://cran.r-project.org/src/contrib/Archive/pbkrtest"&gt;url&lt;/a&gt;. You will see various versions of the package listed there. Choose the version you want to install. Let's say I want to install the &lt;em&gt;pbkrtest_0.4-5.tar.gz&lt;/em&gt; version.
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We now create a variable &lt;em&gt;packageUrl&lt;/em&gt; which contains all this information and assign it the url of the page.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;packageUrl&amp;lt;- "https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-5.tar.gz"
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;You then install this version of the package using
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;install.packages(packageUrl, repos=NULL, type='source')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;And we are done!
&lt;br&gt;
&lt;br&gt;
Now if you have some other package to install, just replace the last word &lt;em&gt;pbkrtest&lt;/em&gt; with the package name you want to install and you will be able to see all the older versions of that package. For &lt;em&gt;Rcpp&lt;/em&gt; the link would be &lt;a href="https://cran.r-project.org/src/contrib/Archive/Rcpp"&gt;url&lt;/a&gt;. Choose the version you want to install and follow the steps stated above.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;
&lt;p&gt;Cheers,&lt;/p&gt;
&lt;p&gt;Manish&lt;/p&gt;</summary><category term="R"></category></entry><entry><title>Shell commands come in handy for a data scientist</title><link href="http://manishbarnwal.github.io/blog/2016/09/30/shell_commands_for_data_scientist/" rel="alternate"></link><updated>2016-09-30T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-30:blog/2016/09/30/shell_commands_for_data_scientist/</id><summary type="html">&lt;p&gt;I am no expert of shell commands. I have been using them for quite some time and thought I give an attempt to list down the most common commands. I am writing these mostly from the perspective of a data-science guy. Let us get started.
&lt;br&gt;
&lt;br&gt;
I will use the file- ‘data.txt’ to illustrate these commands. ‘data.txt’ is a file having 200 rows and 8 columns. You can access the &lt;a href="https://github.com/ManishBarnwal/LearnNew/blob/master/LearnShellCommands/data.txt"&gt;data&lt;/a&gt; here.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;cat&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Throws the the contents of the entire file at your terminal
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;cat data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
We don’t want to bombard our terminal with the complete content of the file. Instead, if you want to have a complete look at the file, open the file in vim editor&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;vim&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Opens the file in the editor  &lt;br /&gt;
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;vim data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;head&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Gives the top 10 rows of the text file at your terminal
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;head data.txt
    &lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;tail&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Gives the bottom 10 rows of the text file at your terminal
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;tail -n 2 data.txt -- This will give you the bottom 2 rows of the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4&gt;The piping operator&lt;/h4&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;cat data.txt | head
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Notice the &lt;strong&gt;|&lt;/strong&gt; operator. This is called pipe operator. Piping is a concept wherein you can perform a sequence of operations in a single command.
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;So what exactly is piping?&lt;/h3&gt;
&lt;p&gt;A pipe is a facility of the shell that makes it easy to chain together multiple commands. When used between two Unix commands, it means that output from the first command should become the input to the second command. Just read the &lt;strong&gt;|&lt;/strong&gt; in the command as &lt;em&gt;pass the data onto&lt;/em&gt;. More on this operator later in the post.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;wc&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;wc&lt;/strong&gt; is a fairly useful shell command that lets you count the number of lines(-l), words(-w) or characters(-c) in a given file
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wc -l data.txt -- gives you the number of lines in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;wc -w data.txt -- gives you the number of words in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;wc -c data.txt -- gives you the number of characters in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;head -n 1 data.txt| wc -w -- gives you the number of columns in the file
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;grep&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider ‘grep’ as a command to filter on the results you get. You may want to print all the lines in your file which have a particular phrase. Say for example you want to see people who are ‘Very Happy’. You simply pass this to grep command.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;grep 'Very Happy' data.txt | head
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Let us say, we want to count the number of users who are ‘Not Happy’
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;grep 'Not Happy' data.txt | wc -l
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;sort&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you want to sort the data based on some column, say ‘Score’; ‘Score’ is the 3rd column in the file- data.txt&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;sort -t ',' -k 3 -n -r data.txt |head -5
-- gives you the top 10 rows having 'Very Happy'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Explanation: -t is used to specify the delimiter; ‘,’ in this case.&lt;/p&gt;
&lt;p&gt;If the delimiter is ‘\t’, we don’t need to specify -t argument. Space is taken as delimiter by default.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k is used to specify the column based on which you want to sort the data; 3 in this case&lt;/li&gt;
&lt;li&gt;n is to specify that sorting is to be done numerically&lt;/li&gt;
&lt;li&gt;r is to imply that the sorting is descending
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;cut&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This command gives you only specific column. Say you want to see only the 4th column of the file.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 4 |head
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
Explanation: ‘,’ is the delimiter. 4 is the column number that you want to see.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;uniq&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Do not confuse this command for ‘unique’. It is slightly different. This &lt;strong&gt;removes sequential duplicates&lt;/strong&gt;.
So if you want to get unique values from a column, you need to first sort the data and then use this &lt;em&gt;uniq&lt;/em&gt; command in sequence.&lt;/p&gt;
&lt;p&gt;To get the unique of a column, say the 2nd column
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 2 |sort|uniq
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
This command could be used with argument -c to count the occurrence of these distinct values. Something like to count distinct in SQL.
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cut data.txt -d ',' -f 2 |sort|uniq-c
&lt;/code&gt;&lt;/pre&gt;
&lt;br&gt;
&lt;strong&gt;tr&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;strong&gt;tr&lt;/strong&gt; stands for translate&lt;/p&gt;
&lt;p&gt;‘Find and Replace’ function that we have in excel. Yes we have that in UNIX as well.
A typical use of this command that I use on regular basis is that I get the file from HIVE which are tab delimited. And say I want to convert it to ‘,’ delimited.&lt;/p&gt;
&lt;p&gt;You may also want to replace certain characters in file with something else using the &lt;em&gt;tr&lt;/em&gt; command.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="shell"&gt;cat data.txt | tr ',' '\t'  -- Changed ',' delimited to '\t' delimited
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
&lt;strong&gt;Save to a new file or append to an existing file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;gt;&lt;/strong&gt; and &lt;strong&gt;&amp;gt;&amp;gt;&lt;/strong&gt; operator
Say you want to save the output of operations to some file. You use ‘&amp;gt;’ or ‘&amp;gt;&amp;gt;’ depending on whether you want it to be a new file or you want to append it to an existing file.
&lt;br&gt;
&lt;br&gt;
I will update this list as and when I see a command deserving enough to be in a data scientist’s toolbox.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="shell-commands"></category></entry><entry><title>ROC and AUC - The three lettered acronyms</title><link href="http://manishbarnwal.github.io/blog/2016/09/26/three_letter_acronym_roc_and_auc/" rel="alternate"></link><updated>2016-09-26T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-26:blog/2016/09/26/three_letter_acronym_roc_and_auc/</id><summary type="html">&lt;p&gt;I don't feel bad to confess this that &lt;em&gt;ROC curve&lt;/em&gt;, &lt;em&gt;AUC&lt;/em&gt;, &lt;em&gt;True-positive&lt;/em&gt; and related terms took quite some time for me to understand. If today I contemplate on the reasons why I found this topic confusing. The first would be there are not many resources that explains intuitively what these mean. They just jump to the terms and the mathematical formula for them. The second being I had not used them even in my project work. You see the project work is never enough for all your learnings.
&lt;br&gt;
&lt;br&gt;
In this post, I will try to explain my understanding both intuitively and mathematically.&lt;/p&gt;
&lt;p&gt;I will illustrate this concept with the help of an example.
&lt;br&gt;
&lt;br&gt;
There is a bank say &lt;em&gt;SBI&lt;/em&gt; that wants to understand which of its future customers would default on a loan granted by the bank. The bank would already have historical data from the past years that says how many of the customers have defaulted, what type of customer were they and many other information about the past loans.
&lt;br&gt;
&lt;br&gt;
It is very rare that customers default. To give an example, say the bank has data having 1000 rows that contains information like &lt;em&gt;age&lt;/em&gt;, &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;income&lt;/em&gt;, &lt;em&gt;marital-status&lt;/em&gt;, other related columns and then a variable named &lt;em&gt;default&lt;/em&gt; that says whether the customer has defaulted or not.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, very few customers default on their loans. So a realistic example would be say 100 customers defaulting out of a total of 1000 customers.
&lt;br&gt;
&lt;br&gt;
The bank is interested in knowing if a customer would default or not. This is a typical binary classification problem. The bank would really be interested in the customers who are likely to default. Let us now try to understand terms like &lt;em&gt;True-positive&lt;/em&gt;, &lt;em&gt;True-negative&lt;/em&gt;, &lt;em&gt;False-positive&lt;/em&gt; and other related terms.
&lt;br&gt;
&lt;br&gt;
There are two levels in the &lt;em&gt;default&lt;/em&gt; variable that we are trying to predict-&lt;strong&gt;default&lt;/strong&gt; and &lt;strong&gt;not-default&lt;/strong&gt;. One has to define first whether &lt;strong&gt;default&lt;/strong&gt; will be treated as positive or &lt;strong&gt;not-default&lt;/strong&gt; as positive. It is just a convention. Normally, the class of interest is treated as positive. So in the bank's case, what do you think we should take as positive? You might have guessed it correctly, we will treat &lt;strong&gt;default&lt;/strong&gt; as positive and &lt;strong&gt;not-default&lt;/strong&gt; as negative.
&lt;br&gt;
&lt;br&gt;
We train a binary classification model on this dataset having 1000 rows. The predictions that the model makes for the training data will either be correct or incorrect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for incorrect predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was negative i.e classifying a customer as default when in actual he is not-default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was positive i.e classifying a customer as not-default when in actual he is default&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for correct predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was positive i.e correctly classifying default as default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was negative i.e correctly classifying not-default as not-default
&lt;br&gt;
&lt;br&gt;
Now, FP, FN are incorrect predictions (notice False in the name) as the name suggests and TP, TN are the correct predictions. Don't be in a hurry here. Take some time to digest these 2 lettered acronyms. Read them loud. Take a notebook and write them down on your own.
&lt;br&gt;
&lt;br&gt;
Once we are comfortable with these terms we will discuss about something called confusion-matrix. Don't get confused yet. If you understood TP, TN, FP, FN then confusion-matrix is just a matrix having these values. The diagonal elements contain the count of correct predictions (TP, TN) whereas the off-diagonal contain the count of incorrect predictions (FP, FN). The rows are the predicted and the columns are the actual.
This looks something like this.
&lt;Insert a picture&gt;
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why we need TPR, FPR if we already have mis-classification error?&lt;/h3&gt;
&lt;p&gt;The data I describe above is a typical case of imbalanced data wherein one of the class is having majority of observations (90% non-defaulters (negatives) in our data) and the remaining class is a minority (only 10% defaulters (positives)). In such cases, the predictions on new dataset will be skewed towards negatives i.e the model will classify a lot of defaulters (positives) into negatives. The bank can't afford to have such predictions. The bank wants to know for sure the defaulters (positives). Imagine, the loss to bank if the model classify a probable defaulter (positive) to non-defaulter.
&lt;br&gt;
&lt;br&gt;
In such cases, accuracy corresponding to mis-classification alone is not acceptable. The bank would be more interested in correctly classifying the positives into positive i.e the bank wants to classify the defaulters into defaulters without fail.
&lt;br&gt;
&lt;br&gt;
Comes into picture &lt;em&gt;TPR&lt;/em&gt;, &lt;em&gt;TNR&lt;/em&gt;, &lt;em&gt;FPR&lt;/em&gt;, &lt;em&gt;FNR&lt;/em&gt;. These 3 lettered acronyms are nothing but the rates of TP, TN, FP and FN respectively.
Below is the formula. To digest the formula, let's move to our data having 1000 rows - 100 defaulters (positives) and 900 non-defaulters(negatives). Suppose we employed a logistic regression that classified 80 defaulters correctly and incorrectly classified 90 non-defaulters as defaulters.
TPR = 80 / 100 = 0.8
FPR = 90/ 900 = 0.1
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How does the TPR and FPR gets calculated?&lt;/h3&gt;
&lt;p&gt;Whenever you do any classification, the model always gives you probabilities of each observation getting classified in each of the classes.
Based on what cut-off you choose, you will get different predictions for the data and hence different TPR and FPR overall. You can choose whatever probability cut-off between [0, 1] and you will get different tuples of TPR and FPR.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TPR and FPR are be generated for each of the probability value one chooses. And these values are then plotted on an ROC curve.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
One can plot these tuples (probability, FPR, TPR) on a graph. You know what this graph is called? ROC-Receiver Operating Characteristics.
There is a trade-off between TPR and FPR. Depending on the requirement one can choose the probability cut-off that best fulfills their purpose. For instance, in the bank's case, the bank wants to not miss a single defaulter(positives) i.e the bank wants a higher TPR. The ROC curve looks something like this.&lt;/p&gt;
&lt;p&gt;Alright, all this is clear to me.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What about AUC?&lt;/h3&gt;
&lt;p&gt;AUC is nothing but the area under curve of ROC curve. Let's say we built a logistic regression model that gave us the probabilities for each row. Now we try with probability cut-offs from 0.1 to 1.0 with step size of 0.1 i.e we would have 10 probabilites to try with and corresponding to each of the 10 values we would have corresponding (FPR, TPR). If we plot these values on a graph we would get a graph having 10 points. This 10 point graph is what we call an ROC curve and the area under this graph is called AUC.
&lt;br&gt;
&lt;br&gt;
The AUC is a common evaluation metric for binary classification problems. Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1: if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5.
&lt;br&gt;
&lt;br&gt;
One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets.
&lt;br&gt;
&lt;br&gt;
The larger the area the better. If we have to choose between two classifiers having different AUCs, we choose the one having larger AUC.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you decide what probability cut-off should you choose to classify them into either of the classes?&lt;/h3&gt;
&lt;p&gt;Some say they take a cut-off of 0.5 i.e observation having probability greater than 0.5 will be classified as positive or else negative. Do you see the problem in here? Try thinking.
&lt;br&gt;
&lt;br&gt;
Come into picture ROC curve. Look at the ROC curve and depending on what value of TPR or FPR you want from your model, you take probability corresponding to that point.&lt;/p&gt;
&lt;p&gt;ROC, AUC can easily be plotted and calculated using modern analytical tools like R or Python. But I would suggest for better understanding of this topic, try writing your own code.
&lt;br&gt;
&lt;br&gt;
I will make an attempt of the same soon and will share it on this space. Keep learning and sharing.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="roc"></category><category term="auc"></category></entry><entry><title>Vim/Vi editor shortcuts</title><link href="http://manishbarnwal.github.io/blog/2016/09/22/vim_shortcuts/" rel="alternate"></link><updated>2016-09-22T04:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-22:blog/2016/09/22/vim_shortcuts/</id><summary type="html">&lt;p&gt;Repetitive tasks should be done using as many shortcuts as possible. You are not doing  anything new and hence not even an extra minute should be spent on doing the same. This post refers to the shortcuts that come in handy when working on the vi/vim editor.&lt;/p&gt;
&lt;p&gt;This is not an exhaustive list. These are the ones I use frequently. Feel free to comment down your favorite shortcuts.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Navigation keys&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves cursor to the start of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves cursor to the end of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;w&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves forward one word&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;b&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves backward one word&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;G&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves to the end of the file&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1 + G&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Moves to the beginning of file
 &lt;br&gt;
 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Delete text&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;dw&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes a word ahead of the cursor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;db&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes a word behind the cursor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;d0&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line till beginning&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;d$&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line till the end&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dd&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the complete line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10dd&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes the following 5 lines&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dG&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Deletes till the end of the file
 &lt;br&gt;
 &lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Undo/Redo operation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;u&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Undo the last operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + r&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Redo the last undo
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Search and Replace keys&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;/search_text&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds 'search_text' in file going forward&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;?search_text&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds 'search_text' in file going backward&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds the next occurrence of 'search_text'&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;N&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Finds the previous occurrence of 'search_text'&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;:%s/replace_what/replace_with&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Replaces first occurrence&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:%s/replace_what/replace_with/g&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Replaces all occurrences globally&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:%s/replace_what/replace_with/c&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Asks for confirmation
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Save and quit&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;:q!&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Force quit without saving&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:wq&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Saves the changes made to the document&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:wq!&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Forcefully saves the changes made to the document&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;:w new_file_name&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Saves the file to a new file named new_file_name
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Command line shortcuts&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + a&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Brings to the beginning of the line&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ctrl + e&lt;/strong&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;   Brings to the end of the line&lt;/li&gt;
&lt;/ul&gt;</summary><category term="vi/vim"></category></entry><entry><title>When R package is not available across the cluster</title><link href="http://manishbarnwal.github.io/blog/2016/08/02/r_packages_not_in_cluster/" rel="alternate"></link><updated>2016-08-02T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-08-02:blog/2016/08/02/r_packages_not_in_cluster/</id><summary type="html">&lt;p&gt;When deploying R codes across the cluster, many a times the reason for the failure of the task is unavailability of a particular package across all nodes of the cluster. We wait for someone to get the package installed across all the nodes. This may take some days. Do we wait for them? &lt;em&gt;Naah!&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
Presenting a temporary solution that one of my colleague came up with. I have used this technique and this works smoothly.
&lt;br&gt;
&lt;br&gt;
The following are the steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install the package you require on one of the edge nodes into a local directory&lt;ul&gt;
&lt;li&gt;Create a local directory. Let's say our directory name is &lt;em&gt;rPackages&lt;/em&gt;&lt;br /&gt;
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="bash"&gt;mkdir rPackages
    &lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Install the required package, say 'randomForest' in the directory just created
    &lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
    &lt;code class="r"&gt;install.packages('randomForest', repos=’repo_name', lib='rPackages/')
    Note that you need to choose the appropriate repo_name. The one that your company allows.
    &lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;
2. Check if you can load the package from this local directory
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt;library(randomForest, lib.loc='rPackages/')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
3. Create zip file of “dir_location” using command
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;zip -r rPackages.zip rPackages/
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
4. Add this zip file in your HIVE hql (or anything else)
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;add file rPackages.zip;
Don’t forget the semicolon
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
5. Unzip the file inside R script i.e. each reducer will have &lt;em&gt;rPackages&lt;/em&gt; directory now
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;unzip('rPackages.zip', overwrite=TRUE)
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
6. Load the package now
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="bash"&gt;library(randomForest, lib.loc='rPackages/')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;And you’re done! Remember, you have to build the package where you want to use, because built packages are OS dependent.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="big data"></category><category term="R"></category></entry></feed>