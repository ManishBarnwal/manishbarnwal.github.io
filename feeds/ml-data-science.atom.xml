<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Manish Barnwal</title><link href="http://manishbarnwal.github.io/" rel="alternate"></link><link href="http://manishbarnwal.github.io/feeds/ml-data-science.atom.xml" rel="self"></link><id>http://manishbarnwal.github.io/</id><updated>2017-01-27T08:00:00-02:00</updated><entry><title>Visualization aka EDA is way too under-rated</title><link href="http://manishbarnwal.github.io/blog/2017/01/27/visualization_is_under-rated/" rel="alternate"></link><updated>2017-01-27T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2017-01-27:blog/2017/01/27/visualization_is_under-rated/</id><summary type="html">&lt;p&gt;Visualization is one of the most important pillars of data science. Every one wants to learn Machine learning but if you explain them the little tasks that involve the overall workflow of the process, it turns them off. Everyone just wants to do the cool stuff. They want to build models and be done with it. And I was one of them. I understand that feeling when you get the data and without much understanding of the features in the dataset, we just want to throw in the data to a model and hope that something good comes out.
&lt;br&gt;
&lt;br&gt;
I have participated in many hacks relating to ML and I used to just hope the trained model would do the task. Sadly, this thought always betrayed me. Even ensemble of various models may not work. Sometimes there are patterns in data which one gets to know only when one does EDA, when one plots a few graphs. One tries to understand the relationship of one feature with the other or do univariate analysis of the columns. An ML model is not always sufficient to understand the patterns in data.
&lt;br&gt;
&lt;br&gt;
After a point, the learning of a model becomes saturated. It can only perform to a certain point. If there is a pattern that you have identified, it would definitely help the model to better train on it.
&lt;br&gt;
&lt;br&gt;
I have learnt this hard way - You can't ignore EDA, visualization if you want to come in the top 1% of the leaderboard. Anyone and everyone can run a random forest model. Tuning the parameters is a little tedious but that too can be done with little practice. But finding the hidden patterns in the dataset, finding the relationship, understanding the little nuances in the data is an art. It's a skill. It takes practice. A lot of practice.
&lt;br&gt;
&lt;br&gt;
I have always wondered, how does the winners go about finding those patterns. Isn't there a course that could teach me these hacks to find the unseen patterns. Unfortunately, there are courses that teaches EDA, they teach you how to use ggplot2. But the thing I am talking about takes altogether a different mindset. One needs to be patient with this chunk of work. There are no defined paths to it. You just keep doing the EDA, observe the patterns, try to create new features based on this and once you have done this a hundred times, then you realize you finally understand this thing.
&lt;br&gt;
&lt;br&gt;
&lt;strong&gt;So how do you master the art of EDA?&lt;/strong&gt;
The short answer is practice. But then how do you practice EDA? You take shorter assignments, try to write the code snippet of common plots like histogram, scatterplot, bar-plot. And when you plot these graphs, I would strongly recommend to use &lt;em&gt;ggplot2&lt;/em&gt; if you are using R. In one of the talks, Hadley Wickham has strongly asserted that one should start learning visualization using &lt;em&gt;ggplot2&lt;/em&gt;. The syntax is very intuitive and makes plots interesting and beautiful. And when a person like Hadley advises a thing, you just follow it.
&lt;br&gt;
&lt;br&gt;
I have learnt more about R by looking at other people's codes. Look at the Kernels shared on &lt;a href="www.Kaggle.com"&gt;Kaggle&lt;/a&gt; by the top performers of Kaggle. The kind of analysis they do is just mind-blowing. And the best part is they share the code for everyone.&lt;/p&gt;
&lt;p&gt;In this post, I would just share the code snippets for the most common visualization tasks. Below are some of the most common plots to know about your data.&lt;/p&gt;
&lt;h3&gt;Barplot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_bar(mapping = aes(x = as.factor(class))) +
  ggtitle(label = 'Plot of class of vehicle') +
  xlab('Class of vehicle') + ylab('Count')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;h3&gt;Scatterplot&lt;/h3&gt;
&lt;p&gt;The scatterplot is useful for displaying the relationship between two continuous variables, although it can also be used with one continuous and one categorical variable, or two categorical variables.&lt;/p&gt;
&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Linechart plot&lt;/h3&gt;
&lt;p&gt;&lt;pre style="font-size:60%; padding:7px; margin:0em;"&gt;
&lt;code class="r"&gt; ggplot(data = mpg) +
  geom_line(mapping = aes(x = displ, y = hwy)) +
  ggtitle(label = 'Plot of engine size v/s mileage') +
  xlab('Engine size (litres)') + ylab('Highway mileage')
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;I will add a tutorial on visualization in R using &lt;strong&gt;ggplot2&lt;/strong&gt; soon. Did you find the article useful? If you did, share your thoughts in the comments.&lt;/p&gt;</summary><category term="ML"></category><category term="data science"></category></entry><entry><title>Random Forest explained intuitively</title><link href="http://manishbarnwal.github.io/blog/2016/10/18/random_forest_explained_intuitively/" rel="alternate"></link><updated>2016-10-18T08:00:00-02:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-10-18:blog/2016/10/18/random_forest_explained_intuitively/</id><summary type="html">&lt;p&gt;Random Forests algorithm has always fascinated me. I like how this algorithm can be easily explained to anyone without much hassle. One quick example, I use very frequently to explain the working of random forests is the way a company has multiple rounds of interview to hire a candidate. Let me elaborate.
&lt;br&gt;
&lt;br&gt;
Say, you appeared for the position of Statistical analyst at WalmartLabs. Now like most of the companies, you don't just have one round of interview. You have multiple rounds of interviews. Each of these interviews is chaired by independent panels. Each panel assess the candidate separately and independently. Generally, even the questions asked in these interviews differ from each other. Randomness is important here.
&lt;br&gt;
&lt;br&gt;
If you have heard about decision tree, then you are not very far from understanding what random forests are. Random forests is a forest of many decision trees. So you now understand why is it called forest.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Why is it called random then?&lt;/h3&gt;
&lt;p&gt;Say our dataset has 1,000 rows and 30 columns.&lt;/p&gt;
&lt;p&gt;There are two levels of randomness in this algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At row level&lt;/strong&gt;: Each of these decision trees gets a random sample of the training data (say 10%) i.e. each of these trees will be trained independently on 100 randomly chosen rows out of 1,000 rows of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;At column level&lt;/strong&gt;: The second level of randomness is introduced at the column level. Not all the columns are passed into training each of the decision trees. Say we want only 10% of columns to be sent to each tree. This means a randomly selected 3 column will be sent to each tree. So for first decision tree, may be column C1, C2 and C4 were chosen. The next DT will have C4, C5, C10 as chosen columns and so on.
&lt;br&gt;
&lt;br&gt;
Let me draw an analogy now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let us now understand how interview selection process resembles a random forest algorithm. Each panel in the interview process is actually a decision tree. Each panel gives a result whether the candidate is a pass or fail and then a majority of these results is declared as final. Say there were 5 panels, 3 said yes and 2 said no. The final verdict will be yes.
&lt;br&gt;
&lt;br&gt;
Something similar happens in random forest as well. The results from each of the tree is taken and final result is declared accordingly. Voting and averaging is used to predict in case of classification and regression respectively.
&lt;br&gt;
&lt;br&gt;
With the advent of huge computational power at our disposal, we hardly think for even a second before we apply random forests. And very conveniently our predictions are made. Let us try to understand other aspects of this algorithm.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;When is a random forest a poor choice relative to other algorithms?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random forests doesn't train well on smaller datasets&lt;/strong&gt; as it fails to pick on the pattern. To simplify, say we know that 1 pen costs INR 1, 2 pens cost INR 2, 3 pens cost INR 6. In this case linear regression will easily estimate the cost of 4 pens but random forests will fail to come up with a good estimate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;There is a problem of interpretability with random forest.&lt;/strong&gt; You can't see or understand the relationship between the response and the dependent variables. Understand that RF is a predictive tool and not a descriptive tool. You get variable importance but this may not suffice in many analysis of interests where the objective might be to see the relationship between response and the independent features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;time taken to train random forests&lt;/strong&gt; may sometimes be too huge as you train multiple decision trees. Also, in case of categorical variable, the time complexity increases exponentially. For a categorical column with n levels, RF tries split at 2^n -1 points to find the maximal splitting point.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In case of regression problem, &lt;strong&gt;the range of values response variable can take&lt;/strong&gt; is determine by the values already available in the training dataset. Unlike linear regression, RF can't take on value outside the training data.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What are the advantages of using random forest?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Since we are using multiple decision trees, &lt;strong&gt;the bias remains same as that of a single decision tree&lt;/strong&gt;. However, the variance decreases and thus we decrease the chances of overfitting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When all you care about is the predictions and &lt;strong&gt;want a quick and dirty way-out&lt;/strong&gt;, random forest comes to the rescue.
&lt;br&gt;
&lt;br&gt;
I will add in the R code snippets as well to get an idea of how this is executed soon.
&lt;br&gt;
&lt;br&gt;
Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="random forest"></category><category term="ml"></category><category term="data-science"></category></entry><entry><title>ROC and AUC - The three lettered acronyms</title><link href="http://manishbarnwal.github.io/blog/2016/09/26/three_letter_acronym_roc_and_auc/" rel="alternate"></link><updated>2016-09-26T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-09-26:blog/2016/09/26/three_letter_acronym_roc_and_auc/</id><summary type="html">&lt;p&gt;I don't feel bad to confess this that &lt;em&gt;ROC curve&lt;/em&gt;, &lt;em&gt;AUC&lt;/em&gt;, &lt;em&gt;True-positive&lt;/em&gt; and related terms took quite some time for me to understand. If today I contemplate on the reasons why I found this topic confusing. The first would be there are not many resources that explains intuitively what these mean. They just jump to the terms and the mathematical formula for them. The second being I had not used them even in my project work. You see the project work is never enough for all your learnings.
&lt;br&gt;
&lt;br&gt;
In this post, I will try to explain my understanding both intuitively and mathematically.&lt;/p&gt;
&lt;p&gt;I will illustrate this concept with the help of an example.
&lt;br&gt;
&lt;br&gt;
There is a bank say &lt;em&gt;SBI&lt;/em&gt; that wants to understand which of its future customers would default on a loan granted by the bank. The bank would already have historical data from the past years that says how many of the customers have defaulted, what type of customer were they and many other information about the past loans.
&lt;br&gt;
&lt;br&gt;
It is very rare that customers default. To give an example, say the bank has data having 1000 rows that contains information like &lt;em&gt;age&lt;/em&gt;, &lt;em&gt;gender&lt;/em&gt;, &lt;em&gt;income&lt;/em&gt;, &lt;em&gt;marital-status&lt;/em&gt;, other related columns and then a variable named &lt;em&gt;default&lt;/em&gt; that says whether the customer has defaulted or not.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, very few customers default on their loans. So a realistic example would be say 100 customers defaulting out of a total of 1000 customers.
&lt;br&gt;
&lt;br&gt;
The bank is interested in knowing if a customer would default or not. This is a typical binary classification problem. The bank would really be interested in the customers who are likely to default. Let us now try to understand terms like &lt;em&gt;True-positive&lt;/em&gt;, &lt;em&gt;True-negative&lt;/em&gt;, &lt;em&gt;False-positive&lt;/em&gt; and other related terms.
&lt;br&gt;
&lt;br&gt;
There are two levels in the &lt;em&gt;default&lt;/em&gt; variable that we are trying to predict-&lt;strong&gt;default&lt;/strong&gt; and &lt;strong&gt;not-default&lt;/strong&gt;. One has to define first whether &lt;strong&gt;default&lt;/strong&gt; will be treated as positive or &lt;strong&gt;not-default&lt;/strong&gt; as positive. It is just a convention. Normally, the class of interest is treated as positive. So in the bank's case, what do you think we should take as positive? You might have guessed it correctly, we will treat &lt;strong&gt;default&lt;/strong&gt; as positive and &lt;strong&gt;not-default&lt;/strong&gt; as negative.
&lt;br&gt;
&lt;br&gt;
We train a binary classification model on this dataset having 1000 rows. The predictions that the model makes for the training data will either be correct or incorrect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for incorrect predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was negative i.e classifying a customer as default when in actual he is not-default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was positive i.e classifying a customer as not-default when in actual he is default&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;There will be two cases for correct predictions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predicting positive when the actual was positive i.e correctly classifying default as default&lt;/li&gt;
&lt;li&gt;Predicting negative when the actual was negative i.e correctly classifying not-default as not-default
&lt;br&gt;
&lt;br&gt;
Now, FP, FN are incorrect predictions (notice False in the name) as the name suggests and TP, TN are the correct predictions. Don't be in a hurry here. Take some time to digest these 2 lettered acronyms. Read them loud. Take a notebook and write them down on your own.
&lt;br&gt;
&lt;br&gt;
Once we are comfortable with these terms we will discuss about something called confusion-matrix. Don't get confused yet. If you understood TP, TN, FP, FN then confusion-matrix is just a matrix having these values. The diagonal elements contain the count of correct predictions (TP, TN) whereas the off-diagonal contain the count of incorrect predictions (FP, FN). The rows are the predicted and the columns are the actual.
This looks something like this.
&lt;Insert a picture&gt;
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Why we need TPR, FPR if we already have mis-classification error?&lt;/h3&gt;
&lt;p&gt;The data I describe above is a typical case of imbalanced data wherein one of the class is having majority of observations (90% non-defaulters (negatives) in our data) and the remaining class is a minority (only 10% defaulters (positives)). In such cases, the predictions on new dataset will be skewed towards negatives i.e the model will classify a lot of defaulters (positives) into negatives. The bank can't afford to have such predictions. The bank wants to know for sure the defaulters (positives). Imagine, the loss to bank if the model classify a probable defaulter (positive) to non-defaulter.
&lt;br&gt;
&lt;br&gt;
In such cases, accuracy corresponding to mis-classification alone is not acceptable. The bank would be more interested in correctly classifying the positives into positive i.e the bank wants to classify the defaulters into defaulters without fail.
&lt;br&gt;
&lt;br&gt;
Comes into picture &lt;em&gt;TPR&lt;/em&gt;, &lt;em&gt;TNR&lt;/em&gt;, &lt;em&gt;FPR&lt;/em&gt;, &lt;em&gt;FNR&lt;/em&gt;. These 3 lettered acronyms are nothing but the rates of TP, TN, FP and FN respectively.
Below is the formula. To digest the formula, let's move to our data having 1000 rows - 100 defaulters (positives) and 900 non-defaulters(negatives). Suppose we employed a logistic regression that classified 80 defaulters correctly and incorrectly classified 90 non-defaulters as defaulters.
TPR = 80 / 100 = 0.8
FPR = 90/ 900 = 0.1
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How does the TPR and FPR gets calculated?&lt;/h3&gt;
&lt;p&gt;Whenever you do any classification, the model always gives you probabilities of each observation getting classified in each of the classes.
Based on what cut-off you choose, you will get different predictions for the data and hence different TPR and FPR overall. You can choose whatever probability cut-off between [0, 1] and you will get different tuples of TPR and FPR.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TPR and FPR are be generated for each of the probability value one chooses. And these values are then plotted on an ROC curve.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
One can plot these tuples (probability, FPR, TPR) on a graph. You know what this graph is called? ROC-Receiver Operating Characteristics.
There is a trade-off between TPR and FPR. Depending on the requirement one can choose the probability cut-off that best fulfills their purpose. For instance, in the bank's case, the bank wants to not miss a single defaulter(positives) i.e the bank wants a higher TPR. The ROC curve looks something like this.&lt;/p&gt;
&lt;p&gt;Alright, all this is clear to me.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What about AUC?&lt;/h3&gt;
&lt;p&gt;AUC is nothing but the area under curve of ROC curve. Let's say we built a logistic regression model that gave us the probabilities for each row. Now we try with probability cut-offs from 0.1 to 1.0 with step size of 0.1 i.e we would have 10 probabilites to try with and corresponding to each of the 10 values we would have corresponding (FPR, TPR). If we plot these values on a graph we would get a graph having 10 points. This 10 point graph is what we call an ROC curve and the area under this graph is called AUC.
&lt;br&gt;
&lt;br&gt;
The AUC is a common evaluation metric for binary classification problems. Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1: if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5.
&lt;br&gt;
&lt;br&gt;
One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets.
&lt;br&gt;
&lt;br&gt;
The larger the area the better. If we have to choose between two classifiers having different AUCs, we choose the one having larger AUC.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;How do you decide what probability cut-off should you choose to classify them into either of the classes?&lt;/h3&gt;
&lt;p&gt;Some say they take a cut-off of 0.5 i.e observation having probability greater than 0.5 will be classified as positive or else negative. Do you see the problem in here? Try thinking.
&lt;br&gt;
&lt;br&gt;
Come into picture ROC curve. Look at the ROC curve and depending on what value of TPR or FPR you want from your model, you take probability corresponding to that point.&lt;/p&gt;
&lt;p&gt;ROC, AUC can easily be plotted and calculated using modern analytical tools like R or Python. But I would suggest for better understanding of this topic, try writing your own code.
&lt;br&gt;
&lt;br&gt;
I will make an attempt of the same soon and will share it on this space. Keep learning and sharing.&lt;/p&gt;
&lt;p&gt;Did you find the article useful? If you did, share your thoughts on the topic in the comments.&lt;/p&gt;</summary><category term="roc"></category><category term="auc"></category></entry></feed>