<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Manish Barnwal</title><link href="http://manishbarnwal.github.io/" rel="alternate"></link><link href="http://manishbarnwal.github.io/feeds/big-data.atom.xml" rel="self"></link><id>http://manishbarnwal.github.io/</id><updated>2020-08-31T08:00:00-03:00</updated><entry><title>Brief introduction to SparkUI</title><link href="http://manishbarnwal.github.io/blog/2020/08/31/brief_introduction_to_spark_ui/" rel="alternate"></link><updated>2020-08-31T08:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2020-08-31:blog/2020/08/31/brief_introduction_to_spark_ui/</id><summary type="html">&lt;p&gt;Recently, I've been working a lot with PySpark in AWS EMR. I have a huge data dump (~300 million users) that I needed to process and transform it into the right format for further processing. The data is in a S3 bucket in AWS and I use AWS EMR to launch a cluster of r5.2xlarge instances to run the spark application. We use a Hadoop YARN resource manager. 
&lt;br&gt;
&lt;br&gt;
Once you launch a spark application, you get access to a corresponding Spark UI interface that gives you an option to look at the overall status of the jobs, tasks, executors and other details. In today's post, I will be talking about what to look for in the Spark UI when you launch a spark application. I've been using SparkUI a lot over the last few months and thought writing a post on this would be useful for others as well. Let us begin.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Spark overview&lt;/h3&gt;
&lt;p&gt;Before we proceed with the introduction to SparkUI, a quick refreshed on Spark vocabulary will be useful
&lt;img alt="photo of spark cluster overview" src="http://manishbarnwal.com/images/brief_introduction_to_sparkui/spark_cluster_overview.png" /&gt;
&lt;br&gt;
&lt;br&gt;
There are three main components of spark:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Driver program (sparkContext)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;this is the heart of spark in cluster - keeps live status of the application&lt;/li&gt;
&lt;li&gt;if the driver process dies due to some reason say OutOfMemory error, your spark application will be killed as well&lt;/li&gt;
&lt;li&gt;contains the code that needs to be run&lt;/li&gt;
&lt;li&gt;it there is only machine/process you could monitor, it has to be the driver process&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cluster manager&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;acts as the bridge between the driver program and the worker nodes&lt;/li&gt;
&lt;li&gt;also allocates resources to different spark applications&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Worker instances&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the instances/machines that form the cluster&lt;/li&gt;
&lt;li&gt;they consists of executors and these executors are the actual horses that do the actual code execution
&lt;br&gt;
&lt;br&gt;
Let us talk a little about the cluster managers. As I've just mentioned above, a cluster manager is the bridge between the sparkContext and the worker nodes and allocates resources to different spark applications. There are four most commonly available cluster managers:&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;In-built spark Standalone cluster manager&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Good for starting but eventually you will have to graduate to other resource managers - Hadoop YARN or Apache Mesos&lt;/li&gt;
&lt;li&gt;Not used by a lot of people because there are better alternatives available&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hadoop YARN (Yet Another Resource Negotiator)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hadoop YARN is just a framework for job scheduling and resource management&lt;/li&gt;
&lt;li&gt;Spark has nothing to do with Hadoop in Hadoop YARN&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apache Mesos&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Founded by the founders of Spark itself&lt;/li&gt;
&lt;li&gt;Apache Mesos is considered very heavy-weight and should be used only if you already have Mesos configured in your large scale production system&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes cluster manager&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This is open-source and is used a lot by people
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;SparkUI under the hood&lt;/h3&gt;
&lt;p&gt;When a spark application is launched within a sparkContext, there are few processes that are run in background that are used by the sparkUI.
Some of the processes are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LiveListenerBus&lt;/strong&gt;: processes events as the application is live and kicking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;JobProgressListener&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;responsible for collecting all data that sparkUI uses to show statistics&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the information that we see under Jobs tab in SparkUI is because of this listener bus&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Creates optional sparkUI if &lt;code&gt;spark.ui.enabled&lt;/code&gt; is set True&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;EnvironmentListener&lt;/strong&gt;, &lt;strong&gt;StorageStatusListener&lt;/strong&gt;, &lt;strong&gt;RDDOperationsGraphListener&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Likewise each of the above buses are responsible for other tabs in the SparkUI
&lt;br&gt;
&lt;br&gt;
I won't go deep into each of these listeners (because I do not truly understand their internals) but if you are interested you should read on them. It is because of these SparkListeners that we are able to see the information on the jobs, executors, tasks in the Spark UI. So in essence, under the hood, Spark UI is a bunch of SparkListeners running and collecting all information for the jobs/executors.
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;SparkContext and spark applications&lt;/h3&gt;
&lt;p&gt;A sparkContext can have multiple Spark applications running and each application will have its own corresponding Spark UI. Spark UI for an application can be accessed in port 4040 and successive applications will be given the next port numbers - 4041, 4042 and likewise. If you are running Spark in local, you can access the Spark UI at http://localhost:4040. You can change the port number using spark.ui.port in conf/spark-env.sh. To persist the Spark UI so that you can take a look at the Spark UI after the application is run, you can do so by accessing the spark history server.&lt;/p&gt;
&lt;p&gt;To enable persisting of Spark UI, the property is &lt;code&gt;spark.eventLog.enabled&lt;/code&gt; which defaults to True. To enable the Spark UI, the property is &lt;code&gt;spark.ui.enabled&lt;/code&gt; which defaults to True.
&lt;br&gt;
&lt;br&gt;
Few examples of Spark applications:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Spark application responsible  for loading data from S3, process and transforming it and then saving it back to S3 for next job to consume it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running two jupyter notebooks in your local Spark are examples of two running applications. Spark UI of each of the applications will be available at http://localhost:4040 and http://localhost:4041
&lt;br&gt;
&lt;br&gt;
Each of the applications in sparkContext are run independent of each other. It is the responsibility of the cluster manager to allocate executors for each of these applications. A worker node has many executors and and the same worker node can be assigned to multiple applications but each executor will allocated to only one application. The cluster manager assures that a single executor is not allocated to more than one application.
&lt;br&gt;
&lt;br&gt;
Now before we start with the Spark UI, let us cover a quick refresher of the Spark vocabulary that will help use better understand what to look for in Spark UI
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Spark vocabulary&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Job&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Whenever an action is called a job is triggered&lt;/li&gt;
&lt;li&gt;Examples of action in Spark are collect, aggregation functions like count, sum, or writing data to s3&lt;/li&gt;
&lt;li&gt;A job is broken into stages, the number of which depends on how many shuffle operations are there in the job&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stages&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a job is broken into DAG (directed acyclic graph) of stages&lt;/li&gt;
&lt;li&gt;a stage is a collection of similar tasks that can be performed together&lt;/li&gt;
&lt;li&gt;stages can either depend on each other or be run independently. Stages that are not interdependent on each other are run in parallel&lt;/li&gt;
&lt;li&gt;the driver keeps track of how the stages should interact with each other to finally get the job done&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A task is a unit of computation that is applied to a block of data (partition)&lt;/li&gt;
&lt;li&gt;So, it is a combination of the operation and the block of data on which the operation will be applied&lt;/li&gt;
&lt;li&gt;It is the only abstraction of job that interacts with the hardware - task is executed in an executor in a worker node&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Executors&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the processes in worker nodes that do the actual execution of the code&lt;/li&gt;
&lt;li&gt;each executor is assigned a task and keeps data in memory or disk&lt;/li&gt;
&lt;li&gt;think of the executors as the horses on the ground that do the actual code execution&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Worker instances&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;the machines that form the cluster
&lt;br&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Navigating the Spark UI tabs&lt;/h3&gt;
&lt;p&gt;Now that we understand the basic vocabulary of Spark, we will go through each of the tabs in Spark UI and what to look for in them.  The following are the six tabs available:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Jobs&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This will contain all the jobs you run in your application - it also tells you how many stages and tasks were there in total in the job&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stages&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This shows all the information around stages - both complete and running.&lt;/li&gt;
&lt;li&gt;Tells you how long each stage it taking and into how many tasks a stage is broken into&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This will contain information about the data that you may have persisted/cached. If there is not data cached, then this will be empty.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Environment&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Contains information about the spark configurations and other and some java configurations&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Executors&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;This is where you will see a detailed information on how your executors are doing with respect to task execution&lt;/li&gt;
&lt;li&gt;It also gives a summary of the time taken by executors to complete the tasks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SQL&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Here you will see the DAG of the query that you have executed (either via Dataframes API or SQL API) in the job.
&lt;br&gt;
&lt;br&gt;
Okay, I understand what Spark UI is but what do I do with it? Understanding Spark UI is one of the first steps toward spark optimisations of your spark job. The UI tells you if there is a particular stage which is taking a long time to run and whether you can do something about it. Having a good understanding of Spark UI is of profound importance if you want to monitor/debug your Spark jobs.
&lt;br&gt;
&lt;br&gt;
And with that I will end this post. Play around with the Spark UI, keep reading and this will become one of your favourite things about running spark jobs. I certainly keep an eye out for anything abnormal happening in the Spark UI when running spark jobs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="spark"></category><category term="big data"></category><category term="spark ui"></category><category term="tutorial"></category><category term="manish barnwal"></category></entry><entry><title>Hadoop Streaming</title><link href="http://manishbarnwal.github.io/blog/2016/08/29/hadoop_streaming/" rel="alternate"></link><updated>2016-08-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-08-29:blog/2016/08/29/hadoop_streaming/</id><summary type="html">&lt;p&gt;A few days ago, I had written a post on &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2016/06/29/big_data_problem/" target="_blank"&gt;The Big Data Problem&lt;/a&gt;&lt;/strong&gt; which attempted to understand why we need big data and what the fuss is all about. You may want to read it &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2016/06/29/big_data_problem/" target="_blank"&gt;here&lt;/a&gt;&lt;/strong&gt;.
&lt;br&gt;
&lt;br&gt;
Having understood why we need big data, let’s understand how we can go about analyzing the same. What is the way out to do analysis on big data? The solution is Streaming…Hadoop Streaming. &lt;em&gt;James Bond style&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
To recount a personal experience, I was faced with the following Data Analysis task — Scaling a particular analytics technique on retail data from one store to all stores across the US. Sounds interesting so far, doesn’t it? The catch however, was that the program took 2 days just to compute the results for 1 department.
&lt;br&gt;
&lt;br&gt;
Imagine, the time it would take if we were restricted to a single machine/Rstudio.
&lt;br&gt;
&lt;br&gt;
Let’s understand the gravity of the problem and get an idea of a rough time estimate. 
So for one department the time taken was two days. Let us assume, each store has close to 100 departments and we have a total of 5000 stores. So the total time taken for all of the stores would be roughly 2 X 100 X 5000 = 1 million days = 2740 years. This is just a rough estimate assuming a linear relationship between the number of departments and the time taken.
&lt;br&gt;
&lt;br&gt;
Obviously, using RStudio stand alone was not a feasible solution. We had at our disposal Hadoop cluster. We wanted to see if we could exploit the number of machines in our cluster to solve this Herculean challenge of the Big Data world.
&lt;br&gt;
&lt;br&gt;
A fresh college graduate, I was a complete stranger to the world of Hadoop and out of anxiety, I purchased a copy of &lt;em&gt;Hadoop-The definitive guide&lt;/em&gt;. I started going through the chapters. It was in no time that I realized this book was meant for someone with an understanding of Java. Unfortunately, I was not one of them. Saddened and almost defeated. What other choice do I have? I came across this wonderful concept — Hadoop streaming.
&lt;br&gt;
&lt;br&gt;
Wow! This looks like it could solve the problem. I started reading more about it on the internet. There were not many resources on this technique. Fortunately, a colleague of mine had worked on Hadoop Streaming earlier and with his help we were able to accomplish the task successfully.
&lt;br&gt;
&lt;br&gt;
Now that you have the context, let me try to answer some basic technical questions.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;So what exactly is Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;Hadoop streaming — ‘hadoop’ and ‘streaming’. We use the availability of machines in the cluster to process data in parallel. Let me elaborate. When we do our analysis on Rstudio, we just have one machine (our laptop). Now imagine we have 100s of such machines in our cluster. We can take advantage of this and distribute our input data in such a way that each machine gets some portion of your input data and they are processed in parallel.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Now, you may ask what is streaming here.&lt;/h3&gt;
&lt;p&gt;The input data is sent to each of the machines in the cluster via stdin() (standard input) and the analyzed output is thrown at stdout() (standard output). Your input data is streamed via stdin and the output gets flushed out to stdout.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What languages can I use for Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;You can use any scripting language — R, Python, Ruby. Care should be taken to choose your scripting language. The analysis you are planning to do on your data should decide your choice of language. Say, we want to do a forecasting of sales at item level for a retail store. We know that R is the suitable choice to use when it comes to do forecasting as we have well-developed packages in R which is not the case with Ruby. So do put some thought into choosing the language.
&lt;br&gt;
&lt;br&gt;
I hope we are now clear on the basics of Hadoop Streaming and its benefits.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hadoop Streaming is not the answer to all your Big Data analysis problems. It can be used only for cases where each machine can independently perform its analysis with a small portion of input data that is fed to it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming can be used&lt;/h3&gt;
&lt;p&gt;Sales forecasting at item level. Say, we have weekly data for 2 years at item level. And say there are 10,000 items for which we want to do the forecasting. Each item has close to 104 rows (2 years weekly data). So our input data has close to 104 X 10,000 = 1,040,000 ~ 1 million rows. 
&lt;br&gt;
&lt;br&gt;
Assume we have 100 machines in our cluster. What we do next is pretty intuitive. We distribute our data such that each machine receives 1 item at a time, and once it is done processing that item, we send the next item to it. So, in a single go, we will have 100 items processed across the cluster. In, the next go, again more 100 items will be processed and this goes on until all the 10,000 items are processed.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming won’t work&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank"&gt;Clustering&lt;/a&gt;&lt;/em&gt;. In clustering we need to find the hidden patterns that are there in the complete dataset. A smaller portion of the dataset can’t be sent to a machine to do the analysis because the purpose will not be served.
&lt;br&gt;
&lt;br&gt;
I think this is worth mentioning.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In Hadoop Streaming, you are not writing an &lt;strong&gt;org.apache.hadoop.mapred.Mapper class&lt;/strong&gt;! This is just a simple script that reads rows from stdin (columns separated by ‘\t’ or any delimiter) and should write rows to stdout (again, columns separated by ‘\t’ or other delimiter). It’s probably worth mentioning this again but you shouldn’t be thinking in traditional map-reduce &lt;strong&gt;Key Value&lt;/strong&gt; terms, you need to think about columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
You can write your script in any language you want, but it needs to be available on all machines in the cluster. Any easy way to do this is to take advantage of the Hadoop &lt;strong&gt;distributed cache support&lt;/strong&gt;, and just use add file /path/to/script within hive. The script will then be distributed and can be run as just ./script (assuming it is executable)
Enough theoretical stuff. The interesting part is the code framework of mapper and reducer that I will explain in the next post. 
&lt;br&gt;
&lt;br&gt;
I hope you feel more educated on big data after reading this. Please leave a comment if you have questions/insights. I will reply as soon as possible.&lt;/p&gt;</summary><category term="big data"></category><category term="hive"></category><category term="tutorial"></category><category term="hadoop"></category><category term="manish barnwal"></category></entry><entry><title>The Big Data Problem</title><link href="http://manishbarnwal.github.io/blog/2016/06/29/big_data_problem/" rel="alternate"></link><updated>2016-06-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-29:blog/2016/06/29/big_data_problem/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Big data&lt;/strong&gt; has become a sensation these days. Anyone and everyone wants to use this in their discussions. When I was still in my college and preparing for campus placements, I had attended almost all the pre-placement talks that companies gave to its prospective candidates.
&lt;br&gt;
&lt;br&gt;
American Express was one such company that had talked extensively about big data and hadoop in their presentation. I remember clearly, the blank faces that all of us had. We were not able to follow a single term. &lt;strong&gt;Hadoop&lt;/strong&gt;, &lt;strong&gt;clusters&lt;/strong&gt;, &lt;strong&gt;big data&lt;/strong&gt;, &lt;strong&gt;distributed environment&lt;/strong&gt; and many other terms were just bouncers for me. I did try to google them up later but these terms were still alien to me.
&lt;br&gt;
&lt;br&gt;
I have worked at WalmartLabs for close to 2 years now and the work has mostly been on the big data side. Walmart is one of the largest companies when it comes to capturing the transactions data that we have on daily basis both at the stores and at the e-commerce. Having worked on the big data technologies a little, I felt I would give it a try to make big data and related questions a little easy to understand.
&lt;br&gt;
&lt;br&gt;
Following are few of the questions that I will try to explain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data? And why should I care about it?&lt;/li&gt;
&lt;li&gt;What is the problem with a single laptop?&lt;/li&gt;
&lt;li&gt;What is a cluster?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The data is distributed across the cluster&lt;/em&gt;. What is meant by this?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is Hadoop and Spark?
&lt;br&gt;
&lt;br&gt;
Everyone is generating data. Data is the side-product of any work we do. &lt;em&gt;Just like smoke is emitted when things burn, data is created when machines work or interact with each other&lt;/em&gt;. These machines used to emit data in the past as well, but we were not advanced enough to collect the data. The technologies weren’t capable of capturing these.
&lt;br&gt;
&lt;br&gt;
Today, every other thing is emitting data. Be it the signals from your refrigerator, the air-conditioners in your room, the cars on the street, the videos that you like or dislike on YouTube, your transactions at the nearby Super-market, the posts you make on Facebook. These are all emissions of your day-to-day work.
&lt;br&gt;
&lt;br&gt;
Companies and analysts want to understand you and your behavior using the data you generate. To give you an idea of how much data is generated, let’s look at the ridiculous amount of data some of the big giants produce on daily basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Facebook’s daily logs are more than &lt;strong&gt;60 terabytes&lt;/strong&gt; every day.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Google’s web index is more than &lt;strong&gt;10 petabytes&lt;/strong&gt; of information.&lt;/li&gt;
&lt;li&gt;Millions of customers visit Walmart stores on daily basis. Imagine the &lt;strong&gt;size of these transactions data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do we need this data? Of course!&lt;/p&gt;
&lt;p&gt;I strongly believe this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every data has a story. With proper analysis and techniques, we can get a lot from data. Now, there will be times when you won’t get anything insightful from it. I consider this also good. Maybe, there is something wrong with the way we are collecting the data and we need to have a look at it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
So far we know that every other thing is puking data. And they are puking in bulk. To get something good out of these pukes, we need to store them somewhere, do our analysis on them and get insights. How do we go about it?&lt;/p&gt;
&lt;p&gt;Storage of this data has never been a problem. The disks are relatively cheap. A terabyte of disk costs ~ $35. So storing them on physical disks is not an issue. What is the problem then?&lt;/p&gt;
&lt;p&gt;Do you have an idea how fast the data can be read from a disk? It is still in Mbs/sec. Let us assume a speed of 100 Mbs/sec. With this speed, the time taken to read 1 terabyte of data would be ~ 3 hours! Below is the calculation for 3 hours.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1 terabytes = 1000,000 Mb. So the total time ~ 1000,000/100 ~ 10000 seconds ~ 10000/3600 ~ 3 hours&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3 hours is too huge a time to read 1 terabyte of data given the amount of data giants like Facebook generates on daily basis. Also, a single machine won’t be able to handle this much data. So what is the solution?
&lt;br&gt;
&lt;br&gt;
What if we could distribute the data across multiple machines? Yes, that sounds promising. The data that is big enough to handle by one machine can be distributed across multiple machines such that each machine has some portion of the actual data. Now these machines are interconnected with each other and so there is no issue of interactions between these machines. And that is exactly what a cluster is.
&lt;br&gt;
&lt;br&gt;
A &lt;em&gt;cluster&lt;/em&gt; is a combination of many machines connected to each other. Think of it like this. There is a process by which, my computer and your computer can be connected to each other and we can have a cluster with two nodes or machines.&lt;/p&gt;
&lt;p&gt;Now, that you have your machines connected with each other in the cluster, you can take advantage of the computation power of each of the machines. And thus, the task which one couldn’t even think of accomplishing by a single machine can now be very easily be done by a cluster of machines.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is hadoop and Spark? And how are they different?&lt;/h3&gt;
&lt;p&gt;Hadoop is a framework that supports the storage and processing of large data sets in a distributed computing environment.It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs.&lt;/p&gt;
&lt;p&gt;Spark is a powerful analytics engine. It is a fast and general engine for large-scale data processing.&lt;/p&gt;
&lt;p&gt;Hadoop provides both the data storage and processing power whereas Spark is meant for doing analysis and processing of big data.&lt;/p&gt;
&lt;p&gt;One thing to notice is, Spark is about 10 to 100 times faster than the Hadoop MapReduce framework by making use of in-memory processing compared to persistence storage used by Hadoop.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Spark is a Swiss army knife of analytics world&lt;/em&gt;. There are various APIs- Python, R, Scala thorough which one can interact with the Spark framework. For machine learning algorithms, there is MlLib which can be used to perform some common analysis like regression, K-means clustering, and classification.
&lt;br&gt;
&lt;br&gt;
We will talk more about Spark in future posts. I am exploring it rigorously and plan to write out my understanding of it soon.&lt;/p&gt;
&lt;p&gt;I hope you enjoyed reading this post and feel a little more familiar with big data now. Hit the share button if you would like your friends to read this.&lt;/p&gt;</summary><category term="big data"></category><category term="hadoop"></category><category term="machine-learning"></category><category term="manish barnwal"></category></entry><entry><title>Understanding HIVE for data science people</title><link href="http://manishbarnwal.github.io/blog/2016/06/22/understanding_hive_data_science/" rel="alternate"></link><updated>2016-06-22T04:43:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-22:blog/2016/06/22/understanding_hive_data_science/</id><summary type="html">&lt;p&gt;I have been working as Statistical analyst for the last 1.5 years and fortunately I got to work on Hadoop on one of my initial projects. Hadoop sounds scary to a lot of people and I am no exception. In this post, I would make an attempt to explain HIVE-the data warehouse for Hadoop ecosystem. &lt;/p&gt;
&lt;p&gt;What is HIVE? And why as an analyst I should care about it?&lt;/p&gt;
&lt;p&gt;HIVE is the part of the Hadoop infrastructure where data gets stored and you can write your query to fetch data from HIVE (tables). Now when I had started working on Hadoop, I knew a little SQL and a tad little of data science. I had take a few courses on Statistics during my college but then who really studies in college! Having allocated to a project which required an understanding of Hadoop, I planned how I need to understand this system so that I can contribute my best to the project.&lt;/p&gt;
&lt;p&gt;The first thing you need to build any model is data. Comes to picture-HIVE. As I said, I knew SQL and as I started working on HIVE I got to know that if one knows SQL then one knows HIVE, it’s just that he is not aware of it yet. &lt;/p&gt;
&lt;p&gt;Why do I say so? 
Because, HIVE was built with the sole purpose that database people who are comfortable working on SQL need not require to learn a completely new language to fetch and interact with data in the Hadoop ecosystem. And I believe the founders of Hadoop have achieved this successfully. &lt;/p&gt;
&lt;p&gt;HIVE hides the map-reduce processes from the user and all you need to worry about is writing your SQL query. You don’t need to worry at all about mappers and reducers. All these information is hidden from the user and if one wants to know more on these, there are ways to know that as well (advanced users of HIVE).&lt;/p&gt;
&lt;p&gt;I will cover most common queries that we deal with when working as an analyst on Hadoop- creating a table (external/internal), getting information about table, setting common HIVE settings and a few others.&lt;/p&gt;
&lt;p&gt;Creating a table
Table in HIVE can be of 2 types: Internal or External
Stay with me for next few lines and I will explain the differences. 
By default, the table that we create in most of the cases is internal table. However, external tables come into picture when you want to build a table over some file.&lt;/p&gt;
&lt;p&gt;The difference between external and internal table is in terms of what happens when we drop a table. 
External table:
 —  If you drop the table, the table and the metadata of the data is dropped but not the data 
 The data is located in hdfs (and not in local file system) and since this data is accessed by many tables we don’t want the data to get dropped.&lt;/p&gt;
&lt;p&gt;Just add the keyword external to specify that we want to build an external table.
 —  CREATE EXTERNAL TABLE external_table_name&lt;/p&gt;
&lt;p&gt;Internal table:
 —  If you drop the table, the table, metadata and even the data is dropped
 —  If the data for the table resides in the local file system, you should go for creating internal table&lt;/p&gt;
&lt;p&gt;—  CREATE INTERNAL TABLE internal_table or just CREATE TABLE internal_table
Once the table gets created, we want to get the data from the table for our analysis. Before we dive into getting the data from the table, it is a good idea to get an idea about the overall structure of the table — column names, column types, whether external or internal, owner of the table.&lt;/p&gt;
&lt;p&gt;All of these information can be retrieved by using this: describe formatted table_name; This gives you a lot of information in a formatted manner. If you are in a hurry and just want to see the column names and no other details, use desc table_name;&lt;/p&gt;
&lt;p&gt;External tables are created over some file. This file should be located in hdfs-file system for hadoop. A few command lines would easily move the file to hdfs.&lt;/p&gt;
&lt;h3&gt;How to move a file to hdfs?&lt;/h3&gt;
&lt;p&gt;Below are the steps:
- Create a directory where you want to move the file
  hadoop fs -mkdir directory_name
- Check if the directory got created
  hadoop fs -ls ; you should see your directory name here
  This directory is empty right now. You can check this using
  hadoop fs -ls directory_name; It's empty
- Move the file from local to hdfs
  hadoop fs -copyFromLocal 'path where file is stored' directory_name
- Check if the file has been copied to directory
  hadoop fs -ls directory_name; You should see the file now.&lt;/p&gt;
&lt;p&gt;Open vim editor and write the below codes in any file. I will name my file create_table.hql
vim create_table.hql
Creating table. Code below&lt;/p&gt;
&lt;p&gt;drop external table if exists db_name.table_name;
create external table if not exists db_name.table_name (
ID    string,
WorkStatus     string,
Score    int,
Residence_Region    string,
income    string,
Gender    smallint,
Alcohol_Consumption    string,
Happy    string
)
row format delimited
fields terminated by ‘\t’
stored as textfile
location ‘/user/mbarnwa/data1’
;&lt;/p&gt;
&lt;p&gt;Dropping a database
Many a times, you may want to drop a database that you don’t need anymore. Say, the database to be dropped is ‘userdb’, then if you do this: drop database userdb; 
Now the above command will work if your database is empty i.e. the database, ‘userdb’ has no tables in it. 
But that is rarely a case, so either you can go ahead on deleting each table in the database by using this command drop table ‘tableName’; or addition of a simple keyword — CASCADE will solve your purpose. So the final command to drop a database (even if it has tables) is 
drop database ‘userdb’ cascade;&lt;/p&gt;</summary><category term="hive"></category><category term="hadoop"></category><category term="big data"></category><category term="manish barnwal"></category></entry></feed>