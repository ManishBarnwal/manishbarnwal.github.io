<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Manish Barnwal</title><link href="http://manishbarnwal.github.io/" rel="alternate"></link><link href="http://manishbarnwal.github.io/feeds/big-data.atom.xml" rel="self"></link><id>http://manishbarnwal.github.io/</id><updated>2016-08-29T07:00:00-03:00</updated><entry><title>Hadoop Streaming</title><link href="http://manishbarnwal.github.io/blog/2016/08/29/hadoop_streaming/" rel="alternate"></link><updated>2016-08-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-08-29:blog/2016/08/29/hadoop_streaming/</id><summary type="html">&lt;p&gt;A few days ago, I had written a post on &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2016/06/29/big_data_problem/" target="_blank"&gt;The Big Data Problem&lt;/a&gt;&lt;/strong&gt; which attempted to understand why we need big data and what the fuss is all about. You may want to read it &lt;strong&gt;&lt;a href="http://manishbarnwal.com/blog/2016/06/29/big_data_problem/" target="_blank"&gt;here&lt;/a&gt;&lt;/strong&gt;.
&lt;br&gt;
&lt;br&gt;
Having understood why we need big data, let’s understand how we can go about analyzing the same. What is the way out to do analysis on big data? The solution is Streaming…Hadoop Streaming. &lt;em&gt;James Bond style&lt;/em&gt;
&lt;br&gt;
&lt;br&gt;
To recount a personal experience, I was faced with the following Data Analysis task — Scaling a particular analytics technique on retail data from one store to all stores across the US. Sounds interesting so far, doesn’t it? The catch however, was that the program took 2 days just to compute the results for 1 department.
&lt;br&gt;
&lt;br&gt;
Imagine, the time it would take if we were restricted to a single machine/Rstudio.
&lt;br&gt;
&lt;br&gt;
Let’s understand the gravity of the problem and get an idea of a rough time estimate. 
So for one department the time taken was two days. Let us assume, each store has close to 100 departments and we have a total of 5000 stores. So the total time taken for all of the stores would be roughly 2 X 100 X 5000 = 1 million days = 2740 years. This is just a rough estimate assuming a linear relationship between the number of departments and the time taken.
&lt;br&gt;
&lt;br&gt;
Obviously, using RStudio stand alone was not a feasible solution. We had at our disposal Hadoop cluster. We wanted to see if we could exploit the number of machines in our cluster to solve this Herculean challenge of the Big Data world.
&lt;br&gt;
&lt;br&gt;
A fresh college graduate, I was a complete stranger to the world of Hadoop and out of anxiety, I purchased a copy of &lt;em&gt;Hadoop-The definitive guide&lt;/em&gt;. I started going through the chapters. It was in no time that I realized this book was meant for someone with an understanding of Java. Unfortunately, I was not one of them. Saddened and almost defeated. What other choice do I have? I came across this wonderful concept — Hadoop streaming.
&lt;br&gt;
&lt;br&gt;
Wow! This looks like it could solve the problem. I started reading more about it on the internet. There were not many resources on this technique. Fortunately, a colleague of mine had worked on Hadoop Streaming earlier and with his help we were able to accomplish the task successfully.
&lt;br&gt;
&lt;br&gt;
Now that you have the context, let me try to answer some basic technical questions.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;So what exactly is Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;Hadoop streaming — ‘hadoop’ and ‘streaming’. We use the availability of machines in the cluster to process data in parallel. Let me elaborate. When we do our analysis on Rstudio, we just have one machine (our laptop). Now imagine we have 100s of such machines in our cluster. We can take advantage of this and distribute our input data in such a way that each machine gets some portion of your input data and they are processed in parallel.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Now, you may ask what is streaming here.&lt;/h3&gt;
&lt;p&gt;The input data is sent to each of the machines in the cluster via stdin() (standard input) and the analyzed output is thrown at stdout() (standard output). Your input data is streamed via stdin and the output gets flushed out to stdout.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What languages can I use for Hadoop streaming?&lt;/h3&gt;
&lt;p&gt;You can use any scripting language — R, Python, Ruby. Care should be taken to choose your scripting language. The analysis you are planning to do on your data should decide your choice of language. Say, we want to do a forecasting of sales at item level for a retail store. We know that R is the suitable choice to use when it comes to do forecasting as we have well-developed packages in R which is not the case with Ruby. So do put some thought into choosing the language.
&lt;br&gt;
&lt;br&gt;
I hope we are now clear on the basics of Hadoop Streaming and its benefits.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hadoop Streaming is not the answer to all your Big Data analysis problems. It can be used only for cases where each machine can independently perform its analysis with a small portion of input data that is fed to it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming can be used&lt;/h3&gt;
&lt;p&gt;Sales forecasting at item level. Say, we have weekly data for 2 years at item level. And say there are 10,000 items for which we want to do the forecasting. Each item has close to 104 rows (2 years weekly data). So our input data has close to 104 X 10,000 = 1,040,000 ~ 1 million rows. 
&lt;br&gt;
&lt;br&gt;
Assume we have 100 machines in our cluster. What we do next is pretty intuitive. We distribute our data such that each machine receives 1 item at a time, and once it is done processing that item, we send the next item to it. So, in a single go, we will have 100 items processed across the cluster. In, the next go, again more 100 items will be processed and this goes on until all the 10,000 items are processed.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Example where Hadoop Streaming won’t work&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank"&gt;Clustering&lt;/a&gt;&lt;/em&gt;. In clustering we need to find the hidden patterns that are there in the complete dataset. A smaller portion of the dataset can’t be sent to a machine to do the analysis because the purpose will not be served.
&lt;br&gt;
&lt;br&gt;
I think this is worth mentioning.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In Hadoop Streaming, you are not writing an &lt;strong&gt;org.apache.hadoop.mapred.Mapper class&lt;/strong&gt;! This is just a simple script that reads rows from stdin (columns separated by ‘\t’ or any delimiter) and should write rows to stdout (again, columns separated by ‘\t’ or other delimiter). It’s probably worth mentioning this again but you shouldn’t be thinking in traditional map-reduce &lt;strong&gt;Key Value&lt;/strong&gt; terms, you need to think about columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
You can write your script in any language you want, but it needs to be available on all machines in the cluster. Any easy way to do this is to take advantage of the Hadoop &lt;strong&gt;distributed cache support&lt;/strong&gt;, and just use add file /path/to/script within hive. The script will then be distributed and can be run as just ./script (assuming it is executable)
Enough theoretical stuff. The interesting part is the code framework of mapper and reducer that I will explain in the next post. 
&lt;br&gt;
&lt;br&gt;
I hope you feel more educated on big data after reading this. Please leave a comment if you have questions/insights. I will reply as soon as possible.&lt;/p&gt;</summary><category term="big data"></category><category term="analytics"></category><category term="hadoop"></category><category term="manish barnwal"></category></entry><entry><title>The Big Data Problem</title><link href="http://manishbarnwal.github.io/blog/2016/06/29/big_data_problem/" rel="alternate"></link><updated>2016-06-29T07:00:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-29:blog/2016/06/29/big_data_problem/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Big data&lt;/strong&gt; has become a sensation these days. Anyone and everyone wants to use this in their discussions. When I was still in my college and preparing for campus placements, I had attended almost all the pre-placement talks that companies gave to its prospective candidates.
&lt;br&gt;
&lt;br&gt;
American Express was one such company that had talked extensively about big data and hadoop in their presentation. I remember clearly, the blank faces that all of us had. We were not able to follow a single term. &lt;strong&gt;Hadoop&lt;/strong&gt;, &lt;strong&gt;clusters&lt;/strong&gt;, &lt;strong&gt;big data&lt;/strong&gt;, &lt;strong&gt;distributed environment&lt;/strong&gt; and many other terms were just bouncers for me. I did try to google them up later but these terms were still alien to me.
&lt;br&gt;
&lt;br&gt;
I have worked at WalmartLabs for close to 2 years now and the work has mostly been on the big data side. Walmart is one of the largest companies when it comes to capturing the transactions data that we have on daily basis both at the stores and at the e-commerce. Having worked on the big data technologies a little, I felt I would give it a try to make big data and related questions a little easy to understand.
&lt;br&gt;
&lt;br&gt;
Following are few of the questions that I will try to explain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data? And why should I care about it?&lt;/li&gt;
&lt;li&gt;What is the problem with a single laptop?&lt;/li&gt;
&lt;li&gt;What is a cluster?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;The data is distributed across the cluster&lt;/em&gt;. What is meant by this?&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is Hadoop and Spark?
&lt;br&gt;
&lt;br&gt;
Everyone is generating data. Data is the side-product of any work we do. &lt;em&gt;Just like smoke is emitted when things burn, data is created when machines work or interact with each other&lt;/em&gt;. These machines used to emit data in the past as well, but we were not advanced enough to collect the data. The technologies weren’t capable of capturing these.
&lt;br&gt;
&lt;br&gt;
Today, every other thing is emitting data. Be it the signals from your refrigerator, the air-conditioners in your room, the cars on the street, the videos that you like or dislike on YouTube, your transactions at the nearby Super-market, the posts you make on Facebook. These are all emissions of your day-to-day work.
&lt;br&gt;
&lt;br&gt;
Companies and analysts want to understand you and your behavior using the data you generate. To give you an idea of how much data is generated, let’s look at the ridiculous amount of data some of the big giants produce on daily basis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Facebook’s daily logs are more than &lt;strong&gt;60 terabytes&lt;/strong&gt; every day.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Google’s web index is more than &lt;strong&gt;10 petabytes&lt;/strong&gt; of information.&lt;/li&gt;
&lt;li&gt;Millions of customers visit Walmart stores on daily basis. Imagine the &lt;strong&gt;size of these transactions data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do we need this data? Of course!&lt;/p&gt;
&lt;p&gt;I strongly believe this.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every data has a story. With proper analysis and techniques, we can get a lot from data. Now, there will be times when you won’t get anything insightful from it. I consider this also good. Maybe, there is something wrong with the way we are collecting the data and we need to have a look at it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;
&lt;br&gt;
So far we know that every other thing is puking data. And they are puking in bulk. To get something good out of these pukes, we need to store them somewhere, do our analysis on them and get insights. How do we go about it?&lt;/p&gt;
&lt;p&gt;Storage of this data has never been a problem. The disks are relatively cheap. A terabyte of disk costs ~ $35. So storing them on physical disks is not an issue. What is the problem then?&lt;/p&gt;
&lt;p&gt;Do you have an idea how fast the data can be read from a disk? It is still in Mbs/sec. Let us assume a speed of 100 Mbs/sec. With this speed, the time taken to read 1 terabyte of data would be ~ 3 hours! Below is the calculation for 3 hours.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1 terabytes = 1000,000 Mb. So the total time ~ 1000,000/100 ~ 10000 seconds ~ 10000/3600 ~ 3 hours&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3 hours is too huge a time to read 1 terabyte of data given the amount of data giants like Facebook generates on daily basis. Also, a single machine won’t be able to handle this much data. So what is the solution?
&lt;br&gt;
&lt;br&gt;
What if we could distribute the data across multiple machines? Yes, that sounds promising. The data that is big enough to handle by one machine can be distributed across multiple machines such that each machine has some portion of the actual data. Now these machines are interconnected with each other and so there is no issue of interactions between these machines. And that is exactly what a cluster is.
&lt;br&gt;
&lt;br&gt;
A &lt;em&gt;cluster&lt;/em&gt; is a combination of many machines connected to each other. Think of it like this. There is a process by which, my computer and your computer can be connected to each other and we can have a cluster with two nodes or machines.&lt;/p&gt;
&lt;p&gt;Now, that you have your machines connected with each other in the cluster, you can take advantage of the computation power of each of the machines. And thus, the task which one couldn’t even think of accomplishing by a single machine can now be very easily be done by a cluster of machines.
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;What is hadoop and Spark? And how are they different?&lt;/h3&gt;
&lt;p&gt;Hadoop is a framework that supports the storage and processing of large data sets in a distributed computing environment.It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs.&lt;/p&gt;
&lt;p&gt;Spark is a powerful analytics engine. It is a fast and general engine for large-scale data processing.&lt;/p&gt;
&lt;p&gt;Hadoop provides both the data storage and processing power whereas Spark is meant for doing analysis and processing of big data.&lt;/p&gt;
&lt;p&gt;One thing to notice is, Spark is about 10 to 100 times faster than the Hadoop MapReduce framework by making use of in-memory processing compared to persistence storage used by Hadoop.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Spark is a Swiss army knife of analytics world&lt;/em&gt;. There are various APIs- Python, R, Scala thorough which one can interact with the Spark framework. For machine learning algorithms, there is MlLib which can be used to perform some common analysis like regression, K-means clustering, and classification.
&lt;br&gt;
&lt;br&gt;
We will talk more about Spark in future posts. I am exploring it rigorously and plan to write out my understanding of it soon.&lt;/p&gt;
&lt;p&gt;I hope you enjoyed reading this post and feel a little more familiar with big data now. Hit the share button if you would like your friends to read this.&lt;/p&gt;</summary><category term="big data"></category><category term="analytics"></category><category term="hadoop"></category><category term="spark"></category><category term="manish barnwal"></category></entry><entry><title>Understanding HIVE for data science people</title><link href="http://manishbarnwal.github.io/blog/2016/06/22/understanding_hive_data_science/" rel="alternate"></link><updated>2016-06-22T04:43:00-03:00</updated><author><name>Manish Barnwal</name></author><id>tag:manishbarnwal.github.io,2016-06-22:blog/2016/06/22/understanding_hive_data_science/</id><summary type="html">&lt;p&gt;I have been working as Statistical analyst for the last 1.5 years and fortunately I got to work on Hadoop on one of my initial projects. Hadoop sounds scary to a lot of people and I am no exception. In this post, I would make an attempt to explain HIVE-the data warehouse for Hadoop ecosystem. &lt;/p&gt;
&lt;p&gt;What is HIVE? And why as an analyst I should care about it?&lt;/p&gt;
&lt;p&gt;HIVE is the part of the Hadoop infrastructure where data gets stored and you can write your query to fetch data from HIVE (tables). Now when I had started working on Hadoop, I knew a little SQL and a tad little of data science. I had take a few courses on Statistics during my college but then who really studies in college! Having allocated to a project which required an understanding of Hadoop, I planned how I need to understand this system so that I can contribute my best to the project.&lt;/p&gt;
&lt;p&gt;The first thing you need to build any model is data. Comes to picture-HIVE. As I said, I knew SQL and as I started working on HIVE I got to know that if one knows SQL then one knows HIVE, it’s just that he is not aware of it yet. &lt;/p&gt;
&lt;p&gt;Why do I say so? 
Because, HIVE was built with the sole purpose that database people who are comfortable working on SQL need not require to learn a completely new language to fetch and interact with data in the Hadoop ecosystem. And I believe the founders of Hadoop have achieved this successfully. &lt;/p&gt;
&lt;p&gt;HIVE hides the map-reduce processes from the user and all you need to worry about is writing your SQL query. You don’t need to worry at all about mappers and reducers. All these information is hidden from the user and if one wants to know more on these, there are ways to know that as well (advanced users of HIVE).&lt;/p&gt;
&lt;p&gt;I will cover most common queries that we deal with when working as an analyst on Hadoop- creating a table (external/internal), getting information about table, setting common HIVE settings and a few others.&lt;/p&gt;
&lt;p&gt;Creating a table
Table in HIVE can be of 2 types: Internal or External
Stay with me for next few lines and I will explain the differences. 
By default, the table that we create in most of the cases is internal table. However, external tables come into picture when you want to build a table over some file.&lt;/p&gt;
&lt;p&gt;The difference between external and internal table is in terms of what happens when we drop a table. 
External table:
 —  If you drop the table, the table and the metadata of the data is dropped but not the data 
 The data is located in hdfs (and not in local file system) and since this data is accessed by many tables we don’t want the data to get dropped.&lt;/p&gt;
&lt;p&gt;Just add the keyword external to specify that we want to build an external table.
 —  CREATE EXTERNAL TABLE external_table_name&lt;/p&gt;
&lt;p&gt;Internal table:
 —  If you drop the table, the table, metadata and even the data is dropped
 —  If the data for the table resides in the local file system, you should go for creating internal table&lt;/p&gt;
&lt;p&gt;—  CREATE INTERNAL TABLE internal_table or just CREATE TABLE internal_table
Once the table gets created, we want to get the data from the table for our analysis. Before we dive into getting the data from the table, it is a good idea to get an idea about the overall structure of the table — column names, column types, whether external or internal, owner of the table.&lt;/p&gt;
&lt;p&gt;All of these information can be retrieved by using this: describe formatted table_name; This gives you a lot of information in a formatted manner. If you are in a hurry and just want to see the column names and no other details, use desc table_name;&lt;/p&gt;
&lt;p&gt;External tables are created over some file. This file should be located in hdfs-file system for hadoop. A few command lines would easily move the file to hdfs.&lt;/p&gt;
&lt;h3&gt;How to move a file to hdfs?&lt;/h3&gt;
&lt;p&gt;Below are the steps:
- Create a directory where you want to move the file
  hadoop fs -mkdir directory_name
- Check if the directory got created
  hadoop fs -ls ; you should see your directory name here
  This directory is empty right now. You can check this using
  hadoop fs -ls directory_name; It's empty
- Move the file from local to hdfs
  hadoop fs -copyFromLocal 'path where file is stored' directory_name
- Check if the file has been copied to directory
  hadoop fs -ls directory_name; You should see the file now.&lt;/p&gt;
&lt;p&gt;Open vim editor and write the below codes in any file. I will name my file create_table.hql
vim create_table.hql
Creating table. Code below&lt;/p&gt;
&lt;p&gt;drop external table if exists db_name.table_name;
create external table if not exists db_name.table_name (
ID    string,
WorkStatus     string,
Score    int,
Residence_Region    string,
income    string,
Gender    smallint,
Alcohol_Consumption    string,
Happy    string
)
row format delimited
fields terminated by ‘\t’
stored as textfile
location ‘/user/mbarnwa/data1’
;&lt;/p&gt;
&lt;p&gt;Dropping a database
Many a times, you may want to drop a database that you don’t need anymore. Say, the database to be dropped is ‘userdb’, then if you do this: drop database userdb; 
Now the above command will work if your database is empty i.e. the database, ‘userdb’ has no tables in it. 
But that is rarely a case, so either you can go ahead on deleting each table in the database by using this command drop table ‘tableName’; or addition of a simple keyword — CASCADE will solve your purpose. So the final command to drop a database (even if it has tables) is 
drop database ‘userdb’ cascade;&lt;/p&gt;</summary><category term="Hive"></category><category term="hadoop"></category><category term="big data"></category><category term="manish barnwal"></category></entry></feed>